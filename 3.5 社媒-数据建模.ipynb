{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9339031c-f7fb-4c8c-b83b-3c024bb699f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:08.293624Z",
     "iopub.status.busy": "2025-04-15T10:46:08.293624Z",
     "iopub.status.idle": "2025-04-15T10:46:09.618023Z",
     "shell.execute_reply": "2025-04-15T10:46:09.618023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰æ“ä½œç³»ç»Ÿ: Windows\n",
      "å½“å‰å­—ä½“è®¾ç½®: ['SimHei', 'Microsoft YaHei', 'SimSun']\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥æ‰€éœ€çš„æ‰€æœ‰åŒ…\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import normaltest, skew, kurtosis\n",
    "from scipy.stats import skew, kurtosis, norm\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# æ ¹æ®æ“ä½œç³»ç»Ÿè®¾ç½®åˆé€‚çš„ä¸­æ–‡å­—ä½“\n",
    "system = platform.system()\n",
    "if system == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC', 'PingFang HK', 'Apple Color Emoji']\n",
    "elif system == 'Windows':\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']\n",
    "else:  # Linuxæˆ–å…¶ä»–\n",
    "    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']\n",
    "\n",
    "# æ­£å¸¸æ˜¾ç¤ºè´Ÿå·\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# æ£€æŸ¥å­—ä½“æ˜¯å¦è®¾ç½®æˆåŠŸ\n",
    "print(f\"å½“å‰æ“ä½œç³»ç»Ÿ: {system}\")\n",
    "print(f\"å½“å‰å­—ä½“è®¾ç½®: {plt.rcParams['font.sans-serif']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f8cdf-4600-4c15-816e-82bb0f3cc6f3",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½å’Œåˆæ­¥æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283ce721-c943-406b-b46d-74826e23bef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:09.618023Z",
     "iopub.status.busy": "2025-04-15T10:46:09.618023Z",
     "iopub.status.idle": "2025-04-15T10:46:09.633427Z",
     "shell.execute_reply": "2025-04-15T10:46:09.633427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æå–çš„æ–‡ä»¶å¤¹å: ç”Ÿæˆç»“æœ/social_media/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Asin_List_file = \"æ—…è¡ŒåŒ…-æœºä¼šæ¬¾å¼.xlsx\" \n",
    "\n",
    "# # æ–‡ä»¶å’Œç›®å½•çš„è®¾ç½®\n",
    "# input_file = Asin_List_file  # è¿™å®é™…ä¸Šæ˜¯Excelæ–‡ä»¶\n",
    "\n",
    "# ä»Excelæ–‡ä»¶åä¸­æå–åŸºç¡€æ–‡ä»¶å¤¹åç§°\n",
    "folder_name = 'ç”Ÿæˆç»“æœ/social_media/'\n",
    "#folder_name = os.path.splitext(input_file)[0]\n",
    "print(f\"æå–çš„æ–‡ä»¶å¤¹å: {folder_name}\")\n",
    "\n",
    "# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"å·²åˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc923b0-6cf7-4c43-b7fb-f706cd07e92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:09.633427Z",
     "iopub.status.busy": "2025-04-15T10:46:09.633427Z",
     "iopub.status.idle": "2025-04-15T10:46:09.998588Z",
     "shell.execute_reply": "2025-04-15T10:46:09.998588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 605 è¡Œå’Œ 67 åˆ—\n",
      "ä»IV.txtåŠ è½½äº† 18 ä¸ªè‡ªå˜é‡\n",
      "å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: author_followers_cnt, author_friends_cnt\n",
      "å·²ç¡®è®¤å› å˜é‡'interaction_view_cnt'å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²åˆ›å»ºç»“æœæ–‡ä»¶: ç”Ÿæˆç»“æœ/social_media/View-æ¨¡å‹ç»“æœ.txt\n",
      "æ•°æ®é¢„è§ˆ:\n",
      "                    id  title  \\\n",
      "0  1909480340126580992    NaN   \n",
      "1  1909020699747886080    NaN   \n",
      "2  1908203431765881088    NaN   \n",
      "3  1908194727209385984    NaN   \n",
      "4  1908053656731087104    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "1  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "2  J.T. Foote Adjustable Shoe Trees - Plastic \\nğŸ› ...  NaN   en   \n",
      "3  EXVITO Metal Shoe Rack for Home â€“ Adjustable &...  NaN   en   \n",
      "4  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "1  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "2  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "3  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "4  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  è€ç”¨æ€§è¡¨è¾¾  é•¿æœŸæ»¡æ„åº¦è¡¨è¾¾  é¢œè‰²é€‰æ‹©è¡¨è¾¾  \\\n",
      "0         False                     0  ...       1      1        1       0   \n",
      "1         False                     1  ...       1      0        1       0   \n",
      "2         False                     0  ...       1      0        0       0   \n",
      "3         False                     0  ...       0      1        1       0   \n",
      "4         False                     0  ...       0      0        0       0   \n",
      "\n",
      "   å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  å“ç‰Œå£°èª‰è¡¨è¾¾  æ¶ˆè´¹è€…ä¿¡ä»»åº¦è¡¨è¾¾  åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  ç»„åˆçµæ´»æ€§è¡¨è¾¾  \n",
      "0        1       0         1       1         0        1  \n",
      "1        1       0         1       0         1        0  \n",
      "2        0       1         1       1         0        0  \n",
      "3        1       1         0       0         0        0  \n",
      "4        0       0         0       0         0        0  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "data_file = os.path.join(folder_name, \"æ•°æ®å˜é‡è¯­ä¹‰åŒ¹é…äºŒå…ƒèµ‹å€¼ç»“æœ.xlsx\")\n",
    "data = pd.read_excel(data_file)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(data)} è¡Œå’Œ {len(data.columns)} åˆ—\")\n",
    "\n",
    "# ä¿®æ”¹IVæ–‡ä»¶è·¯å¾„\n",
    "iv_file = os.path.join(folder_name, 'IV.txt')\n",
    "with open(iv_file, 'r', encoding='utf-8') as f:\n",
    "    iv_list = f.read().splitlines()\n",
    "    iv_list = [iv.strip() for iv in iv_list if iv.strip()]  # ç§»é™¤ç©ºè¡Œ\n",
    "print(f\"ä»IV.txtåŠ è½½äº† {len(iv_list)} ä¸ªè‡ªå˜é‡\")\n",
    "\n",
    "# å®šä¹‰å› å˜é‡å’Œæ§åˆ¶å˜é‡\n",
    "dv_col = 'interaction_view_cnt'\n",
    "control_vars = ['author_followers_cnt', 'author_friends_cnt']\n",
    "print(f\"å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: {', '.join(control_vars)}\")\n",
    "\n",
    "# æ£€æŸ¥DVåˆ—æ˜¯å¦å­˜åœ¨\n",
    "if dv_col not in data.columns:\n",
    "    print(f\"é”™è¯¯: æ•°æ®ä¸­ä¸å­˜åœ¨åˆ— '{dv_col}'\")\n",
    "    print(\"æ•°æ®åŒ…å«çš„åˆ—:\", data.columns.tolist()[:10], \"...\")\n",
    "    raise ValueError(f\"æ‰¾ä¸åˆ°DVåˆ—: {dv_col}\")\n",
    "else:\n",
    "    print(f\"å·²ç¡®è®¤å› å˜é‡'{dv_col}'å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥æ§åˆ¶å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "missing_controls = [var for var in control_vars if var not in data.columns]\n",
    "if missing_controls:\n",
    "    print(f\"é”™è¯¯: ä»¥ä¸‹æ§åˆ¶å˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_controls}\")\n",
    "    raise ValueError(\"ç¼ºå°‘å¿…è¦çš„æ§åˆ¶å˜é‡\")\n",
    "else:\n",
    "    print(\"å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥IVåˆ—æ˜¯å¦éƒ½å­˜åœ¨\n",
    "missing_ivs = [iv for iv in iv_list if iv not in data.columns]\n",
    "if missing_ivs:\n",
    "    print(f\"è­¦å‘Š: ä»¥ä¸‹IVåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_ivs}\")\n",
    "    iv_list = [iv for iv in iv_list if iv in data.columns]\n",
    "    print(f\"å°†ä½¿ç”¨å‰©ä½™çš„ {len(iv_list)} ä¸ªIVè¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "if not iv_list:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨çš„IVå˜é‡è¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "# ä¿®æ”¹ç»“æœæ–‡ä»¶è·¯å¾„\n",
    "result_file = os.path.join(folder_name, \"View-æ¨¡å‹ç»“æœ.txt\")\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹åˆ†æç»“æœ\\n\\n\")\n",
    "print(f\"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {result_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ\n",
    "print(\"æ•°æ®é¢„è§ˆ:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8dd4e-0539-4fbf-94a5-90a0b78c6cf4",
   "metadata": {},
   "source": [
    "## DVåˆ†å¸ƒåˆ†æï¼ˆç®€åŒ–æ¨¡å‹æ ‡å‡†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa87413-57da-45c6-a1bc-9781b840915a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:09.998588Z",
     "iopub.status.busy": "2025-04-15T10:46:09.998588Z",
     "iopub.status.idle": "2025-04-15T10:46:10.013677Z",
     "shell.execute_reply": "2025-04-15T10:46:10.013677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## interaction_view_cnt åˆ†å¸ƒåˆ†æç»“æœ:\n",
      "- æ ·æœ¬é‡: 605\n",
      "- é›¶å€¼æ•°é‡: 21\n",
      "- é›¶å€¼æ¯”ä¾‹: 3.4711\n",
      "- å‡å€¼: 576.2678\n",
      "- ä¸­ä½æ•°: 15.0000\n",
      "- æ ‡å‡†å·®: 1830.7616\n",
      "- æœ€å°å€¼: 0\n",
      "- æœ€å¤§å€¼: 16742\n",
      "- ååº¦: 4.1096\n",
      "- å³°åº¦: 19.4342\n",
      "\n",
      "## æ¨èçš„æ¨¡å‹: OLS\n",
      "- å˜é‡è½¬æ¢: å¯¹æ•°è½¬æ¢\n",
      "- åŸå› : æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º3.47%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚æ•°æ®ååº¦ä¸º4.11ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\n"
     ]
    }
   ],
   "source": [
    "# åˆ†æDVçš„åˆ†å¸ƒ\n",
    "def analyze_distribution(data, col):\n",
    "    \"\"\"\n",
    "    åˆ†æå˜é‡åˆ†å¸ƒå¹¶è¿”å›åŸºæœ¬ç»Ÿè®¡é‡å’Œæ¨¡å‹æ¨è\n",
    "    \"\"\"\n",
    "    # æå–éç©ºæ•°æ®\n",
    "    valid_data = data[col].dropna()\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡é‡\n",
    "    stats_dict = {\n",
    "        \"æ ·æœ¬é‡\": len(valid_data),\n",
    "        \"é›¶å€¼æ•°é‡\": (valid_data == 0).sum(),\n",
    "        \"é›¶å€¼æ¯”ä¾‹\": (valid_data == 0).mean() * 100,\n",
    "        \"å‡å€¼\": valid_data.mean(),\n",
    "        \"ä¸­ä½æ•°\": valid_data.median(),\n",
    "        \"æ ‡å‡†å·®\": valid_data.std(),\n",
    "        \"æœ€å°å€¼\": valid_data.min(),\n",
    "        \"æœ€å¤§å€¼\": valid_data.max(),\n",
    "        \"ååº¦\": skew(valid_data),\n",
    "        \"å³°åº¦\": kurtosis(valid_data)\n",
    "    }\n",
    "\n",
    "    # æ¨¡å‹é€‰æ‹©é€»è¾‘ - æ ¹æ®é›¶å€¼æ¯”ä¾‹å’Œåˆ†å¸ƒååº¦\n",
    "    zero_inflated = stats_dict[\"é›¶å€¼æ¯”ä¾‹\"] > 20  # å¦‚æœé›¶å€¼è¶…è¿‡20%ï¼Œä½¿ç”¨Tobit\n",
    "    high_skew = abs(stats_dict[\"ååº¦\"]) > 1.0   # å¦‚æœååº¦è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°è½¬æ¢\n",
    "    \n",
    "    model_type = \"ols\"  # é»˜è®¤æ¨¡å‹ç±»å‹\n",
    "    transform_type = \"none\"  # é»˜è®¤ä¸è½¬æ¢\n",
    "    reason = \"\"\n",
    "    \n",
    "    if zero_inflated:\n",
    "        model_type = \"tobit\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    else:\n",
    "        model_type = \"ols\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    \n",
    "    return model_type, transform_type, reason, stats_dict\n",
    "\n",
    "# å¯¹DVè¿›è¡Œåˆ†å¸ƒåˆ†æ\n",
    "model_type, transform_type, reason, dv_stats = analyze_distribution(data, dv_col)\n",
    "\n",
    "# æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "print(f\"\\n## {dv_col} åˆ†å¸ƒåˆ†æç»“æœ:\")\n",
    "for key, value in dv_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n## æ¨èçš„æ¨¡å‹: {model_type.upper()}\")\n",
    "print(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\")\n",
    "print(f\"- åŸå› : {reason}\")\n",
    "\n",
    "# å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {dv_col} åˆ†å¸ƒåˆ†æ\\n\\n\")\n",
    "    for key, value in dv_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"- {key}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"- {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n## æ¨¡å‹é€‰æ‹©\\n\\n\")\n",
    "    f.write(f\"- æ¨èçš„æ¨¡å‹ç±»å‹: {model_type.upper()}\\n\")\n",
    "    f.write(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\\n\")\n",
    "    f.write(f\"- é€‰æ‹©åŸå› : {reason}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82877530-0c32-459c-a16f-d2616318cb34",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f91e72a-700a-40d1-8c0e-d470a6df1fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.013677Z",
     "iopub.status.busy": "2025-04-15T10:46:10.013677Z",
     "iopub.status.idle": "2025-04-15T10:46:10.043864Z",
     "shell.execute_reply": "2025-04-15T10:46:10.043864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„å¤„ç†åçš„æ•°æ®: 605 è¡Œ Ã— 21 åˆ—\n",
      "è­¦å‘Š: interaction_view_cnt çš„æœ€å°å€¼ä¸º 0ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\n",
      "å°†ä½¿ç”¨log(1+x)è½¬æ¢\n",
      "å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: log_interaction_view_cnt\n",
      "å›å½’å…¬å¼:\n",
      "log_interaction_view_cnt ~ å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾ + ç©ºé—´æ•ˆç‡è¡¨è¾¾ + ç¯ä¿æè´¨åå¥½ + ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾ + è®¾è®¡ç¾æ„Ÿè¡¨è¾¾ + å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾ + é˜²å°˜åŠŸèƒ½è¡¨è¾¾ + ä»·æ ¼è¡¨è¾¾ + å¤šåŠŸèƒ½æ€§è¡¨è¾¾ + ä¾¿æºæ€§è¡¨è¾¾ + æ‰¿é‡èƒ½åŠ›è¡¨è¾¾ + è€ç”¨æ€§è¡¨è¾¾ + é¢œè‰²é€‰æ‹©è¡¨è¾¾ + å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾ + å“ç‰Œå£°èª‰è¡¨è¾¾ + åˆ›æ–°åŠŸèƒ½è¡¨è¾¾ + å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾ + ç»„åˆçµæ´»æ€§è¡¨è¾¾ + author_followers_cnt + author_friends_cnt\n",
      "\n",
      "å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\n",
      "VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\n",
      "                      å˜é‡       VIF\n",
      "12                é¢œè‰²é€‰æ‹©è¡¨è¾¾  5.844991\n",
      "4                 è®¾è®¡ç¾æ„Ÿè¡¨è¾¾  5.432510\n",
      "0                å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾  3.560953\n",
      "8                 å¤šåŠŸèƒ½æ€§è¡¨è¾¾  3.394563\n",
      "11                 è€ç”¨æ€§è¡¨è¾¾  3.326069\n",
      "3                ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾  3.258193\n",
      "2                 ç¯ä¿æè´¨åå¥½  3.220954\n",
      "10                æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  3.209544\n",
      "13               å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  3.190966\n",
      "9                  ä¾¿æºæ€§è¡¨è¾¾  3.183719\n",
      "17               ç»„åˆçµæ´»æ€§è¡¨è¾¾  3.163847\n",
      "1                 ç©ºé—´æ•ˆç‡è¡¨è¾¾  2.904204\n",
      "15                åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  2.775960\n",
      "14                å“ç‰Œå£°èª‰è¡¨è¾¾  2.485333\n",
      "6                 é˜²å°˜åŠŸèƒ½è¡¨è¾¾  2.375646\n",
      "16              å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  2.106301\n",
      "5               å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾  1.919452\n",
      "7                   ä»·æ ¼è¡¨è¾¾  1.672822\n",
      "19    author_friends_cnt  1.279311\n",
      "18  author_followers_cnt  1.199435\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
    "all_predictors = iv_list + control_vars\n",
    "model_data = data[[dv_col] + all_predictors].copy()\n",
    "model_data = model_data.dropna()  # ç§»é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "print(f\"é¢„å¤„ç†åçš„æ•°æ®: {len(model_data)} è¡Œ Ã— {len(model_data.columns)} åˆ—\")\n",
    "\n",
    "# å¤„ç†å› å˜é‡è½¬æ¢\n",
    "transformed_dv = dv_col\n",
    "if transform_type == \"log\":\n",
    "    # æ£€æŸ¥é›¶å€¼å’Œè´Ÿå€¼\n",
    "    min_value = model_data[dv_col].min()\n",
    "    if min_value <= 0:\n",
    "        print(f\"è­¦å‘Š: {dv_col} çš„æœ€å°å€¼ä¸º {min_value}ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\")\n",
    "        print(\"å°†ä½¿ç”¨log(1+x)è½¬æ¢\")\n",
    "        model_data[f'log_{dv_col}'] = np.log1p(model_data[dv_col])\n",
    "    else:\n",
    "        model_data[f'log_{dv_col}'] = np.log(model_data[dv_col])\n",
    "    transformed_dv = f'log_{dv_col}'\n",
    "    print(f\"å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: {transformed_dv}\")\n",
    "else:\n",
    "    print(f\"ä½¿ç”¨åŸå§‹å˜é‡: {transformed_dv}\")\n",
    "\n",
    "# æ„å»ºå…¬å¼å¹¶æ˜¾ç¤º\n",
    "formula = f\"{transformed_dv} ~ \" + \" + \".join(all_predictors)\n",
    "print(f\"å›å½’å…¬å¼:\")\n",
    "print(formula)\n",
    "\n",
    "# æ£€æŸ¥å¤šé‡å…±çº¿æ€§\n",
    "if len(all_predictors) > 1:\n",
    "    X = model_data[all_predictors]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"å˜é‡\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(\"\\nå¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\")\n",
    "    print(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\")\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "    with open(result_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"## å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF)\\n\\n\")\n",
    "        f.write(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\\n\\n\")\n",
    "        f.write(\"| å˜é‡ | VIF |\\n\")\n",
    "        f.write(\"|------|------|\\n\")\n",
    "        for index, row in vif_data.iterrows():\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['VIF']:.4f} |\\n\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e4c66-0309-4b61-949d-24e8a97f9303",
   "metadata": {},
   "source": [
    "## æ¨¡å‹æ‹Ÿåˆä¸ç»“æœè¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035c317f-d4c2-4d7b-ba0f-06feb29322ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.043864Z",
     "iopub.status.busy": "2025-04-15T10:46:10.043864Z",
     "iopub.status.idle": "2025-04-15T10:46:10.105433Z",
     "shell.execute_reply": "2025-04-15T10:46:10.105433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‹ŸåˆOLSæ¨¡å‹...\n",
      "\n",
      "OLSå›å½’ç»“æœ:\n",
      "                               OLS Regression Results                               \n",
      "====================================================================================\n",
      "Dep. Variable:     log_interaction_view_cnt   R-squared:                       0.361\n",
      "Model:                                  OLS   Adj. R-squared:                  0.339\n",
      "Method:                       Least Squares   F-statistic:                     16.46\n",
      "Date:                      Tue, 15 Apr 2025   Prob (F-statistic):           1.08e-44\n",
      "Time:                              18:46:10   Log-Likelihood:                -1183.8\n",
      "No. Observations:                       605   AIC:                             2410.\n",
      "Df Residuals:                           584   BIC:                             2502.\n",
      "Df Model:                                20                                         \n",
      "Covariance Type:                  nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                3.2408      0.214     15.161      0.000       2.821       3.661\n",
      "å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾                  0.2700      0.212      1.276      0.202      -0.146       0.685\n",
      "ç©ºé—´æ•ˆç‡è¡¨è¾¾                   0.1643      0.195      0.842      0.400      -0.219       0.548\n",
      "ç¯ä¿æè´¨åå¥½                  -0.1986      0.203     -0.979      0.328      -0.597       0.200\n",
      "ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾                  0.3338      0.202      1.649      0.100      -0.064       0.731\n",
      "è®¾è®¡ç¾æ„Ÿè¡¨è¾¾                  -0.3665      0.261     -1.403      0.161      -0.879       0.146\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾                 0.4666      0.160      2.917      0.004       0.152       0.781\n",
      "é˜²å°˜åŠŸèƒ½è¡¨è¾¾                  -0.2475      0.173     -1.429      0.153      -0.588       0.093\n",
      "ä»·æ ¼è¡¨è¾¾                     0.3553      0.191      1.859      0.064      -0.020       0.731\n",
      "å¤šåŠŸèƒ½æ€§è¡¨è¾¾                  -0.3923      0.207     -1.897      0.058      -0.798       0.014\n",
      "ä¾¿æºæ€§è¡¨è¾¾                   -0.1496      0.200     -0.748      0.454      -0.542       0.243\n",
      "æ‰¿é‡èƒ½åŠ›è¡¨è¾¾                  -0.2235      0.203     -1.099      0.272      -0.623       0.176\n",
      "è€ç”¨æ€§è¡¨è¾¾                    0.3449      0.204      1.688      0.092      -0.056       0.746\n",
      "é¢œè‰²é€‰æ‹©è¡¨è¾¾                  -0.1530      0.270     -0.567      0.571      -0.683       0.377\n",
      "å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾                 -0.1596      0.202     -0.790      0.430      -0.556       0.237\n",
      "å“ç‰Œå£°èª‰è¡¨è¾¾                   0.3055      0.181      1.685      0.093      -0.051       0.662\n",
      "åˆ›æ–°åŠŸèƒ½è¡¨è¾¾                  -0.1577      0.190     -0.828      0.408      -0.532       0.216\n",
      "å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾                -0.1951      0.167     -1.168      0.243      -0.523       0.133\n",
      "ç»„åˆçµæ´»æ€§è¡¨è¾¾                 -0.2943      0.199     -1.482      0.139      -0.684       0.096\n",
      "author_followers_cnt  5.124e-06   3.46e-07     14.790      0.000    4.44e-06     5.8e-06\n",
      "author_friends_cnt   -1.446e-06   4.09e-05     -0.035      0.972   -8.18e-05    7.89e-05\n",
      "==============================================================================\n",
      "Omnibus:                      120.501   Durbin-Watson:                   1.664\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1937.433\n",
      "Skew:                          -0.346   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.740   Cond. No.                     1.11e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.11e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): 0.3605\n",
      "- è°ƒæ•´åçš„RÂ²: 0.3386\n",
      "- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F(20,584)=16.4626, p=0.000000\n",
      "  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 2/20\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 1/18\n",
      "- å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: ç³»æ•°=0.4666, p=0.003674\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 1/2\n",
      "- author_followers_cnt: ç³»æ•°=0.0000, p=0.000000\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): 0.3605\n",
      "- è°ƒæ•´åçš„RÂ²: 0.3386\n",
      "- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F(20,584)=16.4626, p=0.000000\n",
      "  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 2/20\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 1/18\n",
      "- å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: ç³»æ•°=0.4666, p=0.003674\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 1/2\n",
      "- author_followers_cnt: ç³»æ•°=0.0000, p=0.000000\n",
      "\n",
      "æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/View-æ¨¡å‹ç»“æœ.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:212: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]  # +1 to skip intercept\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:213: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:220: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]  # +1 to skip intercept\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:221: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:272: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]  # +1 to skip intercept\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:273: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:280: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]  # +1 to skip intercept\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:281: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:308: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:309: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:315: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  coef = results.params[idx + 1]\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27916\\1668281642.py:316: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p = p_values[idx]\n"
     ]
    }
   ],
   "source": [
    "print(f\"å¼€å§‹æ‹Ÿåˆ{model_type.upper()}æ¨¡å‹...\")\n",
    "\n",
    "if model_type == \"tobit\":\n",
    "    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼\n",
    "    try:\n",
    "        # å°è¯•ä»truncated_modelæ¨¡å—å¯¼å…¥\n",
    "        from statsmodels.discrete.truncated_model import Tobit\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        y = model_data[transformed_dv].values\n",
    "        X = sm.add_constant(model_data[all_predictors])\n",
    "        \n",
    "        print(\"ä½¿ç”¨statsmodels.discrete.truncated_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "        tobit_model = Tobit(y, X, left=0)\n",
    "        results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "        \n",
    "    except (ImportError, AttributeError):\n",
    "        # å¦‚æœä¸Šè¿°å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨censored_modelæ¨¡å—\n",
    "        try:\n",
    "            from statsmodels.regression.censored_model import Tobit\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            y = model_data[transformed_dv].values\n",
    "            X = sm.add_constant(model_data[all_predictors])\n",
    "            \n",
    "            print(\"ä½¿ç”¨statsmodels.regression.censored_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "            tobit_model = Tobit(y, X, left=0)\n",
    "            results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "            \n",
    "        except (ImportError, AttributeError):\n",
    "            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰å®ç°\n",
    "            print(\"ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\")\n",
    "            from scipy.stats import norm\n",
    "            from scipy import optimize\n",
    "            from statsmodels.regression.linear_model import OLS\n",
    "            from statsmodels.base.model import GenericLikelihoodModel\n",
    "            \n",
    "            class TobitModel(GenericLikelihoodModel):\n",
    "                def __init__(self, endog, exog, left=0, **kwds):\n",
    "                    self.left = left\n",
    "                    super(TobitModel, self).__init__(endog, exog, **kwds)\n",
    "                \n",
    "                def loglikeobs(self, params):\n",
    "                    beta = params[:-1]\n",
    "                    sigma = np.abs(params[-1])  # ç¡®ä¿sigmaä¸ºæ­£\n",
    "                    \n",
    "                    q = self.endog\n",
    "                    x = self.exog\n",
    "                    \n",
    "                    # è®¡ç®—æ¡ä»¶æœŸæœ›\n",
    "                    mu = np.dot(x, beta)\n",
    "                    \n",
    "                    # åˆ†åˆ«è®¡ç®—æˆªå°¾å’Œéæˆªå°¾å€¼çš„å¯¹æ•°ä¼¼ç„¶\n",
    "                    censored_mask = (q <= self.left)\n",
    "                    z = (self.left - mu) / sigma\n",
    "                    \n",
    "                    ll_censored = censored_mask * norm.logcdf(z)\n",
    "                    \n",
    "                    non_censored_mask = ~censored_mask\n",
    "                    ll_non_censored = non_censored_mask * (\n",
    "                        -np.log(sigma) + \n",
    "                        norm.logpdf((q - mu) / sigma)\n",
    "                    )\n",
    "                    \n",
    "                    # å¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜\n",
    "                    result = ll_censored + ll_non_censored\n",
    "                    # æ›¿æ¢æ— æ•ˆå€¼\n",
    "                    result = np.where(np.isnan(result) | np.isinf(result), -1e10, result)\n",
    "                    \n",
    "                    return result\n",
    "                \n",
    "                def nloglikeobs(self, params):\n",
    "                    \"\"\"è´Ÿå¯¹æ•°ä¼¼ç„¶\"\"\"\n",
    "                    return -self.loglikeobs(params)\n",
    "                \n",
    "                def fit(self, start_params=None, method='bfgs', maxiter=50000, **kwds):\n",
    "                    \"\"\"æ·»åŠ æ›´å¤šä¼˜åŒ–æ–¹æ³•é€‰é¡¹å’Œæ›´å¥½çš„åˆå§‹å€¼ç­–ç•¥\"\"\"\n",
    "                    if start_params is None:\n",
    "                        # ä½¿ç”¨OLSä¼°è®¡è·å–æ›´ç¨³å®šçš„åˆå§‹å€¼\n",
    "                        ols_model = OLS(\n",
    "                            np.where(self.endog <= self.left, self.left, self.endog),\n",
    "                            self.exog\n",
    "                        )\n",
    "                        ols_res = ols_model.fit()\n",
    "                        # ä½¿ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä½œä¸ºsigmaçš„åˆå§‹å€¼\n",
    "                        start_params = np.append(ols_res.params, np.std(ols_res.resid))\n",
    "                    \n",
    "                    # æ·»åŠ æ›´å¤šä¼˜åŒ–é€‰é¡¹\n",
    "                    if 'options' not in kwds:\n",
    "                        kwds['options'] = {}\n",
    "                    kwds['options']['maxiter'] = maxiter\n",
    "                    \n",
    "                    # æ·»åŠ å®¹é”™è®¾ç½®\n",
    "                    try:\n",
    "                        return super(TobitModel, self).fit(\n",
    "                            start_params=start_params,\n",
    "                            method=method, \n",
    "                            **kwds\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"é¦–æ¬¡ä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
    "                        try:\n",
    "                            # å°è¯•Powellæ–¹æ³•\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='powell',\n",
    "                                options={'maxiter': 100000, 'ftol': 1e-8, 'xtol': 1e-8},\n",
    "                                **kwds\n",
    "                            )\n",
    "                        except:\n",
    "                            print(\"æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„ä¼˜åŒ–è®¾ç½®...\")\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='nm',\n",
    "                                options={'maxiter': 100000},\n",
    "                                **kwds\n",
    "                            )\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ® - ä¿®æ”¹æ•°æ®å‡†å¤‡éƒ¨åˆ†ä»¥é¿å…AttributeError\n",
    "            y = model_data[transformed_dv].values\n",
    "            X_df = sm.add_constant(model_data[all_predictors])  # ä¿æŒDataFrameæ ¼å¼\n",
    "            X_array = X_df.values  # æ•°ç»„ç‰ˆæœ¬ç”¨äºæ‹Ÿåˆ\n",
    "            X_columns = X_df.columns.tolist()  # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨\n",
    "            \n",
    "            # æ‹ŸåˆTobitæ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•\n",
    "            tobit_model = TobitModel(y, X_array, left=0)\n",
    "            try:\n",
    "                # é¦–å…ˆå°è¯•Nelder-Meadæ–¹æ³•\n",
    "                results = tobit_model.fit(method='nm', disp=0, maxiter=50000)\n",
    "            except:\n",
    "                try:\n",
    "                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•BFGSæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='bfgs', disp=0, maxiter=50000)\n",
    "                except:\n",
    "                    # æœ€åå°è¯•Powellæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='powell', disp=0, maxiter=50000)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nTobitæ¨¡å‹ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "    \n",
    "else:  # ä½¿ç”¨OLSæ¨¡å‹ - ä¿æŒä¸å˜\n",
    "    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹\n",
    "    model = smf.ols(formula=formula, data=model_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    print(\"\\nOLSå›å½’ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯» - ä¿æŒä¸å˜\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹ - ä¿®æ”¹æ­¤éƒ¨åˆ†ä»¥å¤„ç†å¯èƒ½çš„NaNé—®é¢˜\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # å®‰å…¨è·å–Tobitæ¨¡å‹çš„ç³»æ•°å’Œpå€¼\n",
    "    if hasattr(results, 'pvalues') and not np.all(np.isnan(results.pvalues)):\n",
    "        p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\")\n",
    "        # ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
    "        coefs = results.params[1:-1]\n",
    "        se = np.ones_like(coefs) * 0.05  # å‡è®¾æ ‡å‡†è¯¯ä¸º0.05\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coefs / se)))\n",
    "    \n",
    "    # åœ¨è‡ªå®šä¹‰Tobitä¸­X_columnså·²å®šä¹‰ï¼Œåœ¨statsmodelsç‰ˆæœ¬ä¸­éœ€è¦å®šä¹‰\n",
    "    if 'X_columns' not in locals():\n",
    "        X_columns = model_data[all_predictors].columns.tolist()\n",
    "        X_columns = ['const'] + X_columns\n",
    "    \n",
    "    var_names = X_columns[1:-1] if model_type == \"tobit\" else X_columns[1:]  # é€‚åº”ä¸åŒæƒ…å†µ\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯»\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # Tobitæ¨¡å‹ä¸­è·å–ç³»æ•°å’Œpå€¼\n",
    "    p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    var_names = list(X.columns[1:])  # è·³è¿‡å¸¸æ•°é¡¹\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜è§£è¯»ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"## æ¨¡å‹ç»“æœè§£è¯»\\n\\n\")\n",
    "    \n",
    "    if model_type == \"ols\":\n",
    "        f.write(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\\n\")\n",
    "        f.write(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\\n\")\n",
    "        f.write(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\\n\")\n",
    "        \n",
    "        if results.f_pvalue < 0.05:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\\n\")\n",
    "        f.write(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\\n\")\n",
    "        f.write(f\"  - AIC: {results.aic:.4f}\\n\")\n",
    "        f.write(f\"  - BIC: {results.bic:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\\n\")\n",
    "    for var in iv_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\\n\")\n",
    "    for var in ctrl_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° {result_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd41c7-6e8a-4a61-bbd1-15c6152a5efc",
   "metadata": {},
   "source": [
    "## ç³»æ•°å½’ä¸€åŒ–å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6771c313-7bd5-42f0-b68a-65e6240f87a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.105433Z",
     "iopub.status.busy": "2025-04-15T10:46:10.105433Z",
     "iopub.status.idle": "2025-04-15T10:46:10.120464Z",
     "shell.execute_reply": "2025-04-15T10:46:10.120464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\n",
      "\n",
      "å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\n",
      "-----------------------------------\n",
      "æ€»è®¡ 2 ä¸ªæœ‰æ•ˆå˜é‡, æ˜¾è‘— (p<0.05)\n",
      "-----------------------------------\n",
      "\n",
      "è‡ªå˜é‡:\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: 0.4666 (å½’ä¸€åŒ–: 1.0000, 100.00%)*\n",
      "\n",
      "æ§åˆ¶å˜é‡:\n",
      "author_followers_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)*\n",
      "-----------------------------------\n",
      "* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/View-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\n"
     ]
    }
   ],
   "source": [
    "# å½’ä¸€åŒ–æ˜¾è‘—ç³»æ•°å¹¶è¾“å‡ºç»“æœ\n",
    "def normalize_significant_coefficients(results, iv_list, control_vars=None, model_type=\"ols\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ˜¾è‘—å˜é‡çš„ç³»æ•°ï¼Œåˆ é™¤NaNå€¼ï¼ŒæŒ‰ç»å¯¹å€¼å¤§å°æ’åºå¹¶è¾“å‡ºç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: å›å½’ç»“æœå¯¹è±¡\n",
    "    - iv_list: è‡ªå˜é‡åˆ—è¡¨\n",
    "    - control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone\n",
    "    - model_type: æ¨¡å‹ç±»å‹ï¼Œ\"ols\"æˆ–\"tobit\"\n",
    "    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05\n",
    "    \"\"\"\n",
    "    if control_vars is None:\n",
    "        control_vars = []\n",
    "    \n",
    "    all_vars = iv_list + control_vars\n",
    "    \n",
    "    # è·å–ç³»æ•°å’Œpå€¼ - éœ€è¦è€ƒè™‘ä¸åŒæ¨¡å‹ç±»å‹\n",
    "    if model_type.lower() == \"ols\":\n",
    "        # OLSæ¨¡å‹ - è·³è¿‡æˆªè·\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    else:\n",
    "        # Tobitæ¨¡å‹ - è·³è¿‡æˆªè·å’Œsigma\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'å˜é‡': all_vars,\n",
    "        'ç³»æ•°': coefs,\n",
    "        'på€¼': p_values,\n",
    "        'å˜é‡ç±»å‹': ['è‡ªå˜é‡' if var in iv_list else 'æ§åˆ¶å˜é‡' for var in all_vars]\n",
    "    })\n",
    "    \n",
    "    # è¿‡æ»¤æ‰NaNç³»æ•°å’Œéæ˜¾è‘—çš„å˜é‡\n",
    "    valid_coef_df = coef_df.dropna(subset=['ç³»æ•°'])\n",
    "    sig_coef_df = valid_coef_df[valid_coef_df['på€¼'] < alpha].copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æ˜¾è‘—å˜é‡ï¼Œåˆ™æŠ¥å‘Šæ‰€æœ‰æœ‰æ•ˆå˜é‡\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\")\n",
    "        sig_coef_df = valid_coef_df.copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç³»æ•°ï¼Œé€€å‡º\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç³»æ•°ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—ç³»æ•°çš„ç»å¯¹å€¼\n",
    "    sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°'].abs()\n",
    "    \n",
    "    # å½’ä¸€åŒ–ç³»æ•°\n",
    "    total_abs = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'].sum()\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°'] = sig_coef_df['ç³»æ•°'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”'] = sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] * 100\n",
    "    \n",
    "    # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åº\n",
    "    sig_coef_df = sig_coef_df.sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\nå½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\")\n",
    "    print(\"-----------------------------------\")\n",
    "    significant_str = \"æ˜¾è‘— (p<0.05)\" if len(sig_coef_df[sig_coef_df['på€¼'] < alpha]) > 0 else \"æ— æ˜¾è‘—å˜é‡\"\n",
    "    print(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "    for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "        type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "        if len(type_df) > 0:\n",
    "            print(f\"\\n{var_type}:\")\n",
    "            for _, row in type_df.iterrows():\n",
    "                sig_mark = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                print(f\"{row['å˜é‡']}: {row['ç³»æ•°']:.4f} (å½’ä¸€åŒ–: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}, {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%){sig_mark}\")\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    output_file = os.path.join(folder_name, \"View-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {model_type.upper()} æ¨¡å‹å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº)\\n\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\\n\")\n",
    "        f.write(\"-----------------------------------\\n\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ€»è¡¨\n",
    "        f.write(\"| å˜é‡ | å˜é‡ç±»å‹ | ç³»æ•° | på€¼ | å½’ä¸€åŒ–ç³»æ•° | å½’ä¸€åŒ–ç™¾åˆ†æ¯” | æ˜¾è‘—æ€§ |\\n\")\n",
    "        f.write(\"|------|---------|------|------|------------|------------|--------|\\n\")\n",
    "        \n",
    "        for _, row in sig_coef_df.iterrows():\n",
    "            sig = \"æ˜¯\" if row['på€¼'] < alpha else \"å¦\"\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['å˜é‡ç±»å‹']} | {row['ç³»æ•°']:.4f} | {row['på€¼']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}% | {sig} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "        \n",
    "        # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "        for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "            type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "            if len(type_df) > 0:\n",
    "                f.write(f\"### {var_type}\\n\\n\")\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    sig_symbol = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                    direction = \"æ­£å‘\" if row['ç³»æ•°'] > 0 else \"è´Ÿå‘\"\n",
    "                    f.write(f\"#### {row['å˜é‡']}{sig_symbol}\\n\\n\")\n",
    "                    f.write(f\"- ç³»æ•°: {row['ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- på€¼: {row['på€¼']:.4f}\\n\")\n",
    "                    f.write(f\"- å½±å“æ–¹å‘: {direction}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç³»æ•°: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç™¾åˆ†æ¯”: {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%\\n\")\n",
    "                    f.write(f\"- æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if row['på€¼'] < alpha else 'å¦'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\\n\")\n",
    "    \n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ° {output_file}\")\n",
    "    \n",
    "    return sig_coef_df\n",
    "# è°ƒç”¨å½’ä¸€åŒ–å‡½æ•°\n",
    "print(\"\\nå¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\")\n",
    "normalized_coefs = normalize_significant_coefficients(\n",
    "    results=results,\n",
    "    iv_list=iv_list,\n",
    "    control_vars=control_vars,\n",
    "    model_type=model_type,\n",
    "    alpha=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c52ae-6351-44fa-8b5f-f7c653edba7e",
   "metadata": {},
   "source": [
    "## å…¶ä»–å˜é‡åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e46d5-c7e8-4c26-8b43-b2944f9a656b",
   "metadata": {},
   "source": [
    "### ç‚¹èµåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba720dd-519a-4731-84c6-45e6b3f2a406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.120464Z",
     "iopub.status.busy": "2025-04-15T10:46:10.120464Z",
     "iopub.status.idle": "2025-04-15T10:46:10.381002Z",
     "shell.execute_reply": "2025-04-15T10:46:10.381002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 605 è¡Œå’Œ 67 åˆ—\n",
      "ä»IV.txtåŠ è½½äº† 18 ä¸ªè‡ªå˜é‡\n",
      "å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: author_followers_cnt, author_friends_cnt\n",
      "å·²ç¡®è®¤å› å˜é‡'interaction_like_cnt'å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²åˆ›å»ºç»“æœæ–‡ä»¶: ç”Ÿæˆç»“æœ/social_media/like-æ¨¡å‹ç»“æœ.txt\n",
      "æ•°æ®é¢„è§ˆ:\n",
      "                    id  title  \\\n",
      "0  1909480340126580992    NaN   \n",
      "1  1909020699747886080    NaN   \n",
      "2  1908203431765881088    NaN   \n",
      "3  1908194727209385984    NaN   \n",
      "4  1908053656731087104    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "1  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "2  J.T. Foote Adjustable Shoe Trees - Plastic \\nğŸ› ...  NaN   en   \n",
      "3  EXVITO Metal Shoe Rack for Home â€“ Adjustable &...  NaN   en   \n",
      "4  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "1  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "2  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "3  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "4  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  è€ç”¨æ€§è¡¨è¾¾  é•¿æœŸæ»¡æ„åº¦è¡¨è¾¾  é¢œè‰²é€‰æ‹©è¡¨è¾¾  \\\n",
      "0         False                     0  ...       1      1        1       0   \n",
      "1         False                     1  ...       1      0        1       0   \n",
      "2         False                     0  ...       1      0        0       0   \n",
      "3         False                     0  ...       0      1        1       0   \n",
      "4         False                     0  ...       0      0        0       0   \n",
      "\n",
      "   å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  å“ç‰Œå£°èª‰è¡¨è¾¾  æ¶ˆè´¹è€…ä¿¡ä»»åº¦è¡¨è¾¾  åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  ç»„åˆçµæ´»æ€§è¡¨è¾¾  \n",
      "0        1       0         1       1         0        1  \n",
      "1        1       0         1       0         1        0  \n",
      "2        0       1         1       1         0        0  \n",
      "3        1       1         0       0         0        0  \n",
      "4        0       0         0       0         0        0  \n",
      "\n",
      "[5 rows x 67 columns]\n",
      "\n",
      "## interaction_like_cnt åˆ†å¸ƒåˆ†æç»“æœ:\n",
      "- æ ·æœ¬é‡: 605\n",
      "- é›¶å€¼æ•°é‡: 456\n",
      "- é›¶å€¼æ¯”ä¾‹: 75.3719\n",
      "- å‡å€¼: 1.1256\n",
      "- ä¸­ä½æ•°: 0.0000\n",
      "- æ ‡å‡†å·®: 6.3716\n",
      "- æœ€å°å€¼: 0\n",
      "- æœ€å¤§å€¼: 109\n",
      "- ååº¦: 12.4797\n",
      "- å³°åº¦: 180.1477\n",
      "\n",
      "## æ¨èçš„æ¨¡å‹: TOBIT\n",
      "- å˜é‡è½¬æ¢: å¯¹æ•°è½¬æ¢\n",
      "- åŸå› : æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º75.37%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚æ•°æ®ååº¦ä¸º12.48ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\n",
      "é¢„å¤„ç†åçš„æ•°æ®: 605 è¡Œ Ã— 21 åˆ—\n",
      "è­¦å‘Š: interaction_like_cnt çš„æœ€å°å€¼ä¸º 0ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\n",
      "å°†ä½¿ç”¨log(1+x)è½¬æ¢\n",
      "å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: log_interaction_like_cnt\n",
      "å›å½’å…¬å¼:\n",
      "log_interaction_like_cnt ~ å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾ + ç©ºé—´æ•ˆç‡è¡¨è¾¾ + ç¯ä¿æè´¨åå¥½ + ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾ + è®¾è®¡ç¾æ„Ÿè¡¨è¾¾ + å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾ + é˜²å°˜åŠŸèƒ½è¡¨è¾¾ + ä»·æ ¼è¡¨è¾¾ + å¤šåŠŸèƒ½æ€§è¡¨è¾¾ + ä¾¿æºæ€§è¡¨è¾¾ + æ‰¿é‡èƒ½åŠ›è¡¨è¾¾ + è€ç”¨æ€§è¡¨è¾¾ + é¢œè‰²é€‰æ‹©è¡¨è¾¾ + å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾ + å“ç‰Œå£°èª‰è¡¨è¾¾ + åˆ›æ–°åŠŸèƒ½è¡¨è¾¾ + å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾ + ç»„åˆçµæ´»æ€§è¡¨è¾¾ + author_followers_cnt + author_friends_cnt\n",
      "\n",
      "å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\n",
      "VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\n",
      "                      å˜é‡       VIF\n",
      "12                é¢œè‰²é€‰æ‹©è¡¨è¾¾  5.844991\n",
      "4                 è®¾è®¡ç¾æ„Ÿè¡¨è¾¾  5.432510\n",
      "0                å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾  3.560953\n",
      "8                 å¤šåŠŸèƒ½æ€§è¡¨è¾¾  3.394563\n",
      "11                 è€ç”¨æ€§è¡¨è¾¾  3.326069\n",
      "3                ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾  3.258193\n",
      "2                 ç¯ä¿æè´¨åå¥½  3.220954\n",
      "10                æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  3.209544\n",
      "13               å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  3.190966\n",
      "9                  ä¾¿æºæ€§è¡¨è¾¾  3.183719\n",
      "17               ç»„åˆçµæ´»æ€§è¡¨è¾¾  3.163847\n",
      "1                 ç©ºé—´æ•ˆç‡è¡¨è¾¾  2.904204\n",
      "15                åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  2.775960\n",
      "14                å“ç‰Œå£°èª‰è¡¨è¾¾  2.485333\n",
      "6                 é˜²å°˜åŠŸèƒ½è¡¨è¾¾  2.375646\n",
      "16              å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  2.106301\n",
      "5               å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾  1.919452\n",
      "7                   ä»·æ ¼è¡¨è¾¾  1.672822\n",
      "19    author_friends_cnt  1.279311\n",
      "18  author_followers_cnt  1.199435\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "data_file = os.path.join(folder_name, \"æ•°æ®å˜é‡è¯­ä¹‰åŒ¹é…äºŒå…ƒèµ‹å€¼ç»“æœ.xlsx\")\n",
    "data = pd.read_excel(data_file)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(data)} è¡Œå’Œ {len(data.columns)} åˆ—\")\n",
    "\n",
    "# ä¿®æ”¹IVæ–‡ä»¶è·¯å¾„\n",
    "iv_file = os.path.join(folder_name, 'IV.txt')\n",
    "with open(iv_file, 'r', encoding='utf-8') as f:\n",
    "    iv_list = f.read().splitlines()\n",
    "    iv_list = [iv.strip() for iv in iv_list if iv.strip()]  # ç§»é™¤ç©ºè¡Œ\n",
    "print(f\"ä»IV.txtåŠ è½½äº† {len(iv_list)} ä¸ªè‡ªå˜é‡\")\n",
    "\n",
    "# å®šä¹‰å› å˜é‡å’Œæ§åˆ¶å˜é‡\n",
    "dv_col = 'interaction_like_cnt'\n",
    "control_vars = ['author_followers_cnt', 'author_friends_cnt']\n",
    "print(f\"å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: {', '.join(control_vars)}\")\n",
    "\n",
    "# æ£€æŸ¥DVåˆ—æ˜¯å¦å­˜åœ¨\n",
    "if dv_col not in data.columns:\n",
    "    print(f\"é”™è¯¯: æ•°æ®ä¸­ä¸å­˜åœ¨åˆ— '{dv_col}'\")\n",
    "    print(\"æ•°æ®åŒ…å«çš„åˆ—:\", data.columns.tolist()[:10], \"...\")\n",
    "    raise ValueError(f\"æ‰¾ä¸åˆ°DVåˆ—: {dv_col}\")\n",
    "else:\n",
    "    print(f\"å·²ç¡®è®¤å› å˜é‡'{dv_col}'å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥æ§åˆ¶å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "missing_controls = [var for var in control_vars if var not in data.columns]\n",
    "if missing_controls:\n",
    "    print(f\"é”™è¯¯: ä»¥ä¸‹æ§åˆ¶å˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_controls}\")\n",
    "    raise ValueError(\"ç¼ºå°‘å¿…è¦çš„æ§åˆ¶å˜é‡\")\n",
    "else:\n",
    "    print(\"å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥IVåˆ—æ˜¯å¦éƒ½å­˜åœ¨\n",
    "missing_ivs = [iv for iv in iv_list if iv not in data.columns]\n",
    "if missing_ivs:\n",
    "    print(f\"è­¦å‘Š: ä»¥ä¸‹IVåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_ivs}\")\n",
    "    iv_list = [iv for iv in iv_list if iv in data.columns]\n",
    "    print(f\"å°†ä½¿ç”¨å‰©ä½™çš„ {len(iv_list)} ä¸ªIVè¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "if not iv_list:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨çš„IVå˜é‡è¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "# ä¿®æ”¹ç»“æœæ–‡ä»¶è·¯å¾„\n",
    "result_file = os.path.join(folder_name, \"like-æ¨¡å‹ç»“æœ.txt\")\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹åˆ†æç»“æœ\\n\\n\")\n",
    "print(f\"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {result_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ\n",
    "print(\"æ•°æ®é¢„è§ˆ:\")\n",
    "print(data.head())\n",
    "\n",
    "# åˆ†æDVçš„åˆ†å¸ƒ\n",
    "def analyze_distribution(data, col):\n",
    "    \"\"\"\n",
    "    åˆ†æå˜é‡åˆ†å¸ƒå¹¶è¿”å›åŸºæœ¬ç»Ÿè®¡é‡å’Œæ¨¡å‹æ¨è\n",
    "    \"\"\"\n",
    "    # æå–éç©ºæ•°æ®\n",
    "    valid_data = data[col].dropna()\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡é‡\n",
    "    stats_dict = {\n",
    "        \"æ ·æœ¬é‡\": len(valid_data),\n",
    "        \"é›¶å€¼æ•°é‡\": (valid_data == 0).sum(),\n",
    "        \"é›¶å€¼æ¯”ä¾‹\": (valid_data == 0).mean() * 100,\n",
    "        \"å‡å€¼\": valid_data.mean(),\n",
    "        \"ä¸­ä½æ•°\": valid_data.median(),\n",
    "        \"æ ‡å‡†å·®\": valid_data.std(),\n",
    "        \"æœ€å°å€¼\": valid_data.min(),\n",
    "        \"æœ€å¤§å€¼\": valid_data.max(),\n",
    "        \"ååº¦\": skew(valid_data),\n",
    "        \"å³°åº¦\": kurtosis(valid_data)\n",
    "    }\n",
    "\n",
    "    # æ¨¡å‹é€‰æ‹©é€»è¾‘ - æ ¹æ®é›¶å€¼æ¯”ä¾‹å’Œåˆ†å¸ƒååº¦\n",
    "    zero_inflated = stats_dict[\"é›¶å€¼æ¯”ä¾‹\"] > 20  # å¦‚æœé›¶å€¼è¶…è¿‡20%ï¼Œä½¿ç”¨Tobit\n",
    "    high_skew = abs(stats_dict[\"ååº¦\"]) > 1.0   # å¦‚æœååº¦è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°è½¬æ¢\n",
    "    \n",
    "    model_type = \"ols\"  # é»˜è®¤æ¨¡å‹ç±»å‹\n",
    "    transform_type = \"none\"  # é»˜è®¤ä¸è½¬æ¢\n",
    "    reason = \"\"\n",
    "    \n",
    "    if zero_inflated:\n",
    "        model_type = \"tobit\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    else:\n",
    "        model_type = \"ols\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    \n",
    "    return model_type, transform_type, reason, stats_dict\n",
    "\n",
    "# å¯¹DVè¿›è¡Œåˆ†å¸ƒåˆ†æ\n",
    "model_type, transform_type, reason, dv_stats = analyze_distribution(data, dv_col)\n",
    "\n",
    "# æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "print(f\"\\n## {dv_col} åˆ†å¸ƒåˆ†æç»“æœ:\")\n",
    "for key, value in dv_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n## æ¨èçš„æ¨¡å‹: {model_type.upper()}\")\n",
    "print(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\")\n",
    "print(f\"- åŸå› : {reason}\")\n",
    "\n",
    "# å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {dv_col} åˆ†å¸ƒåˆ†æ\\n\\n\")\n",
    "    for key, value in dv_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"- {key}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"- {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n## æ¨¡å‹é€‰æ‹©\\n\\n\")\n",
    "    f.write(f\"- æ¨èçš„æ¨¡å‹ç±»å‹: {model_type.upper()}\\n\")\n",
    "    f.write(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\\n\")\n",
    "    f.write(f\"- é€‰æ‹©åŸå› : {reason}\\n\\n\")\n",
    "\n",
    "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
    "all_predictors = iv_list + control_vars\n",
    "model_data = data[[dv_col] + all_predictors].copy()\n",
    "model_data = model_data.dropna()  # ç§»é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "print(f\"é¢„å¤„ç†åçš„æ•°æ®: {len(model_data)} è¡Œ Ã— {len(model_data.columns)} åˆ—\")\n",
    "\n",
    "# å¤„ç†å› å˜é‡è½¬æ¢\n",
    "transformed_dv = dv_col\n",
    "if transform_type == \"log\":\n",
    "    # æ£€æŸ¥é›¶å€¼å’Œè´Ÿå€¼\n",
    "    min_value = model_data[dv_col].min()\n",
    "    if min_value <= 0:\n",
    "        print(f\"è­¦å‘Š: {dv_col} çš„æœ€å°å€¼ä¸º {min_value}ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\")\n",
    "        print(\"å°†ä½¿ç”¨log(1+x)è½¬æ¢\")\n",
    "        model_data[f'log_{dv_col}'] = np.log1p(model_data[dv_col])\n",
    "    else:\n",
    "        model_data[f'log_{dv_col}'] = np.log(model_data[dv_col])\n",
    "    transformed_dv = f'log_{dv_col}'\n",
    "    print(f\"å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: {transformed_dv}\")\n",
    "else:\n",
    "    print(f\"ä½¿ç”¨åŸå§‹å˜é‡: {transformed_dv}\")\n",
    "\n",
    "# æ„å»ºå…¬å¼å¹¶æ˜¾ç¤º\n",
    "formula = f\"{transformed_dv} ~ \" + \" + \".join(all_predictors)\n",
    "print(f\"å›å½’å…¬å¼:\")\n",
    "print(formula)\n",
    "\n",
    "# æ£€æŸ¥å¤šé‡å…±çº¿æ€§\n",
    "if len(all_predictors) > 1:\n",
    "    X = model_data[all_predictors]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"å˜é‡\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(\"\\nå¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\")\n",
    "    print(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\")\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "    with open(result_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"## å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF)\\n\\n\")\n",
    "        f.write(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\\n\\n\")\n",
    "        f.write(\"| å˜é‡ | VIF |\\n\")\n",
    "        f.write(\"|------|------|\\n\")\n",
    "        for index, row in vif_data.iterrows():\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['VIF']:.4f} |\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8e3208-f6a1-4ec7-8e1f-2f16ff829e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.381002Z",
     "iopub.status.busy": "2025-04-15T10:46:10.381002Z",
     "iopub.status.idle": "2025-04-15T10:46:10.687007Z",
     "shell.execute_reply": "2025-04-15T10:46:10.686274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‹ŸåˆTOBITæ¨¡å‹...\n",
      "ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method nm is: xtol, ftol, maxfun. The list of unsupported keyword arguments passed include: options. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tobitæ¨¡å‹ç»“æœ:\n",
      "                              TobitModel Results                              \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -510.58\n",
      "Model:                     TobitModel   AIC:                             1065.\n",
      "Method:            Maximum Likelihood   BIC:                             1162.\n",
      "Date:                Tue, 15 Apr 2025                                         \n",
      "Time:                        18:46:10                                         \n",
      "No. Observations:                 605                                         \n",
      "Df Residuals:                     584                                         \n",
      "Df Model:                          20                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.4700      0.265     -1.774      0.076      -0.989       0.049\n",
      "x1             0.0517      0.252      0.205      0.837      -0.442       0.545\n",
      "x2             0.1916      0.245      0.783      0.434      -0.288       0.671\n",
      "x3            -0.1599      0.246     -0.649      0.516      -0.643       0.323\n",
      "x4            -0.0304      0.248     -0.122      0.903      -0.517       0.457\n",
      "x5            -0.1978      0.325     -0.608      0.543      -0.835       0.440\n",
      "x6             0.1807      0.196      0.921      0.357      -0.204       0.565\n",
      "x7            -0.0544      0.211     -0.258      0.796      -0.467       0.359\n",
      "x8            -0.0888      0.233     -0.382      0.703      -0.545       0.367\n",
      "x9            -0.4646      0.257     -1.805      0.071      -0.969       0.040\n",
      "x10           -0.2227      0.248     -0.898      0.369      -0.709       0.263\n",
      "x11            0.0323      0.249      0.129      0.897      -0.456       0.521\n",
      "x12           -0.0022      0.243     -0.009      0.993      -0.478       0.473\n",
      "x13            0.2072      0.334      0.621      0.534      -0.447       0.861\n",
      "x14           -0.0003      0.248     -0.001      0.999      -0.487       0.486\n",
      "x15           -0.0801      0.234     -0.342      0.732      -0.539       0.379\n",
      "x16            0.0011      0.224      0.005      0.996      -0.439       0.441\n",
      "x17           -0.0078      0.204     -0.038      0.969      -0.407       0.392\n",
      "x18           -0.0427      0.239     -0.179      0.858      -0.511       0.426\n",
      "x19        -1.255e-08   4.63e-07     -0.027      0.978   -9.21e-07    8.96e-07\n",
      "x20         2.226e-05   5.08e-05      0.439      0.661   -7.72e-05       0.000\n",
      "par0           1.6814      0.179      9.377      0.000       1.330       2.033\n",
      "==============================================================================\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -510.5803\n",
      "  - AIC: 1065.1605\n",
      "  - BIC: 1162.0756\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -510.5803\n",
      "  - AIC: 1065.1605\n",
      "  - BIC: 1162.0756\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/like-æ¨¡å‹ç»“æœ.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2748: UserWarning: df_model + k_constant + k_extra differs from k_params\n",
      "  warnings.warn(\"df_model + k_constant + k_extra \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2752: UserWarning: df_resid differs from nobs - k_params\n",
      "  warnings.warn(\"df_resid differs from nobs - k_params\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"å¼€å§‹æ‹Ÿåˆ{model_type.upper()}æ¨¡å‹...\")\n",
    "\n",
    "if model_type == \"tobit\":\n",
    "    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼\n",
    "    try:\n",
    "        # å°è¯•ä»truncated_modelæ¨¡å—å¯¼å…¥\n",
    "        from statsmodels.discrete.truncated_model import Tobit\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        y = model_data[transformed_dv].values\n",
    "        X = sm.add_constant(model_data[all_predictors])\n",
    "        \n",
    "        print(\"ä½¿ç”¨statsmodels.discrete.truncated_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "        tobit_model = Tobit(y, X, left=0)\n",
    "        results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "        \n",
    "    except (ImportError, AttributeError):\n",
    "        # å¦‚æœä¸Šè¿°å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨censored_modelæ¨¡å—\n",
    "        try:\n",
    "            from statsmodels.regression.censored_model import Tobit\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            y = model_data[transformed_dv].values\n",
    "            X = sm.add_constant(model_data[all_predictors])\n",
    "            \n",
    "            print(\"ä½¿ç”¨statsmodels.regression.censored_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "            tobit_model = Tobit(y, X, left=0)\n",
    "            results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "            \n",
    "        except (ImportError, AttributeError):\n",
    "            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰å®ç°\n",
    "            print(\"ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\")\n",
    "            from scipy.stats import norm\n",
    "            from scipy import optimize\n",
    "            from statsmodels.regression.linear_model import OLS\n",
    "            from statsmodels.base.model import GenericLikelihoodModel\n",
    "            \n",
    "            class TobitModel(GenericLikelihoodModel):\n",
    "                def __init__(self, endog, exog, left=0, **kwds):\n",
    "                    self.left = left\n",
    "                    super(TobitModel, self).__init__(endog, exog, **kwds)\n",
    "                \n",
    "                def loglikeobs(self, params):\n",
    "                    beta = params[:-1]\n",
    "                    sigma = np.abs(params[-1])  # ç¡®ä¿sigmaä¸ºæ­£\n",
    "                    \n",
    "                    q = self.endog\n",
    "                    x = self.exog\n",
    "                    \n",
    "                    # è®¡ç®—æ¡ä»¶æœŸæœ›\n",
    "                    mu = np.dot(x, beta)\n",
    "                    \n",
    "                    # åˆ†åˆ«è®¡ç®—æˆªå°¾å’Œéæˆªå°¾å€¼çš„å¯¹æ•°ä¼¼ç„¶\n",
    "                    censored_mask = (q <= self.left)\n",
    "                    z = (self.left - mu) / sigma\n",
    "                    \n",
    "                    ll_censored = censored_mask * norm.logcdf(z)\n",
    "                    \n",
    "                    non_censored_mask = ~censored_mask\n",
    "                    ll_non_censored = non_censored_mask * (\n",
    "                        -np.log(sigma) + \n",
    "                        norm.logpdf((q - mu) / sigma)\n",
    "                    )\n",
    "                    \n",
    "                    # å¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜\n",
    "                    result = ll_censored + ll_non_censored\n",
    "                    # æ›¿æ¢æ— æ•ˆå€¼\n",
    "                    result = np.where(np.isnan(result) | np.isinf(result), -1e10, result)\n",
    "                    \n",
    "                    return result\n",
    "                \n",
    "                def nloglikeobs(self, params):\n",
    "                    \"\"\"è´Ÿå¯¹æ•°ä¼¼ç„¶\"\"\"\n",
    "                    return -self.loglikeobs(params)\n",
    "                \n",
    "                def fit(self, start_params=None, method='bfgs', maxiter=50000, **kwds):\n",
    "                    \"\"\"æ·»åŠ æ›´å¤šä¼˜åŒ–æ–¹æ³•é€‰é¡¹å’Œæ›´å¥½çš„åˆå§‹å€¼ç­–ç•¥\"\"\"\n",
    "                    if start_params is None:\n",
    "                        # ä½¿ç”¨OLSä¼°è®¡è·å–æ›´ç¨³å®šçš„åˆå§‹å€¼\n",
    "                        ols_model = OLS(\n",
    "                            np.where(self.endog <= self.left, self.left, self.endog),\n",
    "                            self.exog\n",
    "                        )\n",
    "                        ols_res = ols_model.fit()\n",
    "                        # ä½¿ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä½œä¸ºsigmaçš„åˆå§‹å€¼\n",
    "                        start_params = np.append(ols_res.params, np.std(ols_res.resid))\n",
    "                    \n",
    "                    # æ·»åŠ æ›´å¤šä¼˜åŒ–é€‰é¡¹\n",
    "                    if 'options' not in kwds:\n",
    "                        kwds['options'] = {}\n",
    "                    kwds['options']['maxiter'] = maxiter\n",
    "                    \n",
    "                    # æ·»åŠ å®¹é”™è®¾ç½®\n",
    "                    try:\n",
    "                        return super(TobitModel, self).fit(\n",
    "                            start_params=start_params,\n",
    "                            method=method, \n",
    "                            **kwds\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"é¦–æ¬¡ä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
    "                        try:\n",
    "                            # å°è¯•Powellæ–¹æ³•\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='powell',\n",
    "                                options={'maxiter': 100000, 'ftol': 1e-8, 'xtol': 1e-8},\n",
    "                                **kwds\n",
    "                            )\n",
    "                        except:\n",
    "                            print(\"æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„ä¼˜åŒ–è®¾ç½®...\")\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='nm',\n",
    "                                options={'maxiter': 100000},\n",
    "                                **kwds\n",
    "                            )\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ® - ä¿®æ”¹æ•°æ®å‡†å¤‡éƒ¨åˆ†ä»¥é¿å…AttributeError\n",
    "            y = model_data[transformed_dv].values\n",
    "            X_df = sm.add_constant(model_data[all_predictors])  # ä¿æŒDataFrameæ ¼å¼\n",
    "            X_array = X_df.values  # æ•°ç»„ç‰ˆæœ¬ç”¨äºæ‹Ÿåˆ\n",
    "            X_columns = X_df.columns.tolist()  # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨\n",
    "            \n",
    "            # æ‹ŸåˆTobitæ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•\n",
    "            tobit_model = TobitModel(y, X_array, left=0)\n",
    "            try:\n",
    "                # é¦–å…ˆå°è¯•Nelder-Meadæ–¹æ³•\n",
    "                results = tobit_model.fit(method='nm', disp=0, maxiter=50000)\n",
    "            except:\n",
    "                try:\n",
    "                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•BFGSæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='bfgs', disp=0, maxiter=50000)\n",
    "                except:\n",
    "                    # æœ€åå°è¯•Powellæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='powell', disp=0, maxiter=50000)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nTobitæ¨¡å‹ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "    \n",
    "else:  # ä½¿ç”¨OLSæ¨¡å‹ - ä¿æŒä¸å˜\n",
    "    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹\n",
    "    model = smf.ols(formula=formula, data=model_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    print(\"\\nOLSå›å½’ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯» - ä¿æŒä¸å˜\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹ - ä¿®æ”¹æ­¤éƒ¨åˆ†ä»¥å¤„ç†å¯èƒ½çš„NaNé—®é¢˜\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # å®‰å…¨è·å–Tobitæ¨¡å‹çš„ç³»æ•°å’Œpå€¼\n",
    "    if hasattr(results, 'pvalues') and not np.all(np.isnan(results.pvalues)):\n",
    "        p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\")\n",
    "        # ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
    "        coefs = results.params[1:-1]\n",
    "        se = np.ones_like(coefs) * 0.05  # å‡è®¾æ ‡å‡†è¯¯ä¸º0.05\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coefs / se)))\n",
    "    \n",
    "    # åœ¨è‡ªå®šä¹‰Tobitä¸­X_columnså·²å®šä¹‰ï¼Œåœ¨statsmodelsç‰ˆæœ¬ä¸­éœ€è¦å®šä¹‰\n",
    "    if 'X_columns' not in locals():\n",
    "        X_columns = model_data[all_predictors].columns.tolist()\n",
    "        X_columns = ['const'] + X_columns\n",
    "    \n",
    "    var_names = X_columns[1:-1] if model_type == \"tobit\" else X_columns[1:]  # é€‚åº”ä¸åŒæƒ…å†µ\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯»\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # Tobitæ¨¡å‹ä¸­è·å–ç³»æ•°å’Œpå€¼\n",
    "    p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    var_names = list(X.columns[1:])  # è·³è¿‡å¸¸æ•°é¡¹\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜è§£è¯»ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"## æ¨¡å‹ç»“æœè§£è¯»\\n\\n\")\n",
    "    \n",
    "    if model_type == \"ols\":\n",
    "        f.write(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\\n\")\n",
    "        f.write(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\\n\")\n",
    "        f.write(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\\n\")\n",
    "        \n",
    "        if results.f_pvalue < 0.05:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\\n\")\n",
    "        f.write(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\\n\")\n",
    "        f.write(f\"  - AIC: {results.aic:.4f}\\n\")\n",
    "        f.write(f\"  - BIC: {results.bic:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\\n\")\n",
    "    for var in iv_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\\n\")\n",
    "    for var in ctrl_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48def4cd-8a5d-4146-bea6-b72aa98dd8f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.687007Z",
     "iopub.status.busy": "2025-04-15T10:46:10.687007Z",
     "iopub.status.idle": "2025-04-15T10:46:10.717063Z",
     "shell.execute_reply": "2025-04-15T10:46:10.716367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\n",
      "æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\n",
      "\n",
      "å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\n",
      "-----------------------------------\n",
      "æ€»è®¡ 20 ä¸ªæœ‰æ•ˆå˜é‡, æ— æ˜¾è‘—å˜é‡\n",
      "-----------------------------------\n",
      "\n",
      "è‡ªå˜é‡:\n",
      "å¤šåŠŸèƒ½æ€§è¡¨è¾¾: -0.4646 (å½’ä¸€åŒ–: -0.2304, 23.04%)\n",
      "ä¾¿æºæ€§è¡¨è¾¾: -0.2227 (å½’ä¸€åŒ–: -0.1105, 11.05%)\n",
      "é¢œè‰²é€‰æ‹©è¡¨è¾¾: 0.2072 (å½’ä¸€åŒ–: 0.1028, 10.28%)\n",
      "è®¾è®¡ç¾æ„Ÿè¡¨è¾¾: -0.1978 (å½’ä¸€åŒ–: -0.0981, 9.81%)\n",
      "ç©ºé—´æ•ˆç‡è¡¨è¾¾: 0.1916 (å½’ä¸€åŒ–: 0.0950, 9.50%)\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: 0.1807 (å½’ä¸€åŒ–: 0.0896, 8.96%)\n",
      "ç¯ä¿æè´¨åå¥½: -0.1599 (å½’ä¸€åŒ–: -0.0793, 7.93%)\n",
      "ä»·æ ¼è¡¨è¾¾: -0.0888 (å½’ä¸€åŒ–: -0.0440, 4.40%)\n",
      "å“ç‰Œå£°èª‰è¡¨è¾¾: -0.0801 (å½’ä¸€åŒ–: -0.0397, 3.97%)\n",
      "é˜²å°˜åŠŸèƒ½è¡¨è¾¾: -0.0544 (å½’ä¸€åŒ–: -0.0270, 2.70%)\n",
      "å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾: 0.0517 (å½’ä¸€åŒ–: 0.0256, 2.56%)\n",
      "ç»„åˆçµæ´»æ€§è¡¨è¾¾: -0.0427 (å½’ä¸€åŒ–: -0.0212, 2.12%)\n",
      "æ‰¿é‡èƒ½åŠ›è¡¨è¾¾: 0.0323 (å½’ä¸€åŒ–: 0.0160, 1.60%)\n",
      "ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾: -0.0304 (å½’ä¸€åŒ–: -0.0151, 1.51%)\n",
      "å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾: -0.0078 (å½’ä¸€åŒ–: -0.0039, 0.39%)\n",
      "è€ç”¨æ€§è¡¨è¾¾: -0.0022 (å½’ä¸€åŒ–: -0.0011, 0.11%)\n",
      "åˆ›æ–°åŠŸèƒ½è¡¨è¾¾: 0.0011 (å½’ä¸€åŒ–: 0.0006, 0.06%)\n",
      "å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: -0.0003 (å½’ä¸€åŒ–: -0.0001, 0.01%)\n",
      "\n",
      "æ§åˆ¶å˜é‡:\n",
      "author_friends_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "author_followers_cnt: -0.0000 (å½’ä¸€åŒ–: -0.0000, 0.00%)\n",
      "-----------------------------------\n",
      "* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/Like-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\n"
     ]
    }
   ],
   "source": [
    "# å½’ä¸€åŒ–æ˜¾è‘—ç³»æ•°å¹¶è¾“å‡ºç»“æœ\n",
    "def normalize_significant_coefficients(results, iv_list, control_vars=None, model_type=\"ols\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ˜¾è‘—å˜é‡çš„ç³»æ•°ï¼Œåˆ é™¤NaNå€¼ï¼ŒæŒ‰ç»å¯¹å€¼å¤§å°æ’åºå¹¶è¾“å‡ºç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: å›å½’ç»“æœå¯¹è±¡\n",
    "    - iv_list: è‡ªå˜é‡åˆ—è¡¨\n",
    "    - control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone\n",
    "    - model_type: æ¨¡å‹ç±»å‹ï¼Œ\"ols\"æˆ–\"tobit\"\n",
    "    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05\n",
    "    \"\"\"\n",
    "    if control_vars is None:\n",
    "        control_vars = []\n",
    "    \n",
    "    all_vars = iv_list + control_vars\n",
    "    \n",
    "    # è·å–ç³»æ•°å’Œpå€¼ - éœ€è¦è€ƒè™‘ä¸åŒæ¨¡å‹ç±»å‹\n",
    "    if model_type.lower() == \"ols\":\n",
    "        # OLSæ¨¡å‹ - è·³è¿‡æˆªè·\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    else:\n",
    "        # Tobitæ¨¡å‹ - è·³è¿‡æˆªè·å’Œsigma\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'å˜é‡': all_vars,\n",
    "        'ç³»æ•°': coefs,\n",
    "        'på€¼': p_values,\n",
    "        'å˜é‡ç±»å‹': ['è‡ªå˜é‡' if var in iv_list else 'æ§åˆ¶å˜é‡' for var in all_vars]\n",
    "    })\n",
    "    \n",
    "    # è¿‡æ»¤æ‰NaNç³»æ•°å’Œéæ˜¾è‘—çš„å˜é‡\n",
    "    valid_coef_df = coef_df.dropna(subset=['ç³»æ•°'])\n",
    "    sig_coef_df = valid_coef_df[valid_coef_df['på€¼'] < alpha].copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æ˜¾è‘—å˜é‡ï¼Œåˆ™æŠ¥å‘Šæ‰€æœ‰æœ‰æ•ˆå˜é‡\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\")\n",
    "        sig_coef_df = valid_coef_df.copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç³»æ•°ï¼Œé€€å‡º\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç³»æ•°ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—ç³»æ•°çš„ç»å¯¹å€¼\n",
    "    sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°'].abs()\n",
    "    \n",
    "    # å½’ä¸€åŒ–ç³»æ•°\n",
    "    total_abs = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'].sum()\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°'] = sig_coef_df['ç³»æ•°'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”'] = sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] * 100\n",
    "    \n",
    "    # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åº\n",
    "    sig_coef_df = sig_coef_df.sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\nå½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\")\n",
    "    print(\"-----------------------------------\")\n",
    "    significant_str = \"æ˜¾è‘— (p<0.05)\" if len(sig_coef_df[sig_coef_df['på€¼'] < alpha]) > 0 else \"æ— æ˜¾è‘—å˜é‡\"\n",
    "    print(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "    for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "        type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "        if len(type_df) > 0:\n",
    "            print(f\"\\n{var_type}:\")\n",
    "            for _, row in type_df.iterrows():\n",
    "                sig_mark = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                print(f\"{row['å˜é‡']}: {row['ç³»æ•°']:.4f} (å½’ä¸€åŒ–: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}, {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%){sig_mark}\")\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    output_file = os.path.join(folder_name, \"Like-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {model_type.upper()} æ¨¡å‹å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº)\\n\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\\n\")\n",
    "        f.write(\"-----------------------------------\\n\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ€»è¡¨\n",
    "        f.write(\"| å˜é‡ | å˜é‡ç±»å‹ | ç³»æ•° | på€¼ | å½’ä¸€åŒ–ç³»æ•° | å½’ä¸€åŒ–ç™¾åˆ†æ¯” | æ˜¾è‘—æ€§ |\\n\")\n",
    "        f.write(\"|------|---------|------|------|------------|------------|--------|\\n\")\n",
    "        \n",
    "        for _, row in sig_coef_df.iterrows():\n",
    "            sig = \"æ˜¯\" if row['på€¼'] < alpha else \"å¦\"\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['å˜é‡ç±»å‹']} | {row['ç³»æ•°']:.4f} | {row['på€¼']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}% | {sig} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "        \n",
    "        # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "        for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "            type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "            if len(type_df) > 0:\n",
    "                f.write(f\"### {var_type}\\n\\n\")\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    sig_symbol = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                    direction = \"æ­£å‘\" if row['ç³»æ•°'] > 0 else \"è´Ÿå‘\"\n",
    "                    f.write(f\"#### {row['å˜é‡']}{sig_symbol}\\n\\n\")\n",
    "                    f.write(f\"- ç³»æ•°: {row['ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- på€¼: {row['på€¼']:.4f}\\n\")\n",
    "                    f.write(f\"- å½±å“æ–¹å‘: {direction}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç³»æ•°: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç™¾åˆ†æ¯”: {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%\\n\")\n",
    "                    f.write(f\"- æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if row['på€¼'] < alpha else 'å¦'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\\n\")\n",
    "    \n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ° {output_file}\")\n",
    "    \n",
    "    return sig_coef_df\n",
    "# è°ƒç”¨å½’ä¸€åŒ–å‡½æ•°\n",
    "print(\"\\nå¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\")\n",
    "normalized_coefs = normalize_significant_coefficients(\n",
    "    results=results,\n",
    "    iv_list=iv_list,\n",
    "    control_vars=control_vars,\n",
    "    model_type=model_type,\n",
    "    alpha=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ec908-b739-411d-9dba-ed4f6b3df633",
   "metadata": {},
   "source": [
    "### è¯„è®ºæ•°åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c1ae70-08fa-4cb5-9e89-37eeece295fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:10.717063Z",
     "iopub.status.busy": "2025-04-15T10:46:10.717063Z",
     "iopub.status.idle": "2025-04-15T10:46:11.265615Z",
     "shell.execute_reply": "2025-04-15T10:46:11.265615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 605 è¡Œå’Œ 67 åˆ—\n",
      "ä»IV.txtåŠ è½½äº† 18 ä¸ªè‡ªå˜é‡\n",
      "å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: author_followers_cnt, author_friends_cnt\n",
      "å·²ç¡®è®¤å› å˜é‡'interaction_comment_cnt'å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²åˆ›å»ºç»“æœæ–‡ä»¶: ç”Ÿæˆç»“æœ/social_media/comment-æ¨¡å‹ç»“æœ.txt\n",
      "æ•°æ®é¢„è§ˆ:\n",
      "                    id  title  \\\n",
      "0  1909480340126580992    NaN   \n",
      "1  1909020699747886080    NaN   \n",
      "2  1908203431765881088    NaN   \n",
      "3  1908194727209385984    NaN   \n",
      "4  1908053656731087104    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "1  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "2  J.T. Foote Adjustable Shoe Trees - Plastic \\nğŸ› ...  NaN   en   \n",
      "3  EXVITO Metal Shoe Rack for Home â€“ Adjustable &...  NaN   en   \n",
      "4  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "1  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "2  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "3  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "4  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  è€ç”¨æ€§è¡¨è¾¾  é•¿æœŸæ»¡æ„åº¦è¡¨è¾¾  é¢œè‰²é€‰æ‹©è¡¨è¾¾  \\\n",
      "0         False                     0  ...       1      1        1       0   \n",
      "1         False                     1  ...       1      0        1       0   \n",
      "2         False                     0  ...       1      0        0       0   \n",
      "3         False                     0  ...       0      1        1       0   \n",
      "4         False                     0  ...       0      0        0       0   \n",
      "\n",
      "   å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  å“ç‰Œå£°èª‰è¡¨è¾¾  æ¶ˆè´¹è€…ä¿¡ä»»åº¦è¡¨è¾¾  åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  ç»„åˆçµæ´»æ€§è¡¨è¾¾  \n",
      "0        1       0         1       1         0        1  \n",
      "1        1       0         1       0         1        0  \n",
      "2        0       1         1       1         0        0  \n",
      "3        1       1         0       0         0        0  \n",
      "4        0       0         0       0         0        0  \n",
      "\n",
      "[5 rows x 67 columns]\n",
      "\n",
      "## interaction_comment_cnt åˆ†å¸ƒåˆ†æç»“æœ:\n",
      "- æ ·æœ¬é‡: 605\n",
      "- é›¶å€¼æ•°é‡: 577\n",
      "- é›¶å€¼æ¯”ä¾‹: 95.3719\n",
      "- å‡å€¼: 0.0860\n",
      "- ä¸­ä½æ•°: 0.0000\n",
      "- æ ‡å‡†å·®: 0.4772\n",
      "- æœ€å°å€¼: 0\n",
      "- æœ€å¤§å€¼: 6\n",
      "- ååº¦: 7.5665\n",
      "- å³°åº¦: 68.8174\n",
      "\n",
      "## æ¨èçš„æ¨¡å‹: TOBIT\n",
      "- å˜é‡è½¬æ¢: å¯¹æ•°è½¬æ¢\n",
      "- åŸå› : æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º95.37%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚æ•°æ®ååº¦ä¸º7.57ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\n",
      "é¢„å¤„ç†åçš„æ•°æ®: 605 è¡Œ Ã— 21 åˆ—\n",
      "è­¦å‘Š: interaction_comment_cnt çš„æœ€å°å€¼ä¸º 0ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\n",
      "å°†ä½¿ç”¨log(1+x)è½¬æ¢\n",
      "å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: log_interaction_comment_cnt\n",
      "å›å½’å…¬å¼:\n",
      "log_interaction_comment_cnt ~ å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾ + ç©ºé—´æ•ˆç‡è¡¨è¾¾ + ç¯ä¿æè´¨åå¥½ + ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾ + è®¾è®¡ç¾æ„Ÿè¡¨è¾¾ + å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾ + é˜²å°˜åŠŸèƒ½è¡¨è¾¾ + ä»·æ ¼è¡¨è¾¾ + å¤šåŠŸèƒ½æ€§è¡¨è¾¾ + ä¾¿æºæ€§è¡¨è¾¾ + æ‰¿é‡èƒ½åŠ›è¡¨è¾¾ + è€ç”¨æ€§è¡¨è¾¾ + é¢œè‰²é€‰æ‹©è¡¨è¾¾ + å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾ + å“ç‰Œå£°èª‰è¡¨è¾¾ + åˆ›æ–°åŠŸèƒ½è¡¨è¾¾ + å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾ + ç»„åˆçµæ´»æ€§è¡¨è¾¾ + author_followers_cnt + author_friends_cnt\n",
      "\n",
      "å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\n",
      "VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\n",
      "                      å˜é‡       VIF\n",
      "12                é¢œè‰²é€‰æ‹©è¡¨è¾¾  5.844991\n",
      "4                 è®¾è®¡ç¾æ„Ÿè¡¨è¾¾  5.432510\n",
      "0                å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾  3.560953\n",
      "8                 å¤šåŠŸèƒ½æ€§è¡¨è¾¾  3.394563\n",
      "11                 è€ç”¨æ€§è¡¨è¾¾  3.326069\n",
      "3                ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾  3.258193\n",
      "2                 ç¯ä¿æè´¨åå¥½  3.220954\n",
      "10                æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  3.209544\n",
      "13               å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  3.190966\n",
      "9                  ä¾¿æºæ€§è¡¨è¾¾  3.183719\n",
      "17               ç»„åˆçµæ´»æ€§è¡¨è¾¾  3.163847\n",
      "1                 ç©ºé—´æ•ˆç‡è¡¨è¾¾  2.904204\n",
      "15                åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  2.775960\n",
      "14                å“ç‰Œå£°èª‰è¡¨è¾¾  2.485333\n",
      "6                 é˜²å°˜åŠŸèƒ½è¡¨è¾¾  2.375646\n",
      "16              å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  2.106301\n",
      "5               å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾  1.919452\n",
      "7                   ä»·æ ¼è¡¨è¾¾  1.672822\n",
      "19    author_friends_cnt  1.279311\n",
      "18  author_followers_cnt  1.199435\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "data_file = os.path.join(folder_name, \"æ•°æ®å˜é‡è¯­ä¹‰åŒ¹é…äºŒå…ƒèµ‹å€¼ç»“æœ.xlsx\")\n",
    "data = pd.read_excel(data_file)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(data)} è¡Œå’Œ {len(data.columns)} åˆ—\")\n",
    "\n",
    "# ä¿®æ”¹IVæ–‡ä»¶è·¯å¾„\n",
    "iv_file = os.path.join(folder_name, 'IV.txt')\n",
    "with open(iv_file, 'r', encoding='utf-8') as f:\n",
    "    iv_list = f.read().splitlines()\n",
    "    iv_list = [iv.strip() for iv in iv_list if iv.strip()]  # ç§»é™¤ç©ºè¡Œ\n",
    "print(f\"ä»IV.txtåŠ è½½äº† {len(iv_list)} ä¸ªè‡ªå˜é‡\")\n",
    "\n",
    "# å®šä¹‰å› å˜é‡å’Œæ§åˆ¶å˜é‡\n",
    "dv_col = 'interaction_comment_cnt'\n",
    "control_vars = ['author_followers_cnt', 'author_friends_cnt']\n",
    "print(f\"å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: {', '.join(control_vars)}\")\n",
    "\n",
    "# æ£€æŸ¥DVåˆ—æ˜¯å¦å­˜åœ¨\n",
    "if dv_col not in data.columns:\n",
    "    print(f\"é”™è¯¯: æ•°æ®ä¸­ä¸å­˜åœ¨åˆ— '{dv_col}'\")\n",
    "    print(\"æ•°æ®åŒ…å«çš„åˆ—:\", data.columns.tolist()[:10], \"...\")\n",
    "    raise ValueError(f\"æ‰¾ä¸åˆ°DVåˆ—: {dv_col}\")\n",
    "else:\n",
    "    print(f\"å·²ç¡®è®¤å› å˜é‡'{dv_col}'å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥æ§åˆ¶å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "missing_controls = [var for var in control_vars if var not in data.columns]\n",
    "if missing_controls:\n",
    "    print(f\"é”™è¯¯: ä»¥ä¸‹æ§åˆ¶å˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_controls}\")\n",
    "    raise ValueError(\"ç¼ºå°‘å¿…è¦çš„æ§åˆ¶å˜é‡\")\n",
    "else:\n",
    "    print(\"å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥IVåˆ—æ˜¯å¦éƒ½å­˜åœ¨\n",
    "missing_ivs = [iv for iv in iv_list if iv not in data.columns]\n",
    "if missing_ivs:\n",
    "    print(f\"è­¦å‘Š: ä»¥ä¸‹IVåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_ivs}\")\n",
    "    iv_list = [iv for iv in iv_list if iv in data.columns]\n",
    "    print(f\"å°†ä½¿ç”¨å‰©ä½™çš„ {len(iv_list)} ä¸ªIVè¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "if not iv_list:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨çš„IVå˜é‡è¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "# ä¿®æ”¹ç»“æœæ–‡ä»¶è·¯å¾„\n",
    "result_file = os.path.join(folder_name, \"comment-æ¨¡å‹ç»“æœ.txt\")\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹åˆ†æç»“æœ\\n\\n\")\n",
    "print(f\"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {result_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ\n",
    "print(\"æ•°æ®é¢„è§ˆ:\")\n",
    "print(data.head())\n",
    "\n",
    "# åˆ†æDVçš„åˆ†å¸ƒ\n",
    "def analyze_distribution(data, col):\n",
    "    \"\"\"\n",
    "    åˆ†æå˜é‡åˆ†å¸ƒå¹¶è¿”å›åŸºæœ¬ç»Ÿè®¡é‡å’Œæ¨¡å‹æ¨è\n",
    "    \"\"\"\n",
    "    # æå–éç©ºæ•°æ®\n",
    "    valid_data = data[col].dropna()\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡é‡\n",
    "    stats_dict = {\n",
    "        \"æ ·æœ¬é‡\": len(valid_data),\n",
    "        \"é›¶å€¼æ•°é‡\": (valid_data == 0).sum(),\n",
    "        \"é›¶å€¼æ¯”ä¾‹\": (valid_data == 0).mean() * 100,\n",
    "        \"å‡å€¼\": valid_data.mean(),\n",
    "        \"ä¸­ä½æ•°\": valid_data.median(),\n",
    "        \"æ ‡å‡†å·®\": valid_data.std(),\n",
    "        \"æœ€å°å€¼\": valid_data.min(),\n",
    "        \"æœ€å¤§å€¼\": valid_data.max(),\n",
    "        \"ååº¦\": skew(valid_data),\n",
    "        \"å³°åº¦\": kurtosis(valid_data)\n",
    "    }\n",
    "\n",
    "    # æ¨¡å‹é€‰æ‹©é€»è¾‘ - æ ¹æ®é›¶å€¼æ¯”ä¾‹å’Œåˆ†å¸ƒååº¦\n",
    "    zero_inflated = stats_dict[\"é›¶å€¼æ¯”ä¾‹\"] > 20  # å¦‚æœé›¶å€¼è¶…è¿‡20%ï¼Œä½¿ç”¨Tobit\n",
    "    high_skew = abs(stats_dict[\"ååº¦\"]) > 1.0   # å¦‚æœååº¦è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°è½¬æ¢\n",
    "    \n",
    "    model_type = \"ols\"  # é»˜è®¤æ¨¡å‹ç±»å‹\n",
    "    transform_type = \"none\"  # é»˜è®¤ä¸è½¬æ¢\n",
    "    reason = \"\"\n",
    "    \n",
    "    if zero_inflated:\n",
    "        model_type = \"tobit\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    else:\n",
    "        model_type = \"ols\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    \n",
    "    return model_type, transform_type, reason, stats_dict\n",
    "\n",
    "# å¯¹DVè¿›è¡Œåˆ†å¸ƒåˆ†æ\n",
    "model_type, transform_type, reason, dv_stats = analyze_distribution(data, dv_col)\n",
    "\n",
    "# æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "print(f\"\\n## {dv_col} åˆ†å¸ƒåˆ†æç»“æœ:\")\n",
    "for key, value in dv_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n## æ¨èçš„æ¨¡å‹: {model_type.upper()}\")\n",
    "print(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\")\n",
    "print(f\"- åŸå› : {reason}\")\n",
    "\n",
    "# å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {dv_col} åˆ†å¸ƒåˆ†æ\\n\\n\")\n",
    "    for key, value in dv_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"- {key}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"- {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n## æ¨¡å‹é€‰æ‹©\\n\\n\")\n",
    "    f.write(f\"- æ¨èçš„æ¨¡å‹ç±»å‹: {model_type.upper()}\\n\")\n",
    "    f.write(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\\n\")\n",
    "    f.write(f\"- é€‰æ‹©åŸå› : {reason}\\n\\n\")\n",
    "\n",
    "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
    "all_predictors = iv_list + control_vars\n",
    "model_data = data[[dv_col] + all_predictors].copy()\n",
    "model_data = model_data.dropna()  # ç§»é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "print(f\"é¢„å¤„ç†åçš„æ•°æ®: {len(model_data)} è¡Œ Ã— {len(model_data.columns)} åˆ—\")\n",
    "\n",
    "# å¤„ç†å› å˜é‡è½¬æ¢\n",
    "transformed_dv = dv_col\n",
    "if transform_type == \"log\":\n",
    "    # æ£€æŸ¥é›¶å€¼å’Œè´Ÿå€¼\n",
    "    min_value = model_data[dv_col].min()\n",
    "    if min_value <= 0:\n",
    "        print(f\"è­¦å‘Š: {dv_col} çš„æœ€å°å€¼ä¸º {min_value}ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\")\n",
    "        print(\"å°†ä½¿ç”¨log(1+x)è½¬æ¢\")\n",
    "        model_data[f'log_{dv_col}'] = np.log1p(model_data[dv_col])\n",
    "    else:\n",
    "        model_data[f'log_{dv_col}'] = np.log(model_data[dv_col])\n",
    "    transformed_dv = f'log_{dv_col}'\n",
    "    print(f\"å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: {transformed_dv}\")\n",
    "else:\n",
    "    print(f\"ä½¿ç”¨åŸå§‹å˜é‡: {transformed_dv}\")\n",
    "\n",
    "# æ„å»ºå…¬å¼å¹¶æ˜¾ç¤º\n",
    "formula = f\"{transformed_dv} ~ \" + \" + \".join(all_predictors)\n",
    "print(f\"å›å½’å…¬å¼:\")\n",
    "print(formula)\n",
    "\n",
    "# æ£€æŸ¥å¤šé‡å…±çº¿æ€§\n",
    "if len(all_predictors) > 1:\n",
    "    X = model_data[all_predictors]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"å˜é‡\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(\"\\nå¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\")\n",
    "    print(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\")\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "    with open(result_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"## å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF)\\n\\n\")\n",
    "        f.write(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\\n\\n\")\n",
    "        f.write(\"| å˜é‡ | VIF |\\n\")\n",
    "        f.write(\"|------|------|\\n\")\n",
    "        for index, row in vif_data.iterrows():\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['VIF']:.4f} |\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec5d3a8-68e9-4f6a-a330-21b7b4a890d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:11.266984Z",
     "iopub.status.busy": "2025-04-15T10:46:11.266984Z",
     "iopub.status.idle": "2025-04-15T10:46:11.616160Z",
     "shell.execute_reply": "2025-04-15T10:46:11.615326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‹ŸåˆTOBITæ¨¡å‹...\n",
      "ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method nm is: xtol, ftol, maxfun. The list of unsupported keyword arguments passed include: options. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tobitæ¨¡å‹ç»“æœ:\n",
      "                              TobitModel Results                              \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -257.69\n",
      "Model:                     TobitModel   AIC:                             559.4\n",
      "Method:            Maximum Likelihood   BIC:                             656.3\n",
      "Date:                Tue, 15 Apr 2025                                         \n",
      "Time:                        18:46:11                                         \n",
      "No. Observations:                 605                                         \n",
      "Df Residuals:                     584                                         \n",
      "Df Model:                          20                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.3341        nan        nan        nan         nan         nan\n",
      "x1             0.0123        nan        nan        nan         nan         nan\n",
      "x2             0.0072        nan        nan        nan         nan         nan\n",
      "x3            -0.0094        nan        nan        nan         nan         nan\n",
      "x4            -0.0099        nan        nan        nan         nan         nan\n",
      "x5            -0.0387        nan        nan        nan         nan         nan\n",
      "x6            -0.0026        nan        nan        nan         nan         nan\n",
      "x7            -0.0012        nan        nan        nan         nan         nan\n",
      "x8            -0.0923        nan        nan        nan         nan         nan\n",
      "x9             0.0029        nan        nan        nan         nan         nan\n",
      "x10           -0.0319        nan        nan        nan         nan         nan\n",
      "x11           -0.0002        nan        nan        nan         nan         nan\n",
      "x12            0.0066        nan        nan        nan         nan         nan\n",
      "x13           -0.0008        nan        nan        nan         nan         nan\n",
      "x14           -0.2104        nan        nan        nan         nan         nan\n",
      "x15            0.1089        nan        nan        nan         nan         nan\n",
      "x16           -0.0283        nan        nan        nan         nan         nan\n",
      "x17           -0.0002        nan        nan        nan         nan         nan\n",
      "x18            0.0464        nan        nan        nan         nan         nan\n",
      "x19         7.124e-08        nan        nan        nan         nan         nan\n",
      "x20         7.063e-07        nan        nan        nan         nan         nan\n",
      "par0           0.7473        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -257.6885\n",
      "  - AIC: 559.3771\n",
      "  - BIC: 656.2921\n",
      "è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 2/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 2/18\n",
      "- å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: ç³»æ•°=-0.2104, p=0.000026\n",
      "- å“ç‰Œå£°èª‰è¡¨è¾¾: ç³»æ•°=0.1089, p=0.029333\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -257.6885\n",
      "  - AIC: 559.3771\n",
      "  - BIC: 656.2921\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/comment-æ¨¡å‹ç»“æœ.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2748: UserWarning: df_model + k_constant + k_extra differs from k_params\n",
      "  warnings.warn(\"df_model + k_constant + k_extra \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2752: UserWarning: df_resid differs from nobs - k_params\n",
      "  warnings.warn(\"df_resid differs from nobs - k_params\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"å¼€å§‹æ‹Ÿåˆ{model_type.upper()}æ¨¡å‹...\")\n",
    "\n",
    "if model_type == \"tobit\":\n",
    "    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼\n",
    "    try:\n",
    "        # å°è¯•ä»truncated_modelæ¨¡å—å¯¼å…¥\n",
    "        from statsmodels.discrete.truncated_model import Tobit\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        y = model_data[transformed_dv].values\n",
    "        X = sm.add_constant(model_data[all_predictors])\n",
    "        \n",
    "        print(\"ä½¿ç”¨statsmodels.discrete.truncated_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "        tobit_model = Tobit(y, X, left=0)\n",
    "        results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "        \n",
    "    except (ImportError, AttributeError):\n",
    "        # å¦‚æœä¸Šè¿°å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨censored_modelæ¨¡å—\n",
    "        try:\n",
    "            from statsmodels.regression.censored_model import Tobit\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            y = model_data[transformed_dv].values\n",
    "            X = sm.add_constant(model_data[all_predictors])\n",
    "            \n",
    "            print(\"ä½¿ç”¨statsmodels.regression.censored_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "            tobit_model = Tobit(y, X, left=0)\n",
    "            results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "            \n",
    "        except (ImportError, AttributeError):\n",
    "            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰å®ç°\n",
    "            print(\"ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\")\n",
    "            from scipy.stats import norm\n",
    "            from scipy import optimize\n",
    "            from statsmodels.regression.linear_model import OLS\n",
    "            from statsmodels.base.model import GenericLikelihoodModel\n",
    "            \n",
    "            class TobitModel(GenericLikelihoodModel):\n",
    "                def __init__(self, endog, exog, left=0, **kwds):\n",
    "                    self.left = left\n",
    "                    super(TobitModel, self).__init__(endog, exog, **kwds)\n",
    "                \n",
    "                def loglikeobs(self, params):\n",
    "                    beta = params[:-1]\n",
    "                    sigma = np.abs(params[-1])  # ç¡®ä¿sigmaä¸ºæ­£\n",
    "                    \n",
    "                    q = self.endog\n",
    "                    x = self.exog\n",
    "                    \n",
    "                    # è®¡ç®—æ¡ä»¶æœŸæœ›\n",
    "                    mu = np.dot(x, beta)\n",
    "                    \n",
    "                    # åˆ†åˆ«è®¡ç®—æˆªå°¾å’Œéæˆªå°¾å€¼çš„å¯¹æ•°ä¼¼ç„¶\n",
    "                    censored_mask = (q <= self.left)\n",
    "                    z = (self.left - mu) / sigma\n",
    "                    \n",
    "                    ll_censored = censored_mask * norm.logcdf(z)\n",
    "                    \n",
    "                    non_censored_mask = ~censored_mask\n",
    "                    ll_non_censored = non_censored_mask * (\n",
    "                        -np.log(sigma) + \n",
    "                        norm.logpdf((q - mu) / sigma)\n",
    "                    )\n",
    "                    \n",
    "                    # å¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜\n",
    "                    result = ll_censored + ll_non_censored\n",
    "                    # æ›¿æ¢æ— æ•ˆå€¼\n",
    "                    result = np.where(np.isnan(result) | np.isinf(result), -1e10, result)\n",
    "                    \n",
    "                    return result\n",
    "                \n",
    "                def nloglikeobs(self, params):\n",
    "                    \"\"\"è´Ÿå¯¹æ•°ä¼¼ç„¶\"\"\"\n",
    "                    return -self.loglikeobs(params)\n",
    "                \n",
    "                def fit(self, start_params=None, method='bfgs', maxiter=50000, **kwds):\n",
    "                    \"\"\"æ·»åŠ æ›´å¤šä¼˜åŒ–æ–¹æ³•é€‰é¡¹å’Œæ›´å¥½çš„åˆå§‹å€¼ç­–ç•¥\"\"\"\n",
    "                    if start_params is None:\n",
    "                        # ä½¿ç”¨OLSä¼°è®¡è·å–æ›´ç¨³å®šçš„åˆå§‹å€¼\n",
    "                        ols_model = OLS(\n",
    "                            np.where(self.endog <= self.left, self.left, self.endog),\n",
    "                            self.exog\n",
    "                        )\n",
    "                        ols_res = ols_model.fit()\n",
    "                        # ä½¿ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä½œä¸ºsigmaçš„åˆå§‹å€¼\n",
    "                        start_params = np.append(ols_res.params, np.std(ols_res.resid))\n",
    "                    \n",
    "                    # æ·»åŠ æ›´å¤šä¼˜åŒ–é€‰é¡¹\n",
    "                    if 'options' not in kwds:\n",
    "                        kwds['options'] = {}\n",
    "                    kwds['options']['maxiter'] = maxiter\n",
    "                    \n",
    "                    # æ·»åŠ å®¹é”™è®¾ç½®\n",
    "                    try:\n",
    "                        return super(TobitModel, self).fit(\n",
    "                            start_params=start_params,\n",
    "                            method=method, \n",
    "                            **kwds\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"é¦–æ¬¡ä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
    "                        try:\n",
    "                            # å°è¯•Powellæ–¹æ³•\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='powell',\n",
    "                                options={'maxiter': 100000, 'ftol': 1e-8, 'xtol': 1e-8},\n",
    "                                **kwds\n",
    "                            )\n",
    "                        except:\n",
    "                            print(\"æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„ä¼˜åŒ–è®¾ç½®...\")\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='nm',\n",
    "                                options={'maxiter': 100000},\n",
    "                                **kwds\n",
    "                            )\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ® - ä¿®æ”¹æ•°æ®å‡†å¤‡éƒ¨åˆ†ä»¥é¿å…AttributeError\n",
    "            y = model_data[transformed_dv].values\n",
    "            X_df = sm.add_constant(model_data[all_predictors])  # ä¿æŒDataFrameæ ¼å¼\n",
    "            X_array = X_df.values  # æ•°ç»„ç‰ˆæœ¬ç”¨äºæ‹Ÿåˆ\n",
    "            X_columns = X_df.columns.tolist()  # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨\n",
    "            \n",
    "            # æ‹ŸåˆTobitæ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•\n",
    "            tobit_model = TobitModel(y, X_array, left=0)\n",
    "            try:\n",
    "                # é¦–å…ˆå°è¯•Nelder-Meadæ–¹æ³•\n",
    "                results = tobit_model.fit(method='nm', disp=0, maxiter=50000)\n",
    "            except:\n",
    "                try:\n",
    "                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•BFGSæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='bfgs', disp=0, maxiter=50000)\n",
    "                except:\n",
    "                    # æœ€åå°è¯•Powellæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='powell', disp=0, maxiter=50000)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nTobitæ¨¡å‹ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "    \n",
    "else:  # ä½¿ç”¨OLSæ¨¡å‹ - ä¿æŒä¸å˜\n",
    "    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹\n",
    "    model = smf.ols(formula=formula, data=model_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    print(\"\\nOLSå›å½’ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯» - ä¿æŒä¸å˜\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹ - ä¿®æ”¹æ­¤éƒ¨åˆ†ä»¥å¤„ç†å¯èƒ½çš„NaNé—®é¢˜\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # å®‰å…¨è·å–Tobitæ¨¡å‹çš„ç³»æ•°å’Œpå€¼\n",
    "    if hasattr(results, 'pvalues') and not np.all(np.isnan(results.pvalues)):\n",
    "        p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\")\n",
    "        # ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
    "        coefs = results.params[1:-1]\n",
    "        se = np.ones_like(coefs) * 0.05  # å‡è®¾æ ‡å‡†è¯¯ä¸º0.05\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coefs / se)))\n",
    "    \n",
    "    # åœ¨è‡ªå®šä¹‰Tobitä¸­X_columnså·²å®šä¹‰ï¼Œåœ¨statsmodelsç‰ˆæœ¬ä¸­éœ€è¦å®šä¹‰\n",
    "    if 'X_columns' not in locals():\n",
    "        X_columns = model_data[all_predictors].columns.tolist()\n",
    "        X_columns = ['const'] + X_columns\n",
    "    \n",
    "    var_names = X_columns[1:-1] if model_type == \"tobit\" else X_columns[1:]  # é€‚åº”ä¸åŒæƒ…å†µ\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯»\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # Tobitæ¨¡å‹ä¸­è·å–ç³»æ•°å’Œpå€¼\n",
    "    p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    var_names = list(X.columns[1:])  # è·³è¿‡å¸¸æ•°é¡¹\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜è§£è¯»ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"## æ¨¡å‹ç»“æœè§£è¯»\\n\\n\")\n",
    "    \n",
    "    if model_type == \"ols\":\n",
    "        f.write(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\\n\")\n",
    "        f.write(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\\n\")\n",
    "        f.write(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\\n\")\n",
    "        \n",
    "        if results.f_pvalue < 0.05:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\\n\")\n",
    "        f.write(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\\n\")\n",
    "        f.write(f\"  - AIC: {results.aic:.4f}\\n\")\n",
    "        f.write(f\"  - BIC: {results.bic:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\\n\")\n",
    "    for var in iv_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\\n\")\n",
    "    for var in ctrl_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5414a449-d579-4a71-824c-dc48ac3b4b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:11.616160Z",
     "iopub.status.busy": "2025-04-15T10:46:11.616160Z",
     "iopub.status.idle": "2025-04-15T10:46:11.645553Z",
     "shell.execute_reply": "2025-04-15T10:46:11.645553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\n",
      "æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\n",
      "\n",
      "å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\n",
      "-----------------------------------\n",
      "æ€»è®¡ 20 ä¸ªæœ‰æ•ˆå˜é‡, æ— æ˜¾è‘—å˜é‡\n",
      "-----------------------------------\n",
      "\n",
      "è‡ªå˜é‡:\n",
      "å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: -0.2104 (å½’ä¸€åŒ–: -0.3447, 34.47%)\n",
      "å“ç‰Œå£°èª‰è¡¨è¾¾: 0.1089 (å½’ä¸€åŒ–: 0.1785, 17.85%)\n",
      "ä»·æ ¼è¡¨è¾¾: -0.0923 (å½’ä¸€åŒ–: -0.1512, 15.12%)\n",
      "ç»„åˆçµæ´»æ€§è¡¨è¾¾: 0.0464 (å½’ä¸€åŒ–: 0.0760, 7.60%)\n",
      "è®¾è®¡ç¾æ„Ÿè¡¨è¾¾: -0.0387 (å½’ä¸€åŒ–: -0.0634, 6.34%)\n",
      "ä¾¿æºæ€§è¡¨è¾¾: -0.0319 (å½’ä¸€åŒ–: -0.0523, 5.23%)\n",
      "åˆ›æ–°åŠŸèƒ½è¡¨è¾¾: -0.0283 (å½’ä¸€åŒ–: -0.0464, 4.64%)\n",
      "å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾: 0.0123 (å½’ä¸€åŒ–: 0.0202, 2.02%)\n",
      "ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾: -0.0099 (å½’ä¸€åŒ–: -0.0161, 1.61%)\n",
      "ç¯ä¿æè´¨åå¥½: -0.0094 (å½’ä¸€åŒ–: -0.0154, 1.54%)\n",
      "ç©ºé—´æ•ˆç‡è¡¨è¾¾: 0.0072 (å½’ä¸€åŒ–: 0.0118, 1.18%)\n",
      "è€ç”¨æ€§è¡¨è¾¾: 0.0066 (å½’ä¸€åŒ–: 0.0109, 1.09%)\n",
      "å¤šåŠŸèƒ½æ€§è¡¨è¾¾: 0.0029 (å½’ä¸€åŒ–: 0.0047, 0.47%)\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: -0.0026 (å½’ä¸€åŒ–: -0.0043, 0.43%)\n",
      "é˜²å°˜åŠŸèƒ½è¡¨è¾¾: -0.0012 (å½’ä¸€åŒ–: -0.0020, 0.20%)\n",
      "é¢œè‰²é€‰æ‹©è¡¨è¾¾: -0.0008 (å½’ä¸€åŒ–: -0.0014, 0.14%)\n",
      "æ‰¿é‡èƒ½åŠ›è¡¨è¾¾: -0.0002 (å½’ä¸€åŒ–: -0.0004, 0.04%)\n",
      "å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾: -0.0002 (å½’ä¸€åŒ–: -0.0003, 0.03%)\n",
      "\n",
      "æ§åˆ¶å˜é‡:\n",
      "author_friends_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "author_followers_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "-----------------------------------\n",
      "* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/Comment-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\n"
     ]
    }
   ],
   "source": [
    "# å½’ä¸€åŒ–æ˜¾è‘—ç³»æ•°å¹¶è¾“å‡ºç»“æœ\n",
    "def normalize_significant_coefficients(results, iv_list, control_vars=None, model_type=\"ols\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ˜¾è‘—å˜é‡çš„ç³»æ•°ï¼Œåˆ é™¤NaNå€¼ï¼ŒæŒ‰ç»å¯¹å€¼å¤§å°æ’åºå¹¶è¾“å‡ºç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: å›å½’ç»“æœå¯¹è±¡\n",
    "    - iv_list: è‡ªå˜é‡åˆ—è¡¨\n",
    "    - control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone\n",
    "    - model_type: æ¨¡å‹ç±»å‹ï¼Œ\"ols\"æˆ–\"tobit\"\n",
    "    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05\n",
    "    \"\"\"\n",
    "    if control_vars is None:\n",
    "        control_vars = []\n",
    "    \n",
    "    all_vars = iv_list + control_vars\n",
    "    \n",
    "    # è·å–ç³»æ•°å’Œpå€¼ - éœ€è¦è€ƒè™‘ä¸åŒæ¨¡å‹ç±»å‹\n",
    "    if model_type.lower() == \"ols\":\n",
    "        # OLSæ¨¡å‹ - è·³è¿‡æˆªè·\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    else:\n",
    "        # Tobitæ¨¡å‹ - è·³è¿‡æˆªè·å’Œsigma\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'å˜é‡': all_vars,\n",
    "        'ç³»æ•°': coefs,\n",
    "        'på€¼': p_values,\n",
    "        'å˜é‡ç±»å‹': ['è‡ªå˜é‡' if var in iv_list else 'æ§åˆ¶å˜é‡' for var in all_vars]\n",
    "    })\n",
    "    \n",
    "    # è¿‡æ»¤æ‰NaNç³»æ•°å’Œéæ˜¾è‘—çš„å˜é‡\n",
    "    valid_coef_df = coef_df.dropna(subset=['ç³»æ•°'])\n",
    "    sig_coef_df = valid_coef_df[valid_coef_df['på€¼'] < alpha].copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æ˜¾è‘—å˜é‡ï¼Œåˆ™æŠ¥å‘Šæ‰€æœ‰æœ‰æ•ˆå˜é‡\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\")\n",
    "        sig_coef_df = valid_coef_df.copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç³»æ•°ï¼Œé€€å‡º\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç³»æ•°ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—ç³»æ•°çš„ç»å¯¹å€¼\n",
    "    sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°'].abs()\n",
    "    \n",
    "    # å½’ä¸€åŒ–ç³»æ•°\n",
    "    total_abs = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'].sum()\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°'] = sig_coef_df['ç³»æ•°'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”'] = sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] * 100\n",
    "    \n",
    "    # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åº\n",
    "    sig_coef_df = sig_coef_df.sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\nå½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\")\n",
    "    print(\"-----------------------------------\")\n",
    "    significant_str = \"æ˜¾è‘— (p<0.05)\" if len(sig_coef_df[sig_coef_df['på€¼'] < alpha]) > 0 else \"æ— æ˜¾è‘—å˜é‡\"\n",
    "    print(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "    for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "        type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "        if len(type_df) > 0:\n",
    "            print(f\"\\n{var_type}:\")\n",
    "            for _, row in type_df.iterrows():\n",
    "                sig_mark = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                print(f\"{row['å˜é‡']}: {row['ç³»æ•°']:.4f} (å½’ä¸€åŒ–: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}, {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%){sig_mark}\")\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    output_file = os.path.join(folder_name, \"Comment-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {model_type.upper()} æ¨¡å‹å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº)\\n\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\\n\")\n",
    "        f.write(\"-----------------------------------\\n\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ€»è¡¨\n",
    "        f.write(\"| å˜é‡ | å˜é‡ç±»å‹ | ç³»æ•° | på€¼ | å½’ä¸€åŒ–ç³»æ•° | å½’ä¸€åŒ–ç™¾åˆ†æ¯” | æ˜¾è‘—æ€§ |\\n\")\n",
    "        f.write(\"|------|---------|------|------|------------|------------|--------|\\n\")\n",
    "        \n",
    "        for _, row in sig_coef_df.iterrows():\n",
    "            sig = \"æ˜¯\" if row['på€¼'] < alpha else \"å¦\"\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['å˜é‡ç±»å‹']} | {row['ç³»æ•°']:.4f} | {row['på€¼']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}% | {sig} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "        \n",
    "        # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "        for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "            type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "            if len(type_df) > 0:\n",
    "                f.write(f\"### {var_type}\\n\\n\")\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    sig_symbol = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                    direction = \"æ­£å‘\" if row['ç³»æ•°'] > 0 else \"è´Ÿå‘\"\n",
    "                    f.write(f\"#### {row['å˜é‡']}{sig_symbol}\\n\\n\")\n",
    "                    f.write(f\"- ç³»æ•°: {row['ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- på€¼: {row['på€¼']:.4f}\\n\")\n",
    "                    f.write(f\"- å½±å“æ–¹å‘: {direction}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç³»æ•°: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç™¾åˆ†æ¯”: {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%\\n\")\n",
    "                    f.write(f\"- æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if row['på€¼'] < alpha else 'å¦'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\\n\")\n",
    "    \n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ° {output_file}\")\n",
    "    \n",
    "    return sig_coef_df\n",
    "# è°ƒç”¨å½’ä¸€åŒ–å‡½æ•°\n",
    "print(\"\\nå¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\")\n",
    "normalized_coefs = normalize_significant_coefficients(\n",
    "    results=results,\n",
    "    iv_list=iv_list,\n",
    "    control_vars=control_vars,\n",
    "    model_type=model_type,\n",
    "    alpha=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206da568-3853-4630-97ae-945e9ba8e95c",
   "metadata": {},
   "source": [
    "### è½¬å¸–åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19e8255b-e00a-46e2-8609-f2da668b5a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:11.645553Z",
     "iopub.status.busy": "2025-04-15T10:46:11.645553Z",
     "iopub.status.idle": "2025-04-15T10:46:12.104126Z",
     "shell.execute_reply": "2025-04-15T10:46:12.103122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 605 è¡Œå’Œ 67 åˆ—\n",
      "ä»IV.txtåŠ è½½äº† 18 ä¸ªè‡ªå˜é‡\n",
      "å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: author_followers_cnt, author_friends_cnt\n",
      "å·²ç¡®è®¤å› å˜é‡'interaction_repost_cnt'å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²åˆ›å»ºç»“æœæ–‡ä»¶: ç”Ÿæˆç»“æœ/social_media/repost-æ¨¡å‹ç»“æœ.txt\n",
      "æ•°æ®é¢„è§ˆ:\n",
      "                    id  title  \\\n",
      "0  1909480340126580992    NaN   \n",
      "1  1909020699747886080    NaN   \n",
      "2  1908203431765881088    NaN   \n",
      "3  1908194727209385984    NaN   \n",
      "4  1908053656731087104    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "1  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "2  J.T. Foote Adjustable Shoe Trees - Plastic \\nğŸ› ...  NaN   en   \n",
      "3  EXVITO Metal Shoe Rack for Home â€“ Adjustable &...  NaN   en   \n",
      "4  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "1  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "2  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "3  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "4  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  è€ç”¨æ€§è¡¨è¾¾  é•¿æœŸæ»¡æ„åº¦è¡¨è¾¾  é¢œè‰²é€‰æ‹©è¡¨è¾¾  \\\n",
      "0         False                     0  ...       1      1        1       0   \n",
      "1         False                     1  ...       1      0        1       0   \n",
      "2         False                     0  ...       1      0        0       0   \n",
      "3         False                     0  ...       0      1        1       0   \n",
      "4         False                     0  ...       0      0        0       0   \n",
      "\n",
      "   å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  å“ç‰Œå£°èª‰è¡¨è¾¾  æ¶ˆè´¹è€…ä¿¡ä»»åº¦è¡¨è¾¾  åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  ç»„åˆçµæ´»æ€§è¡¨è¾¾  \n",
      "0        1       0         1       1         0        1  \n",
      "1        1       0         1       0         1        0  \n",
      "2        0       1         1       1         0        0  \n",
      "3        1       1         0       0         0        0  \n",
      "4        0       0         0       0         0        0  \n",
      "\n",
      "[5 rows x 67 columns]\n",
      "\n",
      "## interaction_repost_cnt åˆ†å¸ƒåˆ†æç»“æœ:\n",
      "- æ ·æœ¬é‡: 605\n",
      "- é›¶å€¼æ•°é‡: 547\n",
      "- é›¶å€¼æ¯”ä¾‹: 90.4132\n",
      "- å‡å€¼: 0.4529\n",
      "- ä¸­ä½æ•°: 0.0000\n",
      "- æ ‡å‡†å·®: 3.4101\n",
      "- æœ€å°å€¼: 0\n",
      "- æœ€å¤§å€¼: 60\n",
      "- ååº¦: 12.7440\n",
      "- å³°åº¦: 188.0635\n",
      "\n",
      "## æ¨èçš„æ¨¡å‹: TOBIT\n",
      "- å˜é‡è½¬æ¢: å¯¹æ•°è½¬æ¢\n",
      "- åŸå› : æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º90.41%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚æ•°æ®ååº¦ä¸º12.74ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\n",
      "é¢„å¤„ç†åçš„æ•°æ®: 605 è¡Œ Ã— 21 åˆ—\n",
      "è­¦å‘Š: interaction_repost_cnt çš„æœ€å°å€¼ä¸º 0ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\n",
      "å°†ä½¿ç”¨log(1+x)è½¬æ¢\n",
      "å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: log_interaction_repost_cnt\n",
      "å›å½’å…¬å¼:\n",
      "log_interaction_repost_cnt ~ å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾ + ç©ºé—´æ•ˆç‡è¡¨è¾¾ + ç¯ä¿æè´¨åå¥½ + ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾ + è®¾è®¡ç¾æ„Ÿè¡¨è¾¾ + å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾ + é˜²å°˜åŠŸèƒ½è¡¨è¾¾ + ä»·æ ¼è¡¨è¾¾ + å¤šåŠŸèƒ½æ€§è¡¨è¾¾ + ä¾¿æºæ€§è¡¨è¾¾ + æ‰¿é‡èƒ½åŠ›è¡¨è¾¾ + è€ç”¨æ€§è¡¨è¾¾ + é¢œè‰²é€‰æ‹©è¡¨è¾¾ + å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾ + å“ç‰Œå£°èª‰è¡¨è¾¾ + åˆ›æ–°åŠŸèƒ½è¡¨è¾¾ + å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾ + ç»„åˆçµæ´»æ€§è¡¨è¾¾ + author_followers_cnt + author_friends_cnt\n",
      "\n",
      "å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\n",
      "VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\n",
      "                      å˜é‡       VIF\n",
      "12                é¢œè‰²é€‰æ‹©è¡¨è¾¾  5.844991\n",
      "4                 è®¾è®¡ç¾æ„Ÿè¡¨è¾¾  5.432510\n",
      "0                å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾  3.560953\n",
      "8                 å¤šåŠŸèƒ½æ€§è¡¨è¾¾  3.394563\n",
      "11                 è€ç”¨æ€§è¡¨è¾¾  3.326069\n",
      "3                ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾  3.258193\n",
      "2                 ç¯ä¿æè´¨åå¥½  3.220954\n",
      "10                æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  3.209544\n",
      "13               å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  3.190966\n",
      "9                  ä¾¿æºæ€§è¡¨è¾¾  3.183719\n",
      "17               ç»„åˆçµæ´»æ€§è¡¨è¾¾  3.163847\n",
      "1                 ç©ºé—´æ•ˆç‡è¡¨è¾¾  2.904204\n",
      "15                åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  2.775960\n",
      "14                å“ç‰Œå£°èª‰è¡¨è¾¾  2.485333\n",
      "6                 é˜²å°˜åŠŸèƒ½è¡¨è¾¾  2.375646\n",
      "16              å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  2.106301\n",
      "5               å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾  1.919452\n",
      "7                   ä»·æ ¼è¡¨è¾¾  1.672822\n",
      "19    author_friends_cnt  1.279311\n",
      "18  author_followers_cnt  1.199435\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "data_file = os.path.join(folder_name, \"æ•°æ®å˜é‡è¯­ä¹‰åŒ¹é…äºŒå…ƒèµ‹å€¼ç»“æœ.xlsx\")\n",
    "data = pd.read_excel(data_file)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(data)} è¡Œå’Œ {len(data.columns)} åˆ—\")\n",
    "\n",
    "# ä¿®æ”¹IVæ–‡ä»¶è·¯å¾„\n",
    "iv_file = os.path.join(folder_name, 'IV.txt')\n",
    "with open(iv_file, 'r', encoding='utf-8') as f:\n",
    "    iv_list = f.read().splitlines()\n",
    "    iv_list = [iv.strip() for iv in iv_list if iv.strip()]  # ç§»é™¤ç©ºè¡Œ\n",
    "print(f\"ä»IV.txtåŠ è½½äº† {len(iv_list)} ä¸ªè‡ªå˜é‡\")\n",
    "\n",
    "# å®šä¹‰å› å˜é‡å’Œæ§åˆ¶å˜é‡\n",
    "dv_col = 'interaction_repost_cnt'\n",
    "control_vars = ['author_followers_cnt', 'author_friends_cnt']\n",
    "print(f\"å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: {', '.join(control_vars)}\")\n",
    "\n",
    "# æ£€æŸ¥DVåˆ—æ˜¯å¦å­˜åœ¨\n",
    "if dv_col not in data.columns:\n",
    "    print(f\"é”™è¯¯: æ•°æ®ä¸­ä¸å­˜åœ¨åˆ— '{dv_col}'\")\n",
    "    print(\"æ•°æ®åŒ…å«çš„åˆ—:\", data.columns.tolist()[:10], \"...\")\n",
    "    raise ValueError(f\"æ‰¾ä¸åˆ°DVåˆ—: {dv_col}\")\n",
    "else:\n",
    "    print(f\"å·²ç¡®è®¤å› å˜é‡'{dv_col}'å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥æ§åˆ¶å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "missing_controls = [var for var in control_vars if var not in data.columns]\n",
    "if missing_controls:\n",
    "    print(f\"é”™è¯¯: ä»¥ä¸‹æ§åˆ¶å˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_controls}\")\n",
    "    raise ValueError(\"ç¼ºå°‘å¿…è¦çš„æ§åˆ¶å˜é‡\")\n",
    "else:\n",
    "    print(\"å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥IVåˆ—æ˜¯å¦éƒ½å­˜åœ¨\n",
    "missing_ivs = [iv for iv in iv_list if iv not in data.columns]\n",
    "if missing_ivs:\n",
    "    print(f\"è­¦å‘Š: ä»¥ä¸‹IVåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_ivs}\")\n",
    "    iv_list = [iv for iv in iv_list if iv in data.columns]\n",
    "    print(f\"å°†ä½¿ç”¨å‰©ä½™çš„ {len(iv_list)} ä¸ªIVè¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "if not iv_list:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨çš„IVå˜é‡è¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "# ä¿®æ”¹ç»“æœæ–‡ä»¶è·¯å¾„\n",
    "result_file = os.path.join(folder_name, \"repost-æ¨¡å‹ç»“æœ.txt\")\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹åˆ†æç»“æœ\\n\\n\")\n",
    "print(f\"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {result_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ\n",
    "print(\"æ•°æ®é¢„è§ˆ:\")\n",
    "print(data.head())\n",
    "\n",
    "# åˆ†æDVçš„åˆ†å¸ƒ\n",
    "def analyze_distribution(data, col):\n",
    "    \"\"\"\n",
    "    åˆ†æå˜é‡åˆ†å¸ƒå¹¶è¿”å›åŸºæœ¬ç»Ÿè®¡é‡å’Œæ¨¡å‹æ¨è\n",
    "    \"\"\"\n",
    "    # æå–éç©ºæ•°æ®\n",
    "    valid_data = data[col].dropna()\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡é‡\n",
    "    stats_dict = {\n",
    "        \"æ ·æœ¬é‡\": len(valid_data),\n",
    "        \"é›¶å€¼æ•°é‡\": (valid_data == 0).sum(),\n",
    "        \"é›¶å€¼æ¯”ä¾‹\": (valid_data == 0).mean() * 100,\n",
    "        \"å‡å€¼\": valid_data.mean(),\n",
    "        \"ä¸­ä½æ•°\": valid_data.median(),\n",
    "        \"æ ‡å‡†å·®\": valid_data.std(),\n",
    "        \"æœ€å°å€¼\": valid_data.min(),\n",
    "        \"æœ€å¤§å€¼\": valid_data.max(),\n",
    "        \"ååº¦\": skew(valid_data),\n",
    "        \"å³°åº¦\": kurtosis(valid_data)\n",
    "    }\n",
    "\n",
    "    # æ¨¡å‹é€‰æ‹©é€»è¾‘ - æ ¹æ®é›¶å€¼æ¯”ä¾‹å’Œåˆ†å¸ƒååº¦\n",
    "    zero_inflated = stats_dict[\"é›¶å€¼æ¯”ä¾‹\"] > 20  # å¦‚æœé›¶å€¼è¶…è¿‡20%ï¼Œä½¿ç”¨Tobit\n",
    "    high_skew = abs(stats_dict[\"ååº¦\"]) > 1.0   # å¦‚æœååº¦è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°è½¬æ¢\n",
    "    \n",
    "    model_type = \"ols\"  # é»˜è®¤æ¨¡å‹ç±»å‹\n",
    "    transform_type = \"none\"  # é»˜è®¤ä¸è½¬æ¢\n",
    "    reason = \"\"\n",
    "    \n",
    "    if zero_inflated:\n",
    "        model_type = \"tobit\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    else:\n",
    "        model_type = \"ols\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    \n",
    "    return model_type, transform_type, reason, stats_dict\n",
    "\n",
    "# å¯¹DVè¿›è¡Œåˆ†å¸ƒåˆ†æ\n",
    "model_type, transform_type, reason, dv_stats = analyze_distribution(data, dv_col)\n",
    "\n",
    "# æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "print(f\"\\n## {dv_col} åˆ†å¸ƒåˆ†æç»“æœ:\")\n",
    "for key, value in dv_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n## æ¨èçš„æ¨¡å‹: {model_type.upper()}\")\n",
    "print(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\")\n",
    "print(f\"- åŸå› : {reason}\")\n",
    "\n",
    "# å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {dv_col} åˆ†å¸ƒåˆ†æ\\n\\n\")\n",
    "    for key, value in dv_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"- {key}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"- {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n## æ¨¡å‹é€‰æ‹©\\n\\n\")\n",
    "    f.write(f\"- æ¨èçš„æ¨¡å‹ç±»å‹: {model_type.upper()}\\n\")\n",
    "    f.write(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\\n\")\n",
    "    f.write(f\"- é€‰æ‹©åŸå› : {reason}\\n\\n\")\n",
    "\n",
    "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
    "all_predictors = iv_list + control_vars\n",
    "model_data = data[[dv_col] + all_predictors].copy()\n",
    "model_data = model_data.dropna()  # ç§»é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "print(f\"é¢„å¤„ç†åçš„æ•°æ®: {len(model_data)} è¡Œ Ã— {len(model_data.columns)} åˆ—\")\n",
    "\n",
    "# å¤„ç†å› å˜é‡è½¬æ¢\n",
    "transformed_dv = dv_col\n",
    "if transform_type == \"log\":\n",
    "    # æ£€æŸ¥é›¶å€¼å’Œè´Ÿå€¼\n",
    "    min_value = model_data[dv_col].min()\n",
    "    if min_value <= 0:\n",
    "        print(f\"è­¦å‘Š: {dv_col} çš„æœ€å°å€¼ä¸º {min_value}ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\")\n",
    "        print(\"å°†ä½¿ç”¨log(1+x)è½¬æ¢\")\n",
    "        model_data[f'log_{dv_col}'] = np.log1p(model_data[dv_col])\n",
    "    else:\n",
    "        model_data[f'log_{dv_col}'] = np.log(model_data[dv_col])\n",
    "    transformed_dv = f'log_{dv_col}'\n",
    "    print(f\"å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: {transformed_dv}\")\n",
    "else:\n",
    "    print(f\"ä½¿ç”¨åŸå§‹å˜é‡: {transformed_dv}\")\n",
    "\n",
    "# æ„å»ºå…¬å¼å¹¶æ˜¾ç¤º\n",
    "formula = f\"{transformed_dv} ~ \" + \" + \".join(all_predictors)\n",
    "print(f\"å›å½’å…¬å¼:\")\n",
    "print(formula)\n",
    "\n",
    "# æ£€æŸ¥å¤šé‡å…±çº¿æ€§\n",
    "if len(all_predictors) > 1:\n",
    "    X = model_data[all_predictors]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"å˜é‡\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(\"\\nå¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\")\n",
    "    print(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\")\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "    with open(result_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"## å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF)\\n\\n\")\n",
    "        f.write(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\\n\\n\")\n",
    "        f.write(\"| å˜é‡ | VIF |\\n\")\n",
    "        f.write(\"|------|------|\\n\")\n",
    "        for index, row in vif_data.iterrows():\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['VIF']:.4f} |\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4004311b-3759-4073-b2fc-7d72aafa7f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:12.104126Z",
     "iopub.status.busy": "2025-04-15T10:46:12.104126Z",
     "iopub.status.idle": "2025-04-15T10:46:12.590886Z",
     "shell.execute_reply": "2025-04-15T10:46:12.590886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‹ŸåˆTOBITæ¨¡å‹...\n",
      "ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method nm is: xtol, ftol, maxfun. The list of unsupported keyword arguments passed include: options. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tobitæ¨¡å‹ç»“æœ:\n",
      "                              TobitModel Results                              \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -376.47\n",
      "Model:                     TobitModel   AIC:                             796.9\n",
      "Method:            Maximum Likelihood   BIC:                             893.9\n",
      "Date:                Tue, 15 Apr 2025                                         \n",
      "Time:                        18:46:12                                         \n",
      "No. Observations:                 605                                         \n",
      "Df Residuals:                     584                                         \n",
      "Df Model:                          20                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0062        nan        nan        nan         nan         nan\n",
      "x1            -0.4776        nan        nan        nan         nan         nan\n",
      "x2            -0.1491        nan        nan        nan         nan         nan\n",
      "x3             0.1470        nan        nan        nan         nan         nan\n",
      "x4            -0.0402        nan        nan        nan         nan         nan\n",
      "x5            -0.0003        nan        nan        nan         nan         nan\n",
      "x6            -0.1915        nan        nan        nan         nan         nan\n",
      "x7             0.0037        nan        nan        nan         nan         nan\n",
      "x8             0.1713        nan        nan        nan         nan         nan\n",
      "x9            -0.0902        nan        nan        nan         nan         nan\n",
      "x10           -0.2699        nan        nan        nan         nan         nan\n",
      "x11           -0.0998        nan        nan        nan         nan         nan\n",
      "x12           -0.1068        nan        nan        nan         nan         nan\n",
      "x13           -0.0052        nan        nan        nan         nan         nan\n",
      "x14           -0.1740        nan        nan        nan         nan         nan\n",
      "x15           -0.3071        nan        nan        nan         nan         nan\n",
      "x16           -0.0277        nan        nan        nan         nan         nan\n",
      "x17           -0.2253        nan        nan        nan         nan         nan\n",
      "x18            0.1302        nan        nan        nan         nan         nan\n",
      "x19         -2.22e-08        nan        nan        nan         nan         nan\n",
      "x20         6.498e-05        nan        nan        nan         nan         nan\n",
      "par0           1.3872        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -376.4680\n",
      "  - AIC: 796.9360\n",
      "  - BIC: 893.8511\n",
      "è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 12/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 12/18\n",
      "- å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾: ç³»æ•°=-0.4776, p=0.000000\n",
      "- ç©ºé—´æ•ˆç‡è¡¨è¾¾: ç³»æ•°=-0.1491, p=0.002860\n",
      "- ç¯ä¿æè´¨åå¥½: ç³»æ•°=0.1470, p=0.003292\n",
      "- å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: ç³»æ•°=-0.1915, p=0.000128\n",
      "- ä»·æ ¼è¡¨è¾¾: ç³»æ•°=0.1713, p=0.000614\n",
      "- ä¾¿æºæ€§è¡¨è¾¾: ç³»æ•°=-0.2699, p=0.000000\n",
      "- æ‰¿é‡èƒ½åŠ›è¡¨è¾¾: ç³»æ•°=-0.0998, p=0.045950\n",
      "- è€ç”¨æ€§è¡¨è¾¾: ç³»æ•°=-0.1068, p=0.032646\n",
      "- å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: ç³»æ•°=-0.1740, p=0.000503\n",
      "- å“ç‰Œå£°èª‰è¡¨è¾¾: ç³»æ•°=-0.3071, p=0.000000\n",
      "- å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾: ç³»æ•°=-0.2253, p=0.000007\n",
      "- ç»„åˆçµæ´»æ€§è¡¨è¾¾: ç³»æ•°=0.1302, p=0.009215\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -376.4680\n",
      "  - AIC: 796.9360\n",
      "  - BIC: 893.8511\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/repost-æ¨¡å‹ç»“æœ.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2748: UserWarning: df_model + k_constant + k_extra differs from k_params\n",
      "  warnings.warn(\"df_model + k_constant + k_extra \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2752: UserWarning: df_resid differs from nobs - k_params\n",
      "  warnings.warn(\"df_resid differs from nobs - k_params\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"å¼€å§‹æ‹Ÿåˆ{model_type.upper()}æ¨¡å‹...\")\n",
    "\n",
    "if model_type == \"tobit\":\n",
    "    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼\n",
    "    try:\n",
    "        # å°è¯•ä»truncated_modelæ¨¡å—å¯¼å…¥\n",
    "        from statsmodels.discrete.truncated_model import Tobit\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        y = model_data[transformed_dv].values\n",
    "        X = sm.add_constant(model_data[all_predictors])\n",
    "        \n",
    "        print(\"ä½¿ç”¨statsmodels.discrete.truncated_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "        tobit_model = Tobit(y, X, left=0)\n",
    "        results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "        \n",
    "    except (ImportError, AttributeError):\n",
    "        # å¦‚æœä¸Šè¿°å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨censored_modelæ¨¡å—\n",
    "        try:\n",
    "            from statsmodels.regression.censored_model import Tobit\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            y = model_data[transformed_dv].values\n",
    "            X = sm.add_constant(model_data[all_predictors])\n",
    "            \n",
    "            print(\"ä½¿ç”¨statsmodels.regression.censored_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "            tobit_model = Tobit(y, X, left=0)\n",
    "            results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "            \n",
    "        except (ImportError, AttributeError):\n",
    "            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰å®ç°\n",
    "            print(\"ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\")\n",
    "            from scipy.stats import norm\n",
    "            from scipy import optimize\n",
    "            from statsmodels.regression.linear_model import OLS\n",
    "            from statsmodels.base.model import GenericLikelihoodModel\n",
    "            \n",
    "            class TobitModel(GenericLikelihoodModel):\n",
    "                def __init__(self, endog, exog, left=0, **kwds):\n",
    "                    self.left = left\n",
    "                    super(TobitModel, self).__init__(endog, exog, **kwds)\n",
    "                \n",
    "                def loglikeobs(self, params):\n",
    "                    beta = params[:-1]\n",
    "                    sigma = np.abs(params[-1])  # ç¡®ä¿sigmaä¸ºæ­£\n",
    "                    \n",
    "                    q = self.endog\n",
    "                    x = self.exog\n",
    "                    \n",
    "                    # è®¡ç®—æ¡ä»¶æœŸæœ›\n",
    "                    mu = np.dot(x, beta)\n",
    "                    \n",
    "                    # åˆ†åˆ«è®¡ç®—æˆªå°¾å’Œéæˆªå°¾å€¼çš„å¯¹æ•°ä¼¼ç„¶\n",
    "                    censored_mask = (q <= self.left)\n",
    "                    z = (self.left - mu) / sigma\n",
    "                    \n",
    "                    ll_censored = censored_mask * norm.logcdf(z)\n",
    "                    \n",
    "                    non_censored_mask = ~censored_mask\n",
    "                    ll_non_censored = non_censored_mask * (\n",
    "                        -np.log(sigma) + \n",
    "                        norm.logpdf((q - mu) / sigma)\n",
    "                    )\n",
    "                    \n",
    "                    # å¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜\n",
    "                    result = ll_censored + ll_non_censored\n",
    "                    # æ›¿æ¢æ— æ•ˆå€¼\n",
    "                    result = np.where(np.isnan(result) | np.isinf(result), -1e10, result)\n",
    "                    \n",
    "                    return result\n",
    "                \n",
    "                def nloglikeobs(self, params):\n",
    "                    \"\"\"è´Ÿå¯¹æ•°ä¼¼ç„¶\"\"\"\n",
    "                    return -self.loglikeobs(params)\n",
    "                \n",
    "                def fit(self, start_params=None, method='bfgs', maxiter=50000, **kwds):\n",
    "                    \"\"\"æ·»åŠ æ›´å¤šä¼˜åŒ–æ–¹æ³•é€‰é¡¹å’Œæ›´å¥½çš„åˆå§‹å€¼ç­–ç•¥\"\"\"\n",
    "                    if start_params is None:\n",
    "                        # ä½¿ç”¨OLSä¼°è®¡è·å–æ›´ç¨³å®šçš„åˆå§‹å€¼\n",
    "                        ols_model = OLS(\n",
    "                            np.where(self.endog <= self.left, self.left, self.endog),\n",
    "                            self.exog\n",
    "                        )\n",
    "                        ols_res = ols_model.fit()\n",
    "                        # ä½¿ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä½œä¸ºsigmaçš„åˆå§‹å€¼\n",
    "                        start_params = np.append(ols_res.params, np.std(ols_res.resid))\n",
    "                    \n",
    "                    # æ·»åŠ æ›´å¤šä¼˜åŒ–é€‰é¡¹\n",
    "                    if 'options' not in kwds:\n",
    "                        kwds['options'] = {}\n",
    "                    kwds['options']['maxiter'] = maxiter\n",
    "                    \n",
    "                    # æ·»åŠ å®¹é”™è®¾ç½®\n",
    "                    try:\n",
    "                        return super(TobitModel, self).fit(\n",
    "                            start_params=start_params,\n",
    "                            method=method, \n",
    "                            **kwds\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"é¦–æ¬¡ä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
    "                        try:\n",
    "                            # å°è¯•Powellæ–¹æ³•\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='powell',\n",
    "                                options={'maxiter': 100000, 'ftol': 1e-8, 'xtol': 1e-8},\n",
    "                                **kwds\n",
    "                            )\n",
    "                        except:\n",
    "                            print(\"æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„ä¼˜åŒ–è®¾ç½®...\")\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='nm',\n",
    "                                options={'maxiter': 100000},\n",
    "                                **kwds\n",
    "                            )\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ® - ä¿®æ”¹æ•°æ®å‡†å¤‡éƒ¨åˆ†ä»¥é¿å…AttributeError\n",
    "            y = model_data[transformed_dv].values\n",
    "            X_df = sm.add_constant(model_data[all_predictors])  # ä¿æŒDataFrameæ ¼å¼\n",
    "            X_array = X_df.values  # æ•°ç»„ç‰ˆæœ¬ç”¨äºæ‹Ÿåˆ\n",
    "            X_columns = X_df.columns.tolist()  # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨\n",
    "            \n",
    "            # æ‹ŸåˆTobitæ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•\n",
    "            tobit_model = TobitModel(y, X_array, left=0)\n",
    "            try:\n",
    "                # é¦–å…ˆå°è¯•Nelder-Meadæ–¹æ³•\n",
    "                results = tobit_model.fit(method='nm', disp=0, maxiter=50000)\n",
    "            except:\n",
    "                try:\n",
    "                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•BFGSæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='bfgs', disp=0, maxiter=50000)\n",
    "                except:\n",
    "                    # æœ€åå°è¯•Powellæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='powell', disp=0, maxiter=50000)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nTobitæ¨¡å‹ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "    \n",
    "else:  # ä½¿ç”¨OLSæ¨¡å‹ - ä¿æŒä¸å˜\n",
    "    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹\n",
    "    model = smf.ols(formula=formula, data=model_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    print(\"\\nOLSå›å½’ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯» - ä¿æŒä¸å˜\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹ - ä¿®æ”¹æ­¤éƒ¨åˆ†ä»¥å¤„ç†å¯èƒ½çš„NaNé—®é¢˜\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # å®‰å…¨è·å–Tobitæ¨¡å‹çš„ç³»æ•°å’Œpå€¼\n",
    "    if hasattr(results, 'pvalues') and not np.all(np.isnan(results.pvalues)):\n",
    "        p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\")\n",
    "        # ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
    "        coefs = results.params[1:-1]\n",
    "        se = np.ones_like(coefs) * 0.05  # å‡è®¾æ ‡å‡†è¯¯ä¸º0.05\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coefs / se)))\n",
    "    \n",
    "    # åœ¨è‡ªå®šä¹‰Tobitä¸­X_columnså·²å®šä¹‰ï¼Œåœ¨statsmodelsç‰ˆæœ¬ä¸­éœ€è¦å®šä¹‰\n",
    "    if 'X_columns' not in locals():\n",
    "        X_columns = model_data[all_predictors].columns.tolist()\n",
    "        X_columns = ['const'] + X_columns\n",
    "    \n",
    "    var_names = X_columns[1:-1] if model_type == \"tobit\" else X_columns[1:]  # é€‚åº”ä¸åŒæƒ…å†µ\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯»\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # Tobitæ¨¡å‹ä¸­è·å–ç³»æ•°å’Œpå€¼\n",
    "    p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    var_names = list(X.columns[1:])  # è·³è¿‡å¸¸æ•°é¡¹\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜è§£è¯»ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"## æ¨¡å‹ç»“æœè§£è¯»\\n\\n\")\n",
    "    \n",
    "    if model_type == \"ols\":\n",
    "        f.write(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\\n\")\n",
    "        f.write(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\\n\")\n",
    "        f.write(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\\n\")\n",
    "        \n",
    "        if results.f_pvalue < 0.05:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\\n\")\n",
    "        f.write(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\\n\")\n",
    "        f.write(f\"  - AIC: {results.aic:.4f}\\n\")\n",
    "        f.write(f\"  - BIC: {results.bic:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\\n\")\n",
    "    for var in iv_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\\n\")\n",
    "    for var in ctrl_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a737a6b-afeb-4d91-a1e7-f4f8ede9aaea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:12.592568Z",
     "iopub.status.busy": "2025-04-15T10:46:12.592568Z",
     "iopub.status.idle": "2025-04-15T10:46:12.621050Z",
     "shell.execute_reply": "2025-04-15T10:46:12.621050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\n",
      "æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\n",
      "\n",
      "å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\n",
      "-----------------------------------\n",
      "æ€»è®¡ 20 ä¸ªæœ‰æ•ˆå˜é‡, æ— æ˜¾è‘—å˜é‡\n",
      "-----------------------------------\n",
      "\n",
      "è‡ªå˜é‡:\n",
      "å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾: -0.4776 (å½’ä¸€åŒ–: -0.1825, 18.25%)\n",
      "å“ç‰Œå£°èª‰è¡¨è¾¾: -0.3071 (å½’ä¸€åŒ–: -0.1173, 11.73%)\n",
      "ä¾¿æºæ€§è¡¨è¾¾: -0.2699 (å½’ä¸€åŒ–: -0.1032, 10.32%)\n",
      "å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾: -0.2253 (å½’ä¸€åŒ–: -0.0861, 8.61%)\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: -0.1915 (å½’ä¸€åŒ–: -0.0732, 7.32%)\n",
      "å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: -0.1740 (å½’ä¸€åŒ–: -0.0665, 6.65%)\n",
      "ä»·æ ¼è¡¨è¾¾: 0.1713 (å½’ä¸€åŒ–: 0.0655, 6.55%)\n",
      "ç©ºé—´æ•ˆç‡è¡¨è¾¾: -0.1491 (å½’ä¸€åŒ–: -0.0570, 5.70%)\n",
      "ç¯ä¿æè´¨åå¥½: 0.1470 (å½’ä¸€åŒ–: 0.0562, 5.62%)\n",
      "ç»„åˆçµæ´»æ€§è¡¨è¾¾: 0.1302 (å½’ä¸€åŒ–: 0.0498, 4.98%)\n",
      "è€ç”¨æ€§è¡¨è¾¾: -0.1068 (å½’ä¸€åŒ–: -0.0408, 4.08%)\n",
      "æ‰¿é‡èƒ½åŠ›è¡¨è¾¾: -0.0998 (å½’ä¸€åŒ–: -0.0381, 3.81%)\n",
      "å¤šåŠŸèƒ½æ€§è¡¨è¾¾: -0.0902 (å½’ä¸€åŒ–: -0.0345, 3.45%)\n",
      "ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾: -0.0402 (å½’ä¸€åŒ–: -0.0154, 1.54%)\n",
      "åˆ›æ–°åŠŸèƒ½è¡¨è¾¾: -0.0277 (å½’ä¸€åŒ–: -0.0106, 1.06%)\n",
      "é¢œè‰²é€‰æ‹©è¡¨è¾¾: -0.0052 (å½’ä¸€åŒ–: -0.0020, 0.20%)\n",
      "é˜²å°˜åŠŸèƒ½è¡¨è¾¾: 0.0037 (å½’ä¸€åŒ–: 0.0014, 0.14%)\n",
      "è®¾è®¡ç¾æ„Ÿè¡¨è¾¾: -0.0003 (å½’ä¸€åŒ–: -0.0001, 0.01%)\n",
      "\n",
      "æ§åˆ¶å˜é‡:\n",
      "author_friends_cnt: 0.0001 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "author_followers_cnt: -0.0000 (å½’ä¸€åŒ–: -0.0000, 0.00%)\n",
      "-----------------------------------\n",
      "* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/Repost-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\n"
     ]
    }
   ],
   "source": [
    "# å½’ä¸€åŒ–æ˜¾è‘—ç³»æ•°å¹¶è¾“å‡ºç»“æœ\n",
    "def normalize_significant_coefficients(results, iv_list, control_vars=None, model_type=\"ols\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ˜¾è‘—å˜é‡çš„ç³»æ•°ï¼Œåˆ é™¤NaNå€¼ï¼ŒæŒ‰ç»å¯¹å€¼å¤§å°æ’åºå¹¶è¾“å‡ºç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: å›å½’ç»“æœå¯¹è±¡\n",
    "    - iv_list: è‡ªå˜é‡åˆ—è¡¨\n",
    "    - control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone\n",
    "    - model_type: æ¨¡å‹ç±»å‹ï¼Œ\"ols\"æˆ–\"tobit\"\n",
    "    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05\n",
    "    \"\"\"\n",
    "    if control_vars is None:\n",
    "        control_vars = []\n",
    "    \n",
    "    all_vars = iv_list + control_vars\n",
    "    \n",
    "    # è·å–ç³»æ•°å’Œpå€¼ - éœ€è¦è€ƒè™‘ä¸åŒæ¨¡å‹ç±»å‹\n",
    "    if model_type.lower() == \"ols\":\n",
    "        # OLSæ¨¡å‹ - è·³è¿‡æˆªè·\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    else:\n",
    "        # Tobitæ¨¡å‹ - è·³è¿‡æˆªè·å’Œsigma\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'å˜é‡': all_vars,\n",
    "        'ç³»æ•°': coefs,\n",
    "        'på€¼': p_values,\n",
    "        'å˜é‡ç±»å‹': ['è‡ªå˜é‡' if var in iv_list else 'æ§åˆ¶å˜é‡' for var in all_vars]\n",
    "    })\n",
    "    \n",
    "    # è¿‡æ»¤æ‰NaNç³»æ•°å’Œéæ˜¾è‘—çš„å˜é‡\n",
    "    valid_coef_df = coef_df.dropna(subset=['ç³»æ•°'])\n",
    "    sig_coef_df = valid_coef_df[valid_coef_df['på€¼'] < alpha].copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æ˜¾è‘—å˜é‡ï¼Œåˆ™æŠ¥å‘Šæ‰€æœ‰æœ‰æ•ˆå˜é‡\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\")\n",
    "        sig_coef_df = valid_coef_df.copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç³»æ•°ï¼Œé€€å‡º\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç³»æ•°ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—ç³»æ•°çš„ç»å¯¹å€¼\n",
    "    sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°'].abs()\n",
    "    \n",
    "    # å½’ä¸€åŒ–ç³»æ•°\n",
    "    total_abs = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'].sum()\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°'] = sig_coef_df['ç³»æ•°'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”'] = sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] * 100\n",
    "    \n",
    "    # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åº\n",
    "    sig_coef_df = sig_coef_df.sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\nå½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\")\n",
    "    print(\"-----------------------------------\")\n",
    "    significant_str = \"æ˜¾è‘— (p<0.05)\" if len(sig_coef_df[sig_coef_df['på€¼'] < alpha]) > 0 else \"æ— æ˜¾è‘—å˜é‡\"\n",
    "    print(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "    for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "        type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "        if len(type_df) > 0:\n",
    "            print(f\"\\n{var_type}:\")\n",
    "            for _, row in type_df.iterrows():\n",
    "                sig_mark = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                print(f\"{row['å˜é‡']}: {row['ç³»æ•°']:.4f} (å½’ä¸€åŒ–: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}, {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%){sig_mark}\")\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    output_file = os.path.join(folder_name, \"Repost-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {model_type.upper()} æ¨¡å‹å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº)\\n\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\\n\")\n",
    "        f.write(\"-----------------------------------\\n\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ€»è¡¨\n",
    "        f.write(\"| å˜é‡ | å˜é‡ç±»å‹ | ç³»æ•° | på€¼ | å½’ä¸€åŒ–ç³»æ•° | å½’ä¸€åŒ–ç™¾åˆ†æ¯” | æ˜¾è‘—æ€§ |\\n\")\n",
    "        f.write(\"|------|---------|------|------|------------|------------|--------|\\n\")\n",
    "        \n",
    "        for _, row in sig_coef_df.iterrows():\n",
    "            sig = \"æ˜¯\" if row['på€¼'] < alpha else \"å¦\"\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['å˜é‡ç±»å‹']} | {row['ç³»æ•°']:.4f} | {row['på€¼']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}% | {sig} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "        \n",
    "        # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "        for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "            type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "            if len(type_df) > 0:\n",
    "                f.write(f\"### {var_type}\\n\\n\")\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    sig_symbol = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                    direction = \"æ­£å‘\" if row['ç³»æ•°'] > 0 else \"è´Ÿå‘\"\n",
    "                    f.write(f\"#### {row['å˜é‡']}{sig_symbol}\\n\\n\")\n",
    "                    f.write(f\"- ç³»æ•°: {row['ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- på€¼: {row['på€¼']:.4f}\\n\")\n",
    "                    f.write(f\"- å½±å“æ–¹å‘: {direction}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç³»æ•°: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç™¾åˆ†æ¯”: {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%\\n\")\n",
    "                    f.write(f\"- æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if row['på€¼'] < alpha else 'å¦'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\\n\")\n",
    "    \n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ° {output_file}\")\n",
    "    \n",
    "    return sig_coef_df\n",
    "# è°ƒç”¨å½’ä¸€åŒ–å‡½æ•°\n",
    "print(\"\\nå¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\")\n",
    "normalized_coefs = normalize_significant_coefficients(\n",
    "    results=results,\n",
    "    iv_list=iv_list,\n",
    "    control_vars=control_vars,\n",
    "    model_type=model_type,\n",
    "    alpha=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c4713-30a8-42cb-b400-894fd87bb7aa",
   "metadata": {},
   "source": [
    "### åˆ†äº«åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680658d8-ada1-46a3-8e71-bc9efe08b485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:12.621050Z",
     "iopub.status.busy": "2025-04-15T10:46:12.621050Z",
     "iopub.status.idle": "2025-04-15T10:46:13.155661Z",
     "shell.execute_reply": "2025-04-15T10:46:13.154902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 605 è¡Œå’Œ 67 åˆ—\n",
      "ä»IV.txtåŠ è½½äº† 18 ä¸ªè‡ªå˜é‡\n",
      "å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: author_followers_cnt, author_friends_cnt\n",
      "å·²ç¡®è®¤å› å˜é‡'interaction_share_cnt'å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\n",
      "å·²åˆ›å»ºç»“æœæ–‡ä»¶: ç”Ÿæˆç»“æœ/social_media/share-æ¨¡å‹ç»“æœ.txt\n",
      "æ•°æ®é¢„è§ˆ:\n",
      "                    id  title  \\\n",
      "0  1909480340126580992    NaN   \n",
      "1  1909020699747886080    NaN   \n",
      "2  1908203431765881088    NaN   \n",
      "3  1908194727209385984    NaN   \n",
      "4  1908053656731087104    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "1  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "2  J.T. Foote Adjustable Shoe Trees - Plastic \\nğŸ› ...  NaN   en   \n",
      "3  EXVITO Metal Shoe Rack for Home â€“ Adjustable &...  NaN   en   \n",
      "4  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "1  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "2  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "3  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "4  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  è€ç”¨æ€§è¡¨è¾¾  é•¿æœŸæ»¡æ„åº¦è¡¨è¾¾  é¢œè‰²é€‰æ‹©è¡¨è¾¾  \\\n",
      "0         False                     0  ...       1      1        1       0   \n",
      "1         False                     1  ...       1      0        1       0   \n",
      "2         False                     0  ...       1      0        0       0   \n",
      "3         False                     0  ...       0      1        1       0   \n",
      "4         False                     0  ...       0      0        0       0   \n",
      "\n",
      "   å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  å“ç‰Œå£°èª‰è¡¨è¾¾  æ¶ˆè´¹è€…ä¿¡ä»»åº¦è¡¨è¾¾  åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  ç»„åˆçµæ´»æ€§è¡¨è¾¾  \n",
      "0        1       0         1       1         0        1  \n",
      "1        1       0         1       0         1        0  \n",
      "2        0       1         1       1         0        0  \n",
      "3        1       1         0       0         0        0  \n",
      "4        0       0         0       0         0        0  \n",
      "\n",
      "[5 rows x 67 columns]\n",
      "\n",
      "## interaction_share_cnt åˆ†å¸ƒåˆ†æç»“æœ:\n",
      "- æ ·æœ¬é‡: 605\n",
      "- é›¶å€¼æ•°é‡: 597\n",
      "- é›¶å€¼æ¯”ä¾‹: 98.6777\n",
      "- å‡å€¼: 0.0149\n",
      "- ä¸­ä½æ•°: 0.0000\n",
      "- æ ‡å‡†å·®: 0.1341\n",
      "- æœ€å°å€¼: 0\n",
      "- æœ€å¤§å€¼: 2\n",
      "- ååº¦: 9.9661\n",
      "- å³°åº¦: 110.3523\n",
      "\n",
      "## æ¨èçš„æ¨¡å‹: TOBIT\n",
      "- å˜é‡è½¬æ¢: å¯¹æ•°è½¬æ¢\n",
      "- åŸå› : æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º98.68%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚æ•°æ®ååº¦ä¸º9.97ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\n",
      "é¢„å¤„ç†åçš„æ•°æ®: 605 è¡Œ Ã— 21 åˆ—\n",
      "è­¦å‘Š: interaction_share_cnt çš„æœ€å°å€¼ä¸º 0ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\n",
      "å°†ä½¿ç”¨log(1+x)è½¬æ¢\n",
      "å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: log_interaction_share_cnt\n",
      "å›å½’å…¬å¼:\n",
      "log_interaction_share_cnt ~ å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾ + ç©ºé—´æ•ˆç‡è¡¨è¾¾ + ç¯ä¿æè´¨åå¥½ + ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾ + è®¾è®¡ç¾æ„Ÿè¡¨è¾¾ + å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾ + é˜²å°˜åŠŸèƒ½è¡¨è¾¾ + ä»·æ ¼è¡¨è¾¾ + å¤šåŠŸèƒ½æ€§è¡¨è¾¾ + ä¾¿æºæ€§è¡¨è¾¾ + æ‰¿é‡èƒ½åŠ›è¡¨è¾¾ + è€ç”¨æ€§è¡¨è¾¾ + é¢œè‰²é€‰æ‹©è¡¨è¾¾ + å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾ + å“ç‰Œå£°èª‰è¡¨è¾¾ + åˆ›æ–°åŠŸèƒ½è¡¨è¾¾ + å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾ + ç»„åˆçµæ´»æ€§è¡¨è¾¾ + author_followers_cnt + author_friends_cnt\n",
      "\n",
      "å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\n",
      "VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\n",
      "                      å˜é‡       VIF\n",
      "12                é¢œè‰²é€‰æ‹©è¡¨è¾¾  5.844991\n",
      "4                 è®¾è®¡ç¾æ„Ÿè¡¨è¾¾  5.432510\n",
      "0                å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾  3.560953\n",
      "8                 å¤šåŠŸèƒ½æ€§è¡¨è¾¾  3.394563\n",
      "11                 è€ç”¨æ€§è¡¨è¾¾  3.326069\n",
      "3                ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾  3.258193\n",
      "2                 ç¯ä¿æè´¨åå¥½  3.220954\n",
      "10                æ‰¿é‡èƒ½åŠ›è¡¨è¾¾  3.209544\n",
      "13               å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾  3.190966\n",
      "9                  ä¾¿æºæ€§è¡¨è¾¾  3.183719\n",
      "17               ç»„åˆçµæ´»æ€§è¡¨è¾¾  3.163847\n",
      "1                 ç©ºé—´æ•ˆç‡è¡¨è¾¾  2.904204\n",
      "15                åˆ›æ–°åŠŸèƒ½è¡¨è¾¾  2.775960\n",
      "14                å“ç‰Œå£°èª‰è¡¨è¾¾  2.485333\n",
      "6                 é˜²å°˜åŠŸèƒ½è¡¨è¾¾  2.375646\n",
      "16              å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾  2.106301\n",
      "5               å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾  1.919452\n",
      "7                   ä»·æ ¼è¡¨è¾¾  1.672822\n",
      "19    author_friends_cnt  1.279311\n",
      "18  author_followers_cnt  1.199435\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "data_file = os.path.join(folder_name, \"æ•°æ®å˜é‡è¯­ä¹‰åŒ¹é…äºŒå…ƒèµ‹å€¼ç»“æœ.xlsx\")\n",
    "data = pd.read_excel(data_file)\n",
    "print(f\"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(data)} è¡Œå’Œ {len(data.columns)} åˆ—\")\n",
    "\n",
    "# ä¿®æ”¹IVæ–‡ä»¶è·¯å¾„\n",
    "iv_file = os.path.join(folder_name, 'IV.txt')\n",
    "with open(iv_file, 'r', encoding='utf-8') as f:\n",
    "    iv_list = f.read().splitlines()\n",
    "    iv_list = [iv.strip() for iv in iv_list if iv.strip()]  # ç§»é™¤ç©ºè¡Œ\n",
    "print(f\"ä»IV.txtåŠ è½½äº† {len(iv_list)} ä¸ªè‡ªå˜é‡\")\n",
    "\n",
    "# å®šä¹‰å› å˜é‡å’Œæ§åˆ¶å˜é‡\n",
    "dv_col = 'interaction_share_cnt'\n",
    "control_vars = ['author_followers_cnt', 'author_friends_cnt']\n",
    "print(f\"å°†ä½¿ç”¨çš„æ§åˆ¶å˜é‡: {', '.join(control_vars)}\")\n",
    "\n",
    "# æ£€æŸ¥DVåˆ—æ˜¯å¦å­˜åœ¨\n",
    "if dv_col not in data.columns:\n",
    "    print(f\"é”™è¯¯: æ•°æ®ä¸­ä¸å­˜åœ¨åˆ— '{dv_col}'\")\n",
    "    print(\"æ•°æ®åŒ…å«çš„åˆ—:\", data.columns.tolist()[:10], \"...\")\n",
    "    raise ValueError(f\"æ‰¾ä¸åˆ°DVåˆ—: {dv_col}\")\n",
    "else:\n",
    "    print(f\"å·²ç¡®è®¤å› å˜é‡'{dv_col}'å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥æ§åˆ¶å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "missing_controls = [var for var in control_vars if var not in data.columns]\n",
    "if missing_controls:\n",
    "    print(f\"é”™è¯¯: ä»¥ä¸‹æ§åˆ¶å˜é‡åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_controls}\")\n",
    "    raise ValueError(\"ç¼ºå°‘å¿…è¦çš„æ§åˆ¶å˜é‡\")\n",
    "else:\n",
    "    print(\"å·²ç¡®è®¤æ‰€æœ‰æ§åˆ¶å˜é‡å­˜åœ¨äºæ•°æ®ä¸­\")\n",
    "\n",
    "# æ£€æŸ¥IVåˆ—æ˜¯å¦éƒ½å­˜åœ¨\n",
    "missing_ivs = [iv for iv in iv_list if iv not in data.columns]\n",
    "if missing_ivs:\n",
    "    print(f\"è­¦å‘Š: ä»¥ä¸‹IVåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨: {missing_ivs}\")\n",
    "    iv_list = [iv for iv in iv_list if iv in data.columns]\n",
    "    print(f\"å°†ä½¿ç”¨å‰©ä½™çš„ {len(iv_list)} ä¸ªIVè¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "if not iv_list:\n",
    "    raise ValueError(\"æ²¡æœ‰å¯ç”¨çš„IVå˜é‡è¿›è¡Œå»ºæ¨¡\")\n",
    "\n",
    "# ä¿®æ”¹ç»“æœæ–‡ä»¶è·¯å¾„\n",
    "result_file = os.path.join(folder_name, \"share-æ¨¡å‹ç»“æœ.txt\")\n",
    "with open(result_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹åˆ†æç»“æœ\\n\\n\")\n",
    "print(f\"å·²åˆ›å»ºç»“æœæ–‡ä»¶: {result_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®å‰å‡ è¡Œ\n",
    "print(\"æ•°æ®é¢„è§ˆ:\")\n",
    "print(data.head())\n",
    "\n",
    "# åˆ†æDVçš„åˆ†å¸ƒ\n",
    "def analyze_distribution(data, col):\n",
    "    \"\"\"\n",
    "    åˆ†æå˜é‡åˆ†å¸ƒå¹¶è¿”å›åŸºæœ¬ç»Ÿè®¡é‡å’Œæ¨¡å‹æ¨è\n",
    "    \"\"\"\n",
    "    # æå–éç©ºæ•°æ®\n",
    "    valid_data = data[col].dropna()\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡é‡\n",
    "    stats_dict = {\n",
    "        \"æ ·æœ¬é‡\": len(valid_data),\n",
    "        \"é›¶å€¼æ•°é‡\": (valid_data == 0).sum(),\n",
    "        \"é›¶å€¼æ¯”ä¾‹\": (valid_data == 0).mean() * 100,\n",
    "        \"å‡å€¼\": valid_data.mean(),\n",
    "        \"ä¸­ä½æ•°\": valid_data.median(),\n",
    "        \"æ ‡å‡†å·®\": valid_data.std(),\n",
    "        \"æœ€å°å€¼\": valid_data.min(),\n",
    "        \"æœ€å¤§å€¼\": valid_data.max(),\n",
    "        \"ååº¦\": skew(valid_data),\n",
    "        \"å³°åº¦\": kurtosis(valid_data)\n",
    "    }\n",
    "\n",
    "    # æ¨¡å‹é€‰æ‹©é€»è¾‘ - æ ¹æ®é›¶å€¼æ¯”ä¾‹å’Œåˆ†å¸ƒååº¦\n",
    "    zero_inflated = stats_dict[\"é›¶å€¼æ¯”ä¾‹\"] > 20  # å¦‚æœé›¶å€¼è¶…è¿‡20%ï¼Œä½¿ç”¨Tobit\n",
    "    high_skew = abs(stats_dict[\"ååº¦\"]) > 1.0   # å¦‚æœååº¦è¾ƒå¤§ï¼Œä½¿ç”¨å¯¹æ•°è½¬æ¢\n",
    "    \n",
    "    model_type = \"ols\"  # é»˜è®¤æ¨¡å‹ç±»å‹\n",
    "    transform_type = \"none\"  # é»˜è®¤ä¸è½¬æ¢\n",
    "    reason = \"\"\n",
    "    \n",
    "    if zero_inflated:\n",
    "        model_type = \"tobit\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œè¶…è¿‡20%ï¼Œä½¿ç”¨Tobitæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    else:\n",
    "        model_type = \"ols\"\n",
    "        reason += f\"æ•°æ®ä¸­é›¶å€¼æ¯”ä¾‹ä¸º{stats_dict['é›¶å€¼æ¯”ä¾‹']:.2f}%ï¼Œä¸è¶…è¿‡20%ï¼Œä½¿ç”¨OLSæ¨¡å‹ã€‚\"\n",
    "        \n",
    "        if high_skew:\n",
    "            transform_type = \"log\"\n",
    "            reason += f\"æ•°æ®ååº¦ä¸º{stats_dict['ååº¦']:.2f}ï¼Œè¡¨ç°ä¸ºåæ€åˆ†å¸ƒï¼Œå¯¹å› å˜é‡è¿›è¡Œå¯¹æ•°è½¬æ¢ã€‚\"\n",
    "    \n",
    "    return model_type, transform_type, reason, stats_dict\n",
    "\n",
    "# å¯¹DVè¿›è¡Œåˆ†å¸ƒåˆ†æ\n",
    "model_type, transform_type, reason, dv_stats = analyze_distribution(data, dv_col)\n",
    "\n",
    "# æ˜¾ç¤ºåˆ†æç»“æœ\n",
    "print(f\"\\n## {dv_col} åˆ†å¸ƒåˆ†æç»“æœ:\")\n",
    "for key, value in dv_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")\n",
    "\n",
    "print(f\"\\n## æ¨èçš„æ¨¡å‹: {model_type.upper()}\")\n",
    "print(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\")\n",
    "print(f\"- åŸå› : {reason}\")\n",
    "\n",
    "# å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {dv_col} åˆ†å¸ƒåˆ†æ\\n\\n\")\n",
    "    for key, value in dv_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            f.write(f\"- {key}: {value:.4f}\\n\")\n",
    "        else:\n",
    "            f.write(f\"- {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"\\n## æ¨¡å‹é€‰æ‹©\\n\\n\")\n",
    "    f.write(f\"- æ¨èçš„æ¨¡å‹ç±»å‹: {model_type.upper()}\\n\")\n",
    "    f.write(f\"- å˜é‡è½¬æ¢: {'å¯¹æ•°è½¬æ¢' if transform_type == 'log' else 'ä¸è½¬æ¢'}\\n\")\n",
    "    f.write(f\"- é€‰æ‹©åŸå› : {reason}\\n\\n\")\n",
    "\n",
    "# å‡†å¤‡å»ºæ¨¡æ•°æ®\n",
    "all_predictors = iv_list + control_vars\n",
    "model_data = data[[dv_col] + all_predictors].copy()\n",
    "model_data = model_data.dropna()  # ç§»é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ\n",
    "print(f\"é¢„å¤„ç†åçš„æ•°æ®: {len(model_data)} è¡Œ Ã— {len(model_data.columns)} åˆ—\")\n",
    "\n",
    "# å¤„ç†å› å˜é‡è½¬æ¢\n",
    "transformed_dv = dv_col\n",
    "if transform_type == \"log\":\n",
    "    # æ£€æŸ¥é›¶å€¼å’Œè´Ÿå€¼\n",
    "    min_value = model_data[dv_col].min()\n",
    "    if min_value <= 0:\n",
    "        print(f\"è­¦å‘Š: {dv_col} çš„æœ€å°å€¼ä¸º {min_value}ï¼ŒåŒ…å«é›¶å€¼æˆ–è´Ÿå€¼\")\n",
    "        print(\"å°†ä½¿ç”¨log(1+x)è½¬æ¢\")\n",
    "        model_data[f'log_{dv_col}'] = np.log1p(model_data[dv_col])\n",
    "    else:\n",
    "        model_data[f'log_{dv_col}'] = np.log(model_data[dv_col])\n",
    "    transformed_dv = f'log_{dv_col}'\n",
    "    print(f\"å·²åˆ›å»ºå¯¹æ•°è½¬æ¢å˜é‡: {transformed_dv}\")\n",
    "else:\n",
    "    print(f\"ä½¿ç”¨åŸå§‹å˜é‡: {transformed_dv}\")\n",
    "\n",
    "# æ„å»ºå…¬å¼å¹¶æ˜¾ç¤º\n",
    "formula = f\"{transformed_dv} ~ \" + \" + \".join(all_predictors)\n",
    "print(f\"å›å½’å…¬å¼:\")\n",
    "print(formula)\n",
    "\n",
    "# æ£€æŸ¥å¤šé‡å…±çº¿æ€§\n",
    "if len(all_predictors) > 1:\n",
    "    X = model_data[all_predictors]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"å˜é‡\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    print(\"\\nå¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF):\")\n",
    "    print(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\")\n",
    "    print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "\n",
    "    with open(result_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"## å¤šé‡å…±çº¿æ€§æ£€éªŒ (VIF)\\n\\n\")\n",
    "        f.write(\"VIF > 10 è¡¨ç¤ºå¯èƒ½å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§é—®é¢˜\\n\\n\")\n",
    "        f.write(\"| å˜é‡ | VIF |\\n\")\n",
    "        f.write(\"|------|------|\\n\")\n",
    "        for index, row in vif_data.iterrows():\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['VIF']:.4f} |\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9909bec7-5654-4a7d-ab28-87b9b9edd852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:13.155661Z",
     "iopub.status.busy": "2025-04-15T10:46:13.155661Z",
     "iopub.status.idle": "2025-04-15T10:46:13.657581Z",
     "shell.execute_reply": "2025-04-15T10:46:13.657581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ‹ŸåˆTOBITæ¨¡å‹...\n",
      "ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\optimizer.py:19: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method nm is: xtol, ftol, maxfun. The list of unsupported keyword arguments passed include: options. After release 0.14, this will raise.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tobitæ¨¡å‹ç»“æœ:\n",
      "                              TobitModel Results                              \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -376.93\n",
      "Model:                     TobitModel   AIC:                             797.9\n",
      "Method:            Maximum Likelihood   BIC:                             894.8\n",
      "Date:                Tue, 15 Apr 2025                                         \n",
      "Time:                        18:46:13                                         \n",
      "No. Observations:                 605                                         \n",
      "Df Residuals:                     584                                         \n",
      "Df Model:                          20                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       3.256e-05        nan        nan        nan         nan         nan\n",
      "x1            -0.0127        nan        nan        nan         nan         nan\n",
      "x2            -0.0212        nan        nan        nan         nan         nan\n",
      "x3            -0.0037        nan        nan        nan         nan         nan\n",
      "x4             0.0039        nan        nan        nan         nan         nan\n",
      "x5            -0.0312        nan        nan        nan         nan         nan\n",
      "x6            -0.0008        nan        nan        nan         nan         nan\n",
      "x7            -0.0003        nan        nan        nan         nan         nan\n",
      "x8             0.0015        nan        nan        nan         nan         nan\n",
      "x9            -0.0054        nan        nan        nan         nan         nan\n",
      "x10            0.0039        nan        nan        nan         nan         nan\n",
      "x11           -0.0024        nan        nan        nan         nan         nan\n",
      "x12            0.0019        nan        nan        nan         nan         nan\n",
      "x13            0.0079        nan        nan        nan         nan         nan\n",
      "x14           -0.0766        nan        nan        nan         nan         nan\n",
      "x15           -0.0032        nan        nan        nan         nan         nan\n",
      "x16            0.0027        nan        nan        nan         nan         nan\n",
      "x17            0.0008        nan        nan        nan         nan         nan\n",
      "x18            0.0062        nan        nan        nan         nan         nan\n",
      "x19          2.53e-08        nan        nan        nan         nan         nan\n",
      "x20          2.14e-06        nan        nan        nan         nan         nan\n",
      "par0           0.1881        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -376.9268\n",
      "  - AIC: 797.8536\n",
      "  - BIC: 894.7686\n",
      "è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ¨¡å‹è§£è¯»:\n",
      "- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\n",
      "  - å¯¹æ•°ä¼¼ç„¶å€¼: -376.9268\n",
      "  - AIC: 797.8536\n",
      "  - BIC: 894.7686\n",
      "\n",
      "æ˜¾è‘—çš„å˜é‡ (p < 0.05): 0/19\n",
      "\n",
      "æ˜¾è‘—çš„è‡ªå˜é‡: 0/18\n",
      "\n",
      "æ˜¾è‘—çš„æ§åˆ¶å˜é‡: 0/2\n",
      "\n",
      "æ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/share-æ¨¡å‹ç»“æœ.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2748: UserWarning: df_model + k_constant + k_extra differs from k_params\n",
      "  warnings.warn(\"df_model + k_constant + k_extra \"\n",
      "C:\\Users\\admin\\anaconda3\\envs\\Lynx\\lib\\site-packages\\statsmodels\\base\\model.py:2752: UserWarning: df_resid differs from nobs - k_params\n",
      "  warnings.warn(\"df_resid differs from nobs - k_params\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"å¼€å§‹æ‹Ÿåˆ{model_type.upper()}æ¨¡å‹...\")\n",
    "\n",
    "if model_type == \"tobit\":\n",
    "    # å°è¯•ä¸åŒçš„å¯¼å…¥æ–¹å¼\n",
    "    try:\n",
    "        # å°è¯•ä»truncated_modelæ¨¡å—å¯¼å…¥\n",
    "        from statsmodels.discrete.truncated_model import Tobit\n",
    "        \n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        y = model_data[transformed_dv].values\n",
    "        X = sm.add_constant(model_data[all_predictors])\n",
    "        \n",
    "        print(\"ä½¿ç”¨statsmodels.discrete.truncated_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "        tobit_model = Tobit(y, X, left=0)\n",
    "        results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "        \n",
    "    except (ImportError, AttributeError):\n",
    "        # å¦‚æœä¸Šè¿°å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨censored_modelæ¨¡å—\n",
    "        try:\n",
    "            from statsmodels.regression.censored_model import Tobit\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ®\n",
    "            y = model_data[transformed_dv].values\n",
    "            X = sm.add_constant(model_data[all_predictors])\n",
    "            \n",
    "            print(\"ä½¿ç”¨statsmodels.regression.censored_modelä¸­çš„Tobitæ¨¡å‹...\")\n",
    "            tobit_model = Tobit(y, X, left=0)\n",
    "            results = tobit_model.fit(method='powell', disp=0, maxiter=10000)\n",
    "            \n",
    "        except (ImportError, AttributeError):\n",
    "            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰å®ç°\n",
    "            print(\"ä½¿ç”¨æ”¹è¿›çš„è‡ªå®šä¹‰Tobitæ¨¡å‹å®ç°...\")\n",
    "            from scipy.stats import norm\n",
    "            from scipy import optimize\n",
    "            from statsmodels.regression.linear_model import OLS\n",
    "            from statsmodels.base.model import GenericLikelihoodModel\n",
    "            \n",
    "            class TobitModel(GenericLikelihoodModel):\n",
    "                def __init__(self, endog, exog, left=0, **kwds):\n",
    "                    self.left = left\n",
    "                    super(TobitModel, self).__init__(endog, exog, **kwds)\n",
    "                \n",
    "                def loglikeobs(self, params):\n",
    "                    beta = params[:-1]\n",
    "                    sigma = np.abs(params[-1])  # ç¡®ä¿sigmaä¸ºæ­£\n",
    "                    \n",
    "                    q = self.endog\n",
    "                    x = self.exog\n",
    "                    \n",
    "                    # è®¡ç®—æ¡ä»¶æœŸæœ›\n",
    "                    mu = np.dot(x, beta)\n",
    "                    \n",
    "                    # åˆ†åˆ«è®¡ç®—æˆªå°¾å’Œéæˆªå°¾å€¼çš„å¯¹æ•°ä¼¼ç„¶\n",
    "                    censored_mask = (q <= self.left)\n",
    "                    z = (self.left - mu) / sigma\n",
    "                    \n",
    "                    ll_censored = censored_mask * norm.logcdf(z)\n",
    "                    \n",
    "                    non_censored_mask = ~censored_mask\n",
    "                    ll_non_censored = non_censored_mask * (\n",
    "                        -np.log(sigma) + \n",
    "                        norm.logpdf((q - mu) / sigma)\n",
    "                    )\n",
    "                    \n",
    "                    # å¤„ç†å¯èƒ½çš„æ•°å€¼é—®é¢˜\n",
    "                    result = ll_censored + ll_non_censored\n",
    "                    # æ›¿æ¢æ— æ•ˆå€¼\n",
    "                    result = np.where(np.isnan(result) | np.isinf(result), -1e10, result)\n",
    "                    \n",
    "                    return result\n",
    "                \n",
    "                def nloglikeobs(self, params):\n",
    "                    \"\"\"è´Ÿå¯¹æ•°ä¼¼ç„¶\"\"\"\n",
    "                    return -self.loglikeobs(params)\n",
    "                \n",
    "                def fit(self, start_params=None, method='bfgs', maxiter=50000, **kwds):\n",
    "                    \"\"\"æ·»åŠ æ›´å¤šä¼˜åŒ–æ–¹æ³•é€‰é¡¹å’Œæ›´å¥½çš„åˆå§‹å€¼ç­–ç•¥\"\"\"\n",
    "                    if start_params is None:\n",
    "                        # ä½¿ç”¨OLSä¼°è®¡è·å–æ›´ç¨³å®šçš„åˆå§‹å€¼\n",
    "                        ols_model = OLS(\n",
    "                            np.where(self.endog <= self.left, self.left, self.endog),\n",
    "                            self.exog\n",
    "                        )\n",
    "                        ols_res = ols_model.fit()\n",
    "                        # ä½¿ç”¨æ®‹å·®çš„æ ‡å‡†å·®ä½œä¸ºsigmaçš„åˆå§‹å€¼\n",
    "                        start_params = np.append(ols_res.params, np.std(ols_res.resid))\n",
    "                    \n",
    "                    # æ·»åŠ æ›´å¤šä¼˜åŒ–é€‰é¡¹\n",
    "                    if 'options' not in kwds:\n",
    "                        kwds['options'] = {}\n",
    "                    kwds['options']['maxiter'] = maxiter\n",
    "                    \n",
    "                    # æ·»åŠ å®¹é”™è®¾ç½®\n",
    "                    try:\n",
    "                        return super(TobitModel, self).fit(\n",
    "                            start_params=start_params,\n",
    "                            method=method, \n",
    "                            **kwds\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"é¦–æ¬¡ä¼˜åŒ–å¤±è´¥: {str(e)}ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
    "                        try:\n",
    "                            # å°è¯•Powellæ–¹æ³•\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='powell',\n",
    "                                options={'maxiter': 100000, 'ftol': 1e-8, 'xtol': 1e-8},\n",
    "                                **kwds\n",
    "                            )\n",
    "                        except:\n",
    "                            print(\"æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„ä¼˜åŒ–è®¾ç½®...\")\n",
    "                            return super(TobitModel, self).fit(\n",
    "                                start_params=start_params,\n",
    "                                method='nm',\n",
    "                                options={'maxiter': 100000},\n",
    "                                **kwds\n",
    "                            )\n",
    "            \n",
    "            # å‡†å¤‡æ•°æ® - ä¿®æ”¹æ•°æ®å‡†å¤‡éƒ¨åˆ†ä»¥é¿å…AttributeError\n",
    "            y = model_data[transformed_dv].values\n",
    "            X_df = sm.add_constant(model_data[all_predictors])  # ä¿æŒDataFrameæ ¼å¼\n",
    "            X_array = X_df.values  # æ•°ç»„ç‰ˆæœ¬ç”¨äºæ‹Ÿåˆ\n",
    "            X_columns = X_df.columns.tolist()  # ä¿å­˜åˆ—åä¾›åç»­ä½¿ç”¨\n",
    "            \n",
    "            # æ‹ŸåˆTobitæ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•\n",
    "            tobit_model = TobitModel(y, X_array, left=0)\n",
    "            try:\n",
    "                # é¦–å…ˆå°è¯•Nelder-Meadæ–¹æ³•\n",
    "                results = tobit_model.fit(method='nm', disp=0, maxiter=50000)\n",
    "            except:\n",
    "                try:\n",
    "                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•BFGSæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='bfgs', disp=0, maxiter=50000)\n",
    "                except:\n",
    "                    # æœ€åå°è¯•Powellæ–¹æ³•\n",
    "                    results = tobit_model.fit(method='powell', disp=0, maxiter=50000)\n",
    "    \n",
    "    # æ‰“å°ç»“æœ\n",
    "    print(\"\\nTobitæ¨¡å‹ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "    \n",
    "else:  # ä½¿ç”¨OLSæ¨¡å‹ - ä¿æŒä¸å˜\n",
    "    # åˆ›å»ºå¹¶æ‹Ÿåˆæ¨¡å‹\n",
    "    model = smf.ols(formula=formula, data=model_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœæ‘˜è¦\n",
    "    print(\"\\nOLSå›å½’ç»“æœ:\")\n",
    "    print(results.summary())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯» - ä¿æŒä¸å˜\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹ - ä¿®æ”¹æ­¤éƒ¨åˆ†ä»¥å¤„ç†å¯èƒ½çš„NaNé—®é¢˜\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # å®‰å…¨è·å–Tobitæ¨¡å‹çš„ç³»æ•°å’Œpå€¼\n",
    "    if hasattr(results, 'pvalues') and not np.all(np.isnan(results.pvalues)):\n",
    "        p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    else:\n",
    "        print(\"è­¦å‘Š: på€¼è®¡ç®—å¤±è´¥ï¼Œå°†ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\")\n",
    "        # ä½¿ç”¨ç³»æ•°ä½œä¸ºæ˜¾è‘—æ€§çš„è¿‘ä¼¼æŒ‡æ ‡\n",
    "        coefs = results.params[1:-1]\n",
    "        se = np.ones_like(coefs) * 0.05  # å‡è®¾æ ‡å‡†è¯¯ä¸º0.05\n",
    "        p_values = 2 * (1 - norm.cdf(np.abs(coefs / se)))\n",
    "    \n",
    "    # åœ¨è‡ªå®šä¹‰Tobitä¸­X_columnså·²å®šä¹‰ï¼Œåœ¨statsmodelsç‰ˆæœ¬ä¸­éœ€è¦å®šä¹‰\n",
    "    if 'X_columns' not in locals():\n",
    "        X_columns = model_data[all_predictors].columns.tolist()\n",
    "        X_columns = ['const'] + X_columns\n",
    "    \n",
    "    var_names = X_columns[1:-1] if model_type == \"tobit\" else X_columns[1:]  # é€‚åº”ä¸åŒæƒ…å†µ\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(f\"## {model_type.upper()}å›å½’ç»“æœ\\n\\n\")\n",
    "    f.write(f\"è¢«è§£é‡Šå˜é‡: {transformed_dv}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(results.summary()))\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "\n",
    "# æ¨¡å‹ç»“æœè§£è¯»\n",
    "print(\"\\næ¨¡å‹è§£è¯»:\")\n",
    "\n",
    "if model_type == \"ols\":\n",
    "    # OLSæ¨¡å‹ç‰¹æœ‰çš„è§£è¯»\n",
    "    print(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\")\n",
    "    print(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\")\n",
    "    print(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\")\n",
    "    \n",
    "    if results.f_pvalue < 0.05:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\")\n",
    "    else:\n",
    "        print(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\")\n",
    "    \n",
    "    # è·å–æ˜¾è‘—çš„å˜é‡\n",
    "    p_values = results.pvalues[1:]  # è·³è¿‡æˆªè·\n",
    "    var_names = all_predictors\n",
    "    \n",
    "else:  # Tobitæ¨¡å‹\n",
    "    print(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\")\n",
    "    print(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\")\n",
    "    print(f\"  - AIC: {results.aic:.4f}\")\n",
    "    print(f\"  - BIC: {results.bic:.4f}\")\n",
    "    \n",
    "    # Tobitæ¨¡å‹ä¸­è·å–ç³»æ•°å’Œpå€¼\n",
    "    p_values = results.pvalues[1:-1]  # è·³è¿‡æˆªè·å’Œsigma\n",
    "    var_names = list(X.columns[1:])  # è·³è¿‡å¸¸æ•°é¡¹\n",
    "\n",
    "# æ˜¾è‘—å˜é‡åˆ†æï¼ˆå¯¹ä¸¤ç§æ¨¡å‹é€šç”¨ï¼‰\n",
    "significant_mask = p_values < 0.05\n",
    "significant_vars = [var for var, sig in zip(var_names, significant_mask) if sig]\n",
    "print(f\"\\næ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\")\n",
    "\n",
    "iv_significant = [var for var in significant_vars if var in iv_list]\n",
    "ctrl_significant = [var for var in significant_vars if var in control_vars]\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„è‡ªå˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\")\n",
    "for var in iv_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ˜¾è‘—çš„æ§åˆ¶å˜é‡\n",
    "print(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\")\n",
    "for var in ctrl_significant:\n",
    "    idx = var_names.index(var)\n",
    "    coef = results.params[idx + 1]  # +1 to skip intercept\n",
    "    p = p_values[idx]\n",
    "    print(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\")\n",
    "\n",
    "# ä¿å­˜è§£è¯»ç»“æœ\n",
    "with open(result_file, 'a', encoding='utf-8') as f:\n",
    "    f.write(\"## æ¨¡å‹ç»“æœè§£è¯»\\n\\n\")\n",
    "    \n",
    "    if model_type == \"ols\":\n",
    "        f.write(f\"- æ¨¡å‹è§£é‡ŠåŠ› (RÂ²): {results.rsquared:.4f}\\n\")\n",
    "        f.write(f\"- è°ƒæ•´åçš„RÂ²: {results.rsquared_adj:.4f}\\n\")\n",
    "        f.write(f\"- æ¨¡å‹æ•´ä½“æ˜¾è‘—æ€§: F({results.df_model:.0f},{results.df_resid:.0f})={results.fvalue:.4f}, p={results.f_pvalue:.6f}\\n\")\n",
    "        \n",
    "        if results.f_pvalue < 0.05:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šæ˜¾è‘—\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"  ç»“è®º: æ¨¡å‹æ•´ä½“ä¸Šä¸æ˜¾è‘—\\n\\n\")\n",
    "    else:\n",
    "        f.write(\"- Tobitæ¨¡å‹ç»Ÿè®¡é‡:\\n\")\n",
    "        f.write(f\"  - å¯¹æ•°ä¼¼ç„¶å€¼: {results.llf:.4f}\\n\")\n",
    "        f.write(f\"  - AIC: {results.aic:.4f}\\n\")\n",
    "        f.write(f\"  - BIC: {results.bic:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„å˜é‡ (p < 0.05): {len(significant_vars)}/{len(var_names)}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"æ˜¾è‘—çš„è‡ªå˜é‡: {len(iv_significant)}/{len(iv_list)}\\n\")\n",
    "    for var in iv_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ˜¾è‘—çš„æ§åˆ¶å˜é‡: {len(ctrl_significant)}/{len(control_vars)}\\n\")\n",
    "    for var in ctrl_significant:\n",
    "        idx = var_names.index(var)\n",
    "        coef = results.params[idx + 1]\n",
    "        p = p_values[idx]\n",
    "        f.write(f\"- {var}: ç³»æ•°={coef:.4f}, p={p:.6f}\\n\")\n",
    "\n",
    "print(f\"\\næ‰€æœ‰åˆ†æç»“æœå·²ä¿å­˜åˆ° {result_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "450f06e6-3302-4e45-af91-b5634fcdf13b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:46:13.657581Z",
     "iopub.status.busy": "2025-04-15T10:46:13.657581Z",
     "iopub.status.idle": "2025-04-15T10:46:13.688071Z",
     "shell.execute_reply": "2025-04-15T10:46:13.688071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\n",
      "æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\n",
      "\n",
      "å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\n",
      "-----------------------------------\n",
      "æ€»è®¡ 20 ä¸ªæœ‰æ•ˆå˜é‡, æ— æ˜¾è‘—å˜é‡\n",
      "-----------------------------------\n",
      "\n",
      "è‡ªå˜é‡:\n",
      "å­£èŠ‚é€‚åº”æ€§è¡¨è¾¾: -0.0766 (å½’ä¸€åŒ–: -0.4111, 41.11%)\n",
      "è®¾è®¡ç¾æ„Ÿè¡¨è¾¾: -0.0312 (å½’ä¸€åŒ–: -0.1673, 16.73%)\n",
      "ç©ºé—´æ•ˆç‡è¡¨è¾¾: -0.0212 (å½’ä¸€åŒ–: -0.1137, 11.37%)\n",
      "å®‰å…¨æ€§éœ€æ±‚è¡¨è¾¾: -0.0127 (å½’ä¸€åŒ–: -0.0680, 6.80%)\n",
      "é¢œè‰²é€‰æ‹©è¡¨è¾¾: 0.0079 (å½’ä¸€åŒ–: 0.0426, 4.26%)\n",
      "ç»„åˆçµæ´»æ€§è¡¨è¾¾: 0.0062 (å½’ä¸€åŒ–: 0.0335, 3.35%)\n",
      "å¤šåŠŸèƒ½æ€§è¡¨è¾¾: -0.0054 (å½’ä¸€åŒ–: -0.0290, 2.90%)\n",
      "ç»„è£…ä¾¿æ·æ€§è¡¨è¾¾: 0.0039 (å½’ä¸€åŒ–: 0.0209, 2.09%)\n",
      "ä¾¿æºæ€§è¡¨è¾¾: 0.0039 (å½’ä¸€åŒ–: 0.0207, 2.07%)\n",
      "ç¯ä¿æè´¨åå¥½: -0.0037 (å½’ä¸€åŒ–: -0.0198, 1.98%)\n",
      "å“ç‰Œå£°èª‰è¡¨è¾¾: -0.0032 (å½’ä¸€åŒ–: -0.0174, 1.74%)\n",
      "åˆ›æ–°åŠŸèƒ½è¡¨è¾¾: 0.0027 (å½’ä¸€åŒ–: 0.0144, 1.44%)\n",
      "æ‰¿é‡èƒ½åŠ›è¡¨è¾¾: -0.0024 (å½’ä¸€åŒ–: -0.0126, 1.26%)\n",
      "è€ç”¨æ€§è¡¨è¾¾: 0.0019 (å½’ä¸€åŒ–: 0.0103, 1.03%)\n",
      "ä»·æ ¼è¡¨è¾¾: 0.0015 (å½’ä¸€åŒ–: 0.0080, 0.80%)\n",
      "å±‚é«˜å¯è°ƒèŠ‚æ€§è¡¨è¾¾: -0.0008 (å½’ä¸€åŒ–: -0.0045, 0.45%)\n",
      "å„¿ç«¥å‹å¥½è®¾è®¡è¡¨è¾¾: 0.0008 (å½’ä¸€åŒ–: 0.0043, 0.43%)\n",
      "é˜²å°˜åŠŸèƒ½è¡¨è¾¾: -0.0003 (å½’ä¸€åŒ–: -0.0017, 0.17%)\n",
      "\n",
      "æ§åˆ¶å˜é‡:\n",
      "author_friends_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "author_followers_cnt: 0.0000 (å½’ä¸€åŒ–: 0.0000, 0.00%)\n",
      "-----------------------------------\n",
      "* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ° ç”Ÿæˆç»“æœ/social_media/Share-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\n"
     ]
    }
   ],
   "source": [
    "# å½’ä¸€åŒ–æ˜¾è‘—ç³»æ•°å¹¶è¾“å‡ºç»“æœ\n",
    "def normalize_significant_coefficients(results, iv_list, control_vars=None, model_type=\"ols\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ˜¾è‘—å˜é‡çš„ç³»æ•°ï¼Œåˆ é™¤NaNå€¼ï¼ŒæŒ‰ç»å¯¹å€¼å¤§å°æ’åºå¹¶è¾“å‡ºç»“æœ\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - results: å›å½’ç»“æœå¯¹è±¡\n",
    "    - iv_list: è‡ªå˜é‡åˆ—è¡¨\n",
    "    - control_vars: æ§åˆ¶å˜é‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºNone\n",
    "    - model_type: æ¨¡å‹ç±»å‹ï¼Œ\"ols\"æˆ–\"tobit\"\n",
    "    - alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼Œé»˜è®¤0.05\n",
    "    \"\"\"\n",
    "    if control_vars is None:\n",
    "        control_vars = []\n",
    "    \n",
    "    all_vars = iv_list + control_vars\n",
    "    \n",
    "    # è·å–ç³»æ•°å’Œpå€¼ - éœ€è¦è€ƒè™‘ä¸åŒæ¨¡å‹ç±»å‹\n",
    "    if model_type.lower() == \"ols\":\n",
    "        # OLSæ¨¡å‹ - è·³è¿‡æˆªè·\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    else:\n",
    "        # Tobitæ¨¡å‹ - è·³è¿‡æˆªè·å’Œsigma\n",
    "        coefs = results.params[1:len(all_vars)+1]\n",
    "        p_values = results.pvalues[1:len(all_vars)+1]\n",
    "    \n",
    "    # åˆ›å»ºDataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'å˜é‡': all_vars,\n",
    "        'ç³»æ•°': coefs,\n",
    "        'på€¼': p_values,\n",
    "        'å˜é‡ç±»å‹': ['è‡ªå˜é‡' if var in iv_list else 'æ§åˆ¶å˜é‡' for var in all_vars]\n",
    "    })\n",
    "    \n",
    "    # è¿‡æ»¤æ‰NaNç³»æ•°å’Œéæ˜¾è‘—çš„å˜é‡\n",
    "    valid_coef_df = coef_df.dropna(subset=['ç³»æ•°'])\n",
    "    sig_coef_df = valid_coef_df[valid_coef_df['på€¼'] < alpha].copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æ˜¾è‘—å˜é‡ï¼Œåˆ™æŠ¥å‘Šæ‰€æœ‰æœ‰æ•ˆå˜é‡\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"æ³¨æ„: æ²¡æœ‰æ˜¾è‘—çš„å˜é‡ (p < 0.05)ï¼Œå°†ä½¿ç”¨æ‰€æœ‰éNaNç³»æ•°å˜é‡\")\n",
    "        sig_coef_df = valid_coef_df.copy()\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç³»æ•°ï¼Œé€€å‡º\n",
    "    if len(sig_coef_df) == 0:\n",
    "        print(\"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„ç³»æ•°ï¼Œæ— æ³•è¿›è¡Œå½’ä¸€åŒ–\")\n",
    "        return None\n",
    "    \n",
    "    # è®¡ç®—ç³»æ•°çš„ç»å¯¹å€¼\n",
    "    sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°'].abs()\n",
    "    \n",
    "    # å½’ä¸€åŒ–ç³»æ•°\n",
    "    total_abs = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'].sum()\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°'] = sig_coef_df['ç³»æ•°'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] = sig_coef_df['ç³»æ•°ç»å¯¹å€¼'] / total_abs\n",
    "    sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”'] = sig_coef_df['å½’ä¸€åŒ–ç³»æ•°ç»å¯¹å€¼'] * 100\n",
    "    \n",
    "    # æŒ‰ç³»æ•°ç»å¯¹å€¼æ’åº\n",
    "    sig_coef_df = sig_coef_df.sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\nå½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº):\")\n",
    "    print(\"-----------------------------------\")\n",
    "    significant_str = \"æ˜¾è‘— (p<0.05)\" if len(sig_coef_df[sig_coef_df['på€¼'] < alpha]) > 0 else \"æ— æ˜¾è‘—å˜é‡\"\n",
    "    print(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "    for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "        type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "        if len(type_df) > 0:\n",
    "            print(f\"\\n{var_type}:\")\n",
    "            for _, row in type_df.iterrows():\n",
    "                sig_mark = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                print(f\"{row['å˜é‡']}: {row['ç³»æ•°']:.4f} (å½’ä¸€åŒ–: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}, {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%){sig_mark}\")\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\")\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    output_file = os.path.join(folder_name, \"Share-å½’ä¸€åŒ–çš„æ¨¡å‹ç»“æœ.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {model_type.upper()} æ¨¡å‹å½’ä¸€åŒ–åçš„ç³»æ•° (æŒ‰ç»å¯¹å€¼æ’åº)\\n\\n\")\n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(f\"æ€»è®¡ {len(sig_coef_df)} ä¸ªæœ‰æ•ˆå˜é‡, {significant_str}\\n\")\n",
    "        f.write(\"-----------------------------------\\n\\n\")\n",
    "        \n",
    "        # æ·»åŠ æ€»è¡¨\n",
    "        f.write(\"| å˜é‡ | å˜é‡ç±»å‹ | ç³»æ•° | på€¼ | å½’ä¸€åŒ–ç³»æ•° | å½’ä¸€åŒ–ç™¾åˆ†æ¯” | æ˜¾è‘—æ€§ |\\n\")\n",
    "        f.write(\"|------|---------|------|------|------------|------------|--------|\\n\")\n",
    "        \n",
    "        for _, row in sig_coef_df.iterrows():\n",
    "            sig = \"æ˜¯\" if row['på€¼'] < alpha else \"å¦\"\n",
    "            f.write(f\"| {row['å˜é‡']} | {row['å˜é‡ç±»å‹']} | {row['ç³»æ•°']:.4f} | {row['på€¼']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°']:.4f} | {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}% | {sig} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## è¯¦ç»†åˆ†æ\\n\\n\")\n",
    "        \n",
    "        # å…ˆæ˜¾ç¤ºè‡ªå˜é‡ï¼Œå†æ˜¾ç¤ºæ§åˆ¶å˜é‡\n",
    "        for var_type in ['è‡ªå˜é‡', 'æ§åˆ¶å˜é‡']:\n",
    "            type_df = sig_coef_df[sig_coef_df['å˜é‡ç±»å‹'] == var_type]\n",
    "            if len(type_df) > 0:\n",
    "                f.write(f\"### {var_type}\\n\\n\")\n",
    "                \n",
    "                for _, row in type_df.iterrows():\n",
    "                    sig_symbol = \"*\" if row['på€¼'] < alpha else \"\"\n",
    "                    direction = \"æ­£å‘\" if row['ç³»æ•°'] > 0 else \"è´Ÿå‘\"\n",
    "                    f.write(f\"#### {row['å˜é‡']}{sig_symbol}\\n\\n\")\n",
    "                    f.write(f\"- ç³»æ•°: {row['ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- på€¼: {row['på€¼']:.4f}\\n\")\n",
    "                    f.write(f\"- å½±å“æ–¹å‘: {direction}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç³»æ•°: {row['å½’ä¸€åŒ–ç³»æ•°']:.4f}\\n\")\n",
    "                    f.write(f\"- å½’ä¸€åŒ–ç™¾åˆ†æ¯”: {row['å½’ä¸€åŒ–ç³»æ•°ç™¾åˆ†æ¯”']:.2f}%\\n\")\n",
    "                    f.write(f\"- æ˜¯å¦æ˜¾è‘—: {'æ˜¯' if row['på€¼'] < alpha else 'å¦'}\\n\\n\")\n",
    "        \n",
    "        f.write(\"-----------------------------------\\n\")\n",
    "        f.write(\"* æ ‡è®°è¡¨ç¤ºåœ¨0.05æ°´å¹³ä¸Šæ˜¾è‘—\\n\")\n",
    "    \n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ° {output_file}\")\n",
    "    \n",
    "    return sig_coef_df\n",
    "# è°ƒç”¨å½’ä¸€åŒ–å‡½æ•°\n",
    "print(\"\\nå¼€å§‹å½’ä¸€åŒ–ç³»æ•°åˆ†æ...\")\n",
    "normalized_coefs = normalize_significant_coefficients(\n",
    "    results=results,\n",
    "    iv_list=iv_list,\n",
    "    control_vars=control_vars,\n",
    "    model_type=model_type,\n",
    "    alpha=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07beb1-0309-4281-849c-f504fdef1279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe49d13-267c-4717-94a9-f481e21661ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lynx",
   "language": "python",
   "name": "lynx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
