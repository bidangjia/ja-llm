{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ae067e-b5af-49a1-9814-edc144293e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了 1 个处理过的Excel文件:\n",
      " - 环球一百狗窝打标_校对_processed.xlsx (已选择)\n",
      "\n",
      "Asin_List_file = \"环球一百狗窝打标_校对_processed.xlsx\"\n",
      "完整文件路径：Data\\环球一百狗窝打标_校对_processed.xlsx\n",
      "输出目录: 生成结果/social_media/\n",
      "输出目录: 生成结果/social_media/\n",
      "使用CUDA加速 - NVIDIA GeForce RTX 4090 D\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# 查找Data目录中所有以_processed.xlsx结尾的文件\n",
    "processed_files = glob.glob(os.path.join(\"Data\", \"*_processed.xlsx\"))\n",
    "\n",
    "# 确保找到了文件\n",
    "if not processed_files:\n",
    "    print(\"错误：在Data目录中没有找到任何以_processed.xlsx结尾的文件\")\n",
    "    Asin_List_file = None  # 设置为None表示未找到文件\n",
    "    input_file_path = None\n",
    "    input_file = None\n",
    "else:\n",
    "    # 获取第一个匹配文件的完整路径\n",
    "    input_file_path = processed_files[0]\n",
    "    \n",
    "    # 同时设置input_file为选定文件的路径字符串(不是列表)\n",
    "    input_file = input_file_path\n",
    "    \n",
    "    # 仅提取文件名（不包含路径）\n",
    "    Asin_List_file = os.path.basename(input_file_path)\n",
    "    \n",
    "    print(f\"找到了 {len(processed_files)} 个处理过的Excel文件:\")\n",
    "    for file in processed_files:\n",
    "        file_name = os.path.basename(file)\n",
    "        if file_name == Asin_List_file:\n",
    "            print(f\" - {file_name} (已选择)\")\n",
    "        else:\n",
    "            print(f\" - {file_name}\")\n",
    "    \n",
    "    print(f\"\\nAsin_List_file = \\\"{Asin_List_file}\\\"\")\n",
    "    print(f\"完整文件路径：{input_file}\")\n",
    "\n",
    "# 创建并保存输出目录路径为全局变量，以便在所有代码块中使用\n",
    "output_dir = '生成结果/social_media/'\n",
    "print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "# 创建目录（如果不存在）\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# # 从Excel文件名中提取基础文件夹名称\n",
    "# base_folder_name = os.path.splitext(input_file)[0]\n",
    "\n",
    "# # 创建目录（如果不存在）\n",
    "# if not os.path.exists(base_folder_name):\n",
    "#     os.makedirs(base_folder_name)\n",
    "\n",
    "# 创建并保存输出目录路径为全局变量，以便在所有代码块中使用\n",
    "output_dir = '生成结果/social_media/'\n",
    "print(f\"输出目录: {output_dir}\")\n",
    "\n",
    "# 创建目录（如果不存在）\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 检查GPU加速\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(f\"使用CUDA加速 - {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "        print(\"使用MPS加速\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(\"使用CPU\")\n",
    "    return device\n",
    "\n",
    "# 替换原来的设备检测\n",
    "device = get_device()\n",
    "\n",
    "# 清理文本数据\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # 转为小写\n",
    "    text = text.lower()\n",
    "    # 移除特殊字符和数字，但保留产品相关的关键字符如\"-\", \"+\", \"&\"\n",
    "    text = re.sub(r'[^\\w\\s\\-\\+&]', ' ', text)\n",
    "    # 移除多余空格\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 清理和规范化关键词\n",
    "def clean_keyword(keyword):\n",
    "    if not isinstance(keyword, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 删除常见的非规范开头字符\n",
    "    keyword = re.sub(r'^[\\'\"\\*\\-\\(\\)\\[\\]\\{\\}]+', '', keyword)\n",
    "    \n",
    "    # 删除常见的非规范结尾字符\n",
    "    keyword = re.sub(r'[\\'\"\\*\\-\\(\\)\\[\\]\\{\\}]+$', '', keyword)\n",
    "    \n",
    "    # 移除奇怪的符号和标点\n",
    "    keyword = re.sub(r'[^\\w\\s\\-]', '', keyword)\n",
    "    \n",
    "    # 规范化空格\n",
    "    keyword = re.sub(r'\\s+', ' ', keyword).strip()\n",
    "    \n",
    "    # 移除数字序号开头的情况\n",
    "    keyword = re.sub(r'^\\d+[\\.\\)\\-]\\s*', '', keyword)\n",
    "    \n",
    "    # 检查关键词长度和格式\n",
    "    if len(keyword) <= 1:\n",
    "        return \"\"\n",
    "    if keyword.count(' ') > 3:  # 不超过4个词的组合\n",
    "        return \"\"\n",
    "    if len(keyword) > 50:  # 不超过50个字符\n",
    "        return \"\"\n",
    "    \n",
    "    return keyword\n",
    "\n",
    "# 步骤2: 数据读取和预处理\n",
    "def load_and_preprocess_data():\n",
    "    # 根据文件扩展名决定读取方式\n",
    "    print(\"正在读取数据文件...\")\n",
    "    file_extension = os.path.splitext(input_file)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            df = pd.read_csv(input_file)\n",
    "            print(f\"成功读取CSV文件: {len(df)} 条记录\")\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(input_file, sheet_name=0)\n",
    "            print(f\"成功读取Excel文件: {len(df)} 条记录\")\n",
    "        else:\n",
    "            print(f\"不支持的文件格式: {file_extension}\")\n",
    "            df = pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    # 确保需要的列存在\n",
    "    required_columns = ['title', 'nodeLabelPath']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    \n",
    "    # 只汇总'title'和'nodeLabelPath'两列的文本\n",
    "    df['combined_text'] = df.apply(\n",
    "        lambda row: f\"{row['title']} {row['nodeLabelPath']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # 清理组合文本\n",
    "    df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "    \n",
    "    # 去重步骤\n",
    "    original_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['cleaned_text'])\n",
    "    removed_count = original_count - len(df)\n",
    "    print(f\"去重完成: 移除了 {removed_count} 条重复记录，剩余 {len(df)} 条记录\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 修改步骤3: 加载NLP模型（不使用spaCy）\n",
    "# 加载NLP模型时明确指定设备\n",
    "def load_nlp_models():\n",
    "    print(\"正在加载NLP模型...\")\n",
    "    \n",
    "    # 尝试使用spaCy\n",
    "    nlp = None\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"成功加载spaCy模型\")\n",
    "    except:\n",
    "        print(\"无法加载spaCy模型，将使用简单文本处理\")\n",
    "    \n",
    "    # 明确加载并移动模型到适当设备\n",
    "    print(f\"加载facebook/bart-large-mnli模型到 {device} 设备...\")\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=0 if device == \"cuda\" else device  # 对CUDA使用GPU索引\n",
    "    )\n",
    "    \n",
    "    return nlp, classifier\n",
    "\n",
    "# 修改步骤4: 从文本提取初始关键词（不依赖spaCy）\n",
    "def extract_initial_keywords(df, nlp):\n",
    "    print(\"提取初始关键词...\")\n",
    "    all_keywords = []\n",
    "    \n",
    "    # 如果spaCy可用，使用spaCy提取\n",
    "    if nlp is not None:\n",
    "        print(\"使用spaCy提取关键词\")\n",
    "        for _, row in df.iterrows():\n",
    "            cleaned_text = row['cleaned_text']\n",
    "            \n",
    "            # 只处理有意义的文本\n",
    "            if len(cleaned_text) > 5:\n",
    "                # 使用spaCy进行分析\n",
    "                doc = nlp(cleaned_text)\n",
    "                \n",
    "                # 提取名词和形容词\n",
    "                for token in doc:\n",
    "                    if token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and len(token.text) > 2:\n",
    "                        keyword = clean_keyword(token.lemma_)\n",
    "                        if keyword:\n",
    "                            all_keywords.append(keyword)\n",
    "                \n",
    "                # 提取名词短语\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    if len(chunk.text) > 2:\n",
    "                        keyword = clean_keyword(clean_text(chunk.text))\n",
    "                        if keyword:\n",
    "                            all_keywords.append(keyword)\n",
    "    else:\n",
    "        # 使用简单的文本分词方法\n",
    "        print(\"使用简单方法提取关键词\")\n",
    "        for _, row in df.iterrows():\n",
    "            cleaned_text = row['cleaned_text']\n",
    "            \n",
    "            # 只处理有意义的文本\n",
    "            if len(cleaned_text) > 5:\n",
    "                # 简单分词\n",
    "                words = cleaned_text.split()\n",
    "                \n",
    "                # 提取单词\n",
    "                for word in words:\n",
    "                    if len(word) > 2:  # 只保留长度大于2的词\n",
    "                        keyword = clean_keyword(word)\n",
    "                        if keyword:\n",
    "                            all_keywords.append(keyword)\n",
    "                \n",
    "                # 提取词组（简单的2-gram和3-gram）\n",
    "                for i in range(len(words)-1):\n",
    "                    if i < len(words)-1:\n",
    "                        bigram = f\"{words[i]} {words[i+1]}\"\n",
    "                        keyword = clean_keyword(bigram)\n",
    "                        if keyword:\n",
    "                            all_keywords.append(keyword)\n",
    "                    \n",
    "                    if i < len(words)-2:\n",
    "                        trigram = f\"{words[i]} {words[i+1]} {words[i+2]}\"\n",
    "                        keyword = clean_keyword(trigram)\n",
    "                        if keyword:\n",
    "                            all_keywords.append(keyword)\n",
    "    \n",
    "    return all_keywords\n",
    "\n",
    "# 步骤5: 使用TF-IDF提取重要关键词并自动生成产品标签\n",
    "def extract_tfidf_keywords_and_labels(df, max_features=200):\n",
    "    print(\"使用TF-IDF提取文档级关键词并生成产品标签...\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features, \n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)  # 提取1-gram和2-gram\n",
    "    )\n",
    "    \n",
    "    # 拟合并转换文档\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'].tolist())\n",
    "    \n",
    "    # 获取特征名称（关键词）\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # 计算每个词的平均TF-IDF得分\n",
    "    mean_tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "    \n",
    "    # 按得分排序\n",
    "    sorted_indices = np.argsort(mean_tfidf_scores)[::-1]\n",
    "    top_feature_indices = sorted_indices[:max_features]\n",
    "    \n",
    "    # 获取顶级关键词\n",
    "    top_keywords = []\n",
    "    for i in top_feature_indices:\n",
    "        keyword = clean_keyword(feature_names[i])\n",
    "        if keyword:\n",
    "            top_keywords.append(keyword)\n",
    "    \n",
    "    # 自动生成产品标签列表 - 使用TF-IDF得分最高的前10个词作为标签\n",
    "    # 不需要任何预定义的模式或关键词\n",
    "    product_labels = top_keywords[:10]\n",
    "    \n",
    "    print(f\"自动生成的产品标签列表: {product_labels}\")\n",
    "    \n",
    "    return top_keywords, product_labels\n",
    "\n",
    "# 步骤6: 使用BART-MNLI模型过滤关键词 - 优化版本\n",
    "def filter_keywords_with_bart(keywords, product_labels, classifier):\n",
    "    print(\"使用BART-MNLI模型过滤关键词...\")\n",
    "    \n",
    "    # 如果没有足够的产品标签，则使用关键词本身作为标签\n",
    "    if not product_labels:\n",
    "        print(\"没有足够的产品标签，使用关键词自身作为标签\")\n",
    "        counter = Counter(keywords)\n",
    "        product_labels = [kw for kw, _ in counter.most_common(10)]\n",
    "    \n",
    "    # 增加批量大小以充分利用GPU\n",
    "    batch_size = 128 if device == \"cuda\" else 20\n",
    "    total_batches = (len(keywords) + batch_size - 1) // batch_size\n",
    "    \n",
    "    filtered_keywords = []\n",
    "    \n",
    "    print(f\"处理 {len(keywords)} 个关键词，分为 {total_batches} 批\")\n",
    "    \n",
    "    # 为所有关键词准备查询\n",
    "    queries = [f\"Relevance of the term: {keyword}\" for keyword in keywords]\n",
    "    \n",
    "    # 批量处理关键词\n",
    "    for i in range(0, len(keywords), batch_size):\n",
    "        print(f\"处理批次 {i//batch_size + 1}/{total_batches}\")\n",
    "        batch_keywords = keywords[i:i+batch_size]\n",
    "        batch_queries = queries[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # 使用模型的批处理能力一次性处理整个批次\n",
    "            batch_results = classifier(\n",
    "                batch_queries, \n",
    "                candidate_labels=product_labels,\n",
    "                multi_label=True,\n",
    "                batch_size=32  # 控制内部批处理大小\n",
    "            )\n",
    "            \n",
    "            # 如果返回的不是列表（单个查询的情况），则转换为列表\n",
    "            if not isinstance(batch_results, list):\n",
    "                batch_results = [batch_results]\n",
    "            \n",
    "            # 处理每个结果\n",
    "            for j, result in enumerate(batch_results):\n",
    "                if max(result['scores']) > 0.3:\n",
    "                    filtered_keywords.append(batch_keywords[j])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理批次 {i//batch_size + 1} 时出错: {e}\")\n",
    "            # 出错时改为逐个处理\n",
    "            for j, keyword in enumerate(batch_keywords):\n",
    "                try:\n",
    "                    result = classifier(\n",
    "                        batch_queries[j],\n",
    "                        candidate_labels=product_labels,\n",
    "                        multi_label=True\n",
    "                    )\n",
    "                    if max(result['scores']) > 0.3:\n",
    "                        filtered_keywords.append(keyword)\n",
    "                except Exception as e:\n",
    "                    print(f\"过滤关键词时出错 '{keyword}': {e}\")\n",
    "    \n",
    "    print(f\"过滤后保留了 {len(filtered_keywords)} 个关键词\")\n",
    "    return filtered_keywords\n",
    "\n",
    "# 步骤7: 组合、去重和排序关键词\n",
    "def process_final_keywords(keywords):\n",
    "    # 再次清理每个关键词\n",
    "    cleaned_keywords = []\n",
    "    for kw in keywords:\n",
    "        cleaned = clean_keyword(kw)\n",
    "        if cleaned:\n",
    "            # 检查关键词的组成\n",
    "            words = cleaned.split()\n",
    "            if len(words) > 1:\n",
    "                # 检查多词组合中是否有无意义的词\n",
    "                if all(len(word) > 1 for word in words):\n",
    "                    cleaned_keywords.append(cleaned)\n",
    "            else:\n",
    "                # 单词关键词至少要有2个字符\n",
    "                if len(cleaned) >= 2:\n",
    "                    cleaned_keywords.append(cleaned)\n",
    "    \n",
    "    # 去重\n",
    "    unique_keywords = list(set(cleaned_keywords))\n",
    "    \n",
    "    # 按字母顺序排序\n",
    "    sorted_keywords = sorted(unique_keywords)\n",
    "    \n",
    "    return sorted_keywords\n",
    "\n",
    "# 步骤8: 保存关键词到文件\n",
    "def save_keywords(keywords):\n",
    "    print(\"正在保存关键词...\")\n",
    "    # 将初始关键词保存到临时文件\n",
    "    output_path = os.path.join(output_dir, 'keywords_raw.txt')\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for keyword in keywords:\n",
    "            # 确保每一行都是格式规范的关键词\n",
    "            f.write(f\"{keyword}\\n\")\n",
    "    \n",
    "    print(f\"关键词已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a71409ca-7117-4a2e-a2d4-cc6b1484eef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据文件...\n",
      "成功读取Excel文件: 202 条记录\n",
      "去重完成: 移除了 6 条重复记录，剩余 196 条记录\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>skuList</th>\n",
       "      <th>overviews</th>\n",
       "      <th>brand</th>\n",
       "      <th>brandUrl</th>\n",
       "      <th>title</th>\n",
       "      <th>asinUrl</th>\n",
       "      <th>imageUrl</th>\n",
       "      <th>parent</th>\n",
       "      <th>nodeLabelPath</th>\n",
       "      <th>...</th>\n",
       "      <th>Carrying Case</th>\n",
       "      <th>Straps (Carry/Shoulder)</th>\n",
       "      <th>dimensions.1</th>\n",
       "      <th>Height</th>\n",
       "      <th>Length</th>\n",
       "      <th>Length - Level</th>\n",
       "      <th>Width</th>\n",
       "      <th>InteriorPadHeight</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0D8H5ZSHC</td>\n",
       "      <td>Color: Black/Grey</td>\n",
       "      <td>Color:Black/Grey | Material:Velvet | Brand:Woo...</td>\n",
       "      <td>Wooaidagg</td>\n",
       "      <td>/stores/WOOAIDAGG/page/DA557548-E7D2-4825-93B1...</td>\n",
       "      <td>Dog Car Seat for Medium Sized Dog,Pet Travel C...</td>\n",
       "      <td>https://www.amazon.com/dp/B0D8H5ZSHC?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41UPRkZow+...</td>\n",
       "      <td>B0D5LYP1DH</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>20 x 20 x 20 inches</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.65</td>\n",
       "      <td>20.01-25</td>\n",
       "      <td>21.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dog Car Seat for Medium Sized Dog,Pet Travel C...</td>\n",
       "      <td>dog car seat for medium sized dog pet travel c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0DNSNMPWT</td>\n",
       "      <td>Pattern Name: BlackBrown</td>\n",
       "      <td>Color:BlackBrown | Brand:melafa365 | Maximum W...</td>\n",
       "      <td>melafa365</td>\n",
       "      <td>/stores/Qualityproductsqualityservice/page/B62...</td>\n",
       "      <td>Dog Car Seat for Small/Medium Dogs, Memory Foa...</td>\n",
       "      <td>https://www.amazon.com/dp/B0DNSNMPWT?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51j7IiGGHt...</td>\n",
       "      <td>B0DNSKSWPN</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>13.7 x 9 x 7 inches</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>17.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>Dog Car Seat for Small/Medium Dogs, Memory Foa...</td>\n",
       "      <td>dog car seat for small medium dogs memory foam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0DP4DDJJT</td>\n",
       "      <td>Color: Black/Grey | Size: Medium</td>\n",
       "      <td>Color:Black/Grey | Material:Velvet | Brand:IND...</td>\n",
       "      <td>INDYBUD</td>\n",
       "      <td>/stores/INDYBUD/page/7FC74FB8-CCA1-45CA-A23C-3...</td>\n",
       "      <td>Dog Booster Car Seat for Dogs Up to 35lbs, Saf...</td>\n",
       "      <td>https://www.amazon.com/dp/B0DP4DDJJT?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mSv17wdk...</td>\n",
       "      <td>B0DKXNXNQT</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>19 x 19 x 19 inches</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>19.00</td>\n",
       "      <td>5.91</td>\n",
       "      <td>Dog Booster Car Seat for Dogs Up to 35lbs, Saf...</td>\n",
       "      <td>dog booster car seat for dogs up to 35lbs safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0D1QYCCV2</td>\n",
       "      <td>Color: Black/Brown</td>\n",
       "      <td>Color:Black/Brown | Material:Velvet | Brand:Wo...</td>\n",
       "      <td>Wooaidagg</td>\n",
       "      <td>/stores/WOOAIDAGG/page/DA557548-E7D2-4825-93B1...</td>\n",
       "      <td>Dog Car Seat for Medium Sized Dog,Pet Travel C...</td>\n",
       "      <td>https://www.amazon.com/dp/B0D1QYCCV2?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41NXcIa4I3...</td>\n",
       "      <td>B0D5LYP1DH</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>20 x 20 x 20 inches</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>20.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dog Car Seat for Medium Sized Dog,Pet Travel C...</td>\n",
       "      <td>dog car seat for medium sized dog pet travel c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B09H26QDXG</td>\n",
       "      <td>Material Type: Cationic Blue | Pattern Name: U...</td>\n",
       "      <td>Color:Dark Blue | Material:Cationic Blue | Bra...</td>\n",
       "      <td>GENORTH</td>\n",
       "      <td>/GENORTH/b/ref=bl_dp_s_web_119204902011?ie=UTF...</td>\n",
       "      <td>Dog Car Seats for Small and Medium Dogs,Portab...</td>\n",
       "      <td>https://www.amazon.com/dp/B09H26QDXG?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/412zzYGgSn...</td>\n",
       "      <td>B0CKRBQYBJ</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>18.9 x 14.96 x 10.62 inches</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.90</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>14.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dog Car Seats for Small and Medium Dogs,Portab...</td>\n",
       "      <td>dog car seats for small and medium dogs portab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>B0CRB9NRSV</td>\n",
       "      <td>Color: Black</td>\n",
       "      <td>Color:Black | Material:Short plush | Brand:NEE...</td>\n",
       "      <td>NEEZUKAR</td>\n",
       "      <td>/stores/NEEZUKAR/page/0FCDAB2E-9534-4BE5-B71D-...</td>\n",
       "      <td>Dog Car Seat for Large Medium Dogs,Portable Wa...</td>\n",
       "      <td>https://www.amazon.com/dp/B0CRB9NRSV?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41MjhbAeCc...</td>\n",
       "      <td>B0CW98M1LG</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>29.5 x 20 x 20 inches</td>\n",
       "      <td>20.00</td>\n",
       "      <td>29.50</td>\n",
       "      <td>Above 25.01</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Dog Car Seat for Large Medium Dogs,Portable Wa...</td>\n",
       "      <td>dog car seat for large medium dogs portable wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>B0CRBBKV6K</td>\n",
       "      <td>Color: Black</td>\n",
       "      <td>Brand:GL GLENSLAVE | Breed Recommendation:smal...</td>\n",
       "      <td>GL GLENSLAVE</td>\n",
       "      <td>/stores/GLGLENSLAVE/page/AF5DA707-F137-4FFF-84...</td>\n",
       "      <td>Dog Car Seat for Small Medium Dogs, Memory Foa...</td>\n",
       "      <td>https://www.amazon.com/dp/B0CRBBKV6K?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41u4KEsHDn...</td>\n",
       "      <td>B0CRD2FK2H</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>19.69 x 17.72 x 11.81 inches</td>\n",
       "      <td>11.70</td>\n",
       "      <td>19.70</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>17.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dog Car Seat for Small Medium Dogs, Memory Foa...</td>\n",
       "      <td>dog car seat for small medium dogs memory foam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>B0CSSK989S</td>\n",
       "      <td>Color: Blue</td>\n",
       "      <td>Color:Blue | Material:Polyester | Brand:Guloko...</td>\n",
       "      <td>Gulokoka</td>\n",
       "      <td>/Gulokoka/b/ref=bl_dp_s_web_36890320011?ie=UTF...</td>\n",
       "      <td>Dog Car Seat for Small Dogs, Small Dog Booster...</td>\n",
       "      <td>https://www.amazon.com/dp/B0CSSK989S?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41udyDUeGr...</td>\n",
       "      <td>B0CSSJY7N9</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Yes']</td>\n",
       "      <td>17.01 x 6.89 x 6.85 inches</td>\n",
       "      <td>13.00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>20.01-25</td>\n",
       "      <td>19.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Dog Car Seat for Small Dogs, Small Dog Booster...</td>\n",
       "      <td>dog car seat for small dogs small dog booster ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>B0CSY76B8L</td>\n",
       "      <td>Color: Grey | Size: M</td>\n",
       "      <td>Color:Grey | Material:Metal, Plastic, Velvet, ...</td>\n",
       "      <td>Ytmuzic</td>\n",
       "      <td>/Ytmuzic/b/ref=bl_dp_s_web_121070718011?ie=UTF...</td>\n",
       "      <td>Dog Car Seat for Small Medium Dogs, Pet Car Se...</td>\n",
       "      <td>https://www.amazon.com/dp/B0CSY76B8L?psc=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41Ibg4XcCk...</td>\n",
       "      <td>B0D78B6X1N</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>18.9 x 18.9 x 19 inches</td>\n",
       "      <td>18.90</td>\n",
       "      <td>19.00</td>\n",
       "      <td>15.01-20</td>\n",
       "      <td>18.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dog Car Seat for Small Medium Dogs, Pet Car Se...</td>\n",
       "      <td>dog car seat for small medium dogs pet car sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>B0CTGV5PJF</td>\n",
       "      <td>Pattern Name: 23‘’ x 22'' x 13''</td>\n",
       "      <td>Brand:Heeyoo | Breed Recommendation:Small Bree...</td>\n",
       "      <td>Heeyoo</td>\n",
       "      <td>/stores/Heeyoo/page/A1F923D9-5A8A-40A5-862F-04...</td>\n",
       "      <td>Heeyoo Dog Car Seat for Small Dogs, Portable D...</td>\n",
       "      <td>https://www.amazon.com/dp/B0CTGV5PJF?th=1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41fAHSfJg3...</td>\n",
       "      <td>B0CTGV5PJF</td>\n",
       "      <td>Pet Supplies:Dogs:Carriers &amp; Travel Products:C...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>['Not Mention']</td>\n",
       "      <td>16.89 x 9.8 x 7.76 inches</td>\n",
       "      <td>13.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>20.01-25</td>\n",
       "      <td>22.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heeyoo Dog Car Seat for Small Dogs, Portable D...</td>\n",
       "      <td>heeyoo dog car seat for small dogs portable do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           asin                                            skuList  \\\n",
       "0    B0D8H5ZSHC                                  Color: Black/Grey   \n",
       "1    B0DNSNMPWT                           Pattern Name: BlackBrown   \n",
       "2    B0DP4DDJJT                   Color: Black/Grey | Size: Medium   \n",
       "3    B0D1QYCCV2                                 Color: Black/Brown   \n",
       "4    B09H26QDXG  Material Type: Cationic Blue | Pattern Name: U...   \n",
       "..          ...                                                ...   \n",
       "197  B0CRB9NRSV                                       Color: Black   \n",
       "198  B0CRBBKV6K                                       Color: Black   \n",
       "199  B0CSSK989S                                        Color: Blue   \n",
       "200  B0CSY76B8L                              Color: Grey | Size: M   \n",
       "201  B0CTGV5PJF                   Pattern Name: 23‘’ x 22'' x 13''   \n",
       "\n",
       "                                             overviews         brand  \\\n",
       "0    Color:Black/Grey | Material:Velvet | Brand:Woo...     Wooaidagg   \n",
       "1    Color:BlackBrown | Brand:melafa365 | Maximum W...     melafa365   \n",
       "2    Color:Black/Grey | Material:Velvet | Brand:IND...       INDYBUD   \n",
       "3    Color:Black/Brown | Material:Velvet | Brand:Wo...     Wooaidagg   \n",
       "4    Color:Dark Blue | Material:Cationic Blue | Bra...       GENORTH   \n",
       "..                                                 ...           ...   \n",
       "197  Color:Black | Material:Short plush | Brand:NEE...      NEEZUKAR   \n",
       "198  Brand:GL GLENSLAVE | Breed Recommendation:smal...  GL GLENSLAVE   \n",
       "199  Color:Blue | Material:Polyester | Brand:Guloko...      Gulokoka   \n",
       "200  Color:Grey | Material:Metal, Plastic, Velvet, ...       Ytmuzic   \n",
       "201  Brand:Heeyoo | Breed Recommendation:Small Bree...        Heeyoo   \n",
       "\n",
       "                                              brandUrl  \\\n",
       "0    /stores/WOOAIDAGG/page/DA557548-E7D2-4825-93B1...   \n",
       "1    /stores/Qualityproductsqualityservice/page/B62...   \n",
       "2    /stores/INDYBUD/page/7FC74FB8-CCA1-45CA-A23C-3...   \n",
       "3    /stores/WOOAIDAGG/page/DA557548-E7D2-4825-93B1...   \n",
       "4    /GENORTH/b/ref=bl_dp_s_web_119204902011?ie=UTF...   \n",
       "..                                                 ...   \n",
       "197  /stores/NEEZUKAR/page/0FCDAB2E-9534-4BE5-B71D-...   \n",
       "198  /stores/GLGLENSLAVE/page/AF5DA707-F137-4FFF-84...   \n",
       "199  /Gulokoka/b/ref=bl_dp_s_web_36890320011?ie=UTF...   \n",
       "200  /Ytmuzic/b/ref=bl_dp_s_web_121070718011?ie=UTF...   \n",
       "201  /stores/Heeyoo/page/A1F923D9-5A8A-40A5-862F-04...   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Dog Car Seat for Medium Sized Dog,Pet Travel C...   \n",
       "1    Dog Car Seat for Small/Medium Dogs, Memory Foa...   \n",
       "2    Dog Booster Car Seat for Dogs Up to 35lbs, Saf...   \n",
       "3    Dog Car Seat for Medium Sized Dog,Pet Travel C...   \n",
       "4    Dog Car Seats for Small and Medium Dogs,Portab...   \n",
       "..                                                 ...   \n",
       "197  Dog Car Seat for Large Medium Dogs,Portable Wa...   \n",
       "198  Dog Car Seat for Small Medium Dogs, Memory Foa...   \n",
       "199  Dog Car Seat for Small Dogs, Small Dog Booster...   \n",
       "200  Dog Car Seat for Small Medium Dogs, Pet Car Se...   \n",
       "201  Heeyoo Dog Car Seat for Small Dogs, Portable D...   \n",
       "\n",
       "                                        asinUrl  \\\n",
       "0    https://www.amazon.com/dp/B0D8H5ZSHC?psc=1   \n",
       "1    https://www.amazon.com/dp/B0DNSNMPWT?psc=1   \n",
       "2    https://www.amazon.com/dp/B0DP4DDJJT?psc=1   \n",
       "3    https://www.amazon.com/dp/B0D1QYCCV2?psc=1   \n",
       "4    https://www.amazon.com/dp/B09H26QDXG?psc=1   \n",
       "..                                          ...   \n",
       "197  https://www.amazon.com/dp/B0CRB9NRSV?psc=1   \n",
       "198  https://www.amazon.com/dp/B0CRBBKV6K?psc=1   \n",
       "199  https://www.amazon.com/dp/B0CSSK989S?psc=1   \n",
       "200  https://www.amazon.com/dp/B0CSY76B8L?psc=1   \n",
       "201   https://www.amazon.com/dp/B0CTGV5PJF?th=1   \n",
       "\n",
       "                                              imageUrl      parent  \\\n",
       "0    https://m.media-amazon.com/images/I/41UPRkZow+...  B0D5LYP1DH   \n",
       "1    https://m.media-amazon.com/images/I/51j7IiGGHt...  B0DNSKSWPN   \n",
       "2    https://m.media-amazon.com/images/I/41mSv17wdk...  B0DKXNXNQT   \n",
       "3    https://m.media-amazon.com/images/I/41NXcIa4I3...  B0D5LYP1DH   \n",
       "4    https://m.media-amazon.com/images/I/412zzYGgSn...  B0CKRBQYBJ   \n",
       "..                                                 ...         ...   \n",
       "197  https://m.media-amazon.com/images/I/41MjhbAeCc...  B0CW98M1LG   \n",
       "198  https://m.media-amazon.com/images/I/41u4KEsHDn...  B0CRD2FK2H   \n",
       "199  https://m.media-amazon.com/images/I/41udyDUeGr...  B0CSSJY7N9   \n",
       "200  https://m.media-amazon.com/images/I/41Ibg4XcCk...  B0D78B6X1N   \n",
       "201  https://m.media-amazon.com/images/I/41fAHSfJg3...  B0CTGV5PJF   \n",
       "\n",
       "                                         nodeLabelPath  ...    Carrying Case  \\\n",
       "0    Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "1    Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "2    Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "3    Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "4    Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "..                                                 ...  ...              ...   \n",
       "197  Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "198  Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "199  Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "200  Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "201  Pet Supplies:Dogs:Carriers & Travel Products:C...  ...  ['Not Mention']   \n",
       "\n",
       "     Straps (Carry/Shoulder)                  dimensions.1  Height Length  \\\n",
       "0            ['Not Mention']           20 x 20 x 20 inches   19.69  21.65   \n",
       "1            ['Not Mention']           13.7 x 9 x 7 inches   19.00  17.00   \n",
       "2            ['Not Mention']           19 x 19 x 19 inches   19.00  19.00   \n",
       "3            ['Not Mention']           20 x 20 x 20 inches   20.00  20.00   \n",
       "4            ['Not Mention']   18.9 x 14.96 x 10.62 inches   10.60  18.90   \n",
       "..                       ...                           ...     ...    ...   \n",
       "197          ['Not Mention']         29.5 x 20 x 20 inches   20.00  29.50   \n",
       "198          ['Not Mention']  19.69 x 17.72 x 11.81 inches   11.70  19.70   \n",
       "199                  ['Yes']    17.01 x 6.89 x 6.85 inches   13.00  20.50   \n",
       "200          ['Not Mention']       18.9 x 18.9 x 19 inches   18.90  19.00   \n",
       "201          ['Not Mention']     16.89 x 9.8 x 7.76 inches   13.00  23.00   \n",
       "\n",
       "    Length - Level  Width  InteriorPadHeight  \\\n",
       "0         20.01-25  21.26                NaN   \n",
       "1         15.01-20  17.00               6.00   \n",
       "2         15.01-20  19.00               5.91   \n",
       "3         15.01-20  20.00                NaN   \n",
       "4         15.01-20  14.96                NaN   \n",
       "..             ...    ...                ...   \n",
       "197    Above 25.01  20.00               4.00   \n",
       "198       15.01-20  17.70                NaN   \n",
       "199       20.01-25  19.00               3.00   \n",
       "200       15.01-20  18.90                NaN   \n",
       "201       20.01-25  22.00                NaN   \n",
       "\n",
       "                                         combined_text  \\\n",
       "0    Dog Car Seat for Medium Sized Dog,Pet Travel C...   \n",
       "1    Dog Car Seat for Small/Medium Dogs, Memory Foa...   \n",
       "2    Dog Booster Car Seat for Dogs Up to 35lbs, Saf...   \n",
       "3    Dog Car Seat for Medium Sized Dog,Pet Travel C...   \n",
       "4    Dog Car Seats for Small and Medium Dogs,Portab...   \n",
       "..                                                 ...   \n",
       "197  Dog Car Seat for Large Medium Dogs,Portable Wa...   \n",
       "198  Dog Car Seat for Small Medium Dogs, Memory Foa...   \n",
       "199  Dog Car Seat for Small Dogs, Small Dog Booster...   \n",
       "200  Dog Car Seat for Small Medium Dogs, Pet Car Se...   \n",
       "201  Heeyoo Dog Car Seat for Small Dogs, Portable D...   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    dog car seat for medium sized dog pet travel c...  \n",
       "1    dog car seat for small medium dogs memory foam...  \n",
       "2    dog booster car seat for dogs up to 35lbs safe...  \n",
       "3    dog car seat for medium sized dog pet travel c...  \n",
       "4    dog car seats for small and medium dogs portab...  \n",
       "..                                                 ...  \n",
       "197  dog car seat for large medium dogs portable wa...  \n",
       "198  dog car seat for small medium dogs memory foam...  \n",
       "199  dog car seat for small dogs small dog booster ...  \n",
       "200  dog car seat for small medium dogs pet car sea...  \n",
       "201  heeyoo dog car seat for small dogs portable do...  \n",
       "\n",
       "[196 rows x 163 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_and_preprocess_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5c262e-ded6-45a6-808a-4555e22e206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据文件...\n",
      "成功读取Excel文件: 202 条记录\n",
      "去重完成: 移除了 6 条重复记录，剩余 196 条记录\n",
      "正在加载NLP模型...\n",
      "成功加载spaCy模型\n",
      "加载facebook/bart-large-mnli模型到 cuda 设备...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取初始关键词...\n",
      "使用spaCy提取关键词\n",
      "使用TF-IDF提取文档级关键词并生成产品标签...\n",
      "自动生成的产品标签列表: ['car', 'travel', 'seat', 'dog', 'dogs', 'booster', 'pet', 'car seat', 'seats', 'dog car']\n",
      "使用BART-MNLI模型过滤关键词...\n",
      "处理 734 个关键词，分为 6 批\n",
      "处理批次 1/6\n",
      "处理批次 2/6\n",
      "处理批次 3/6\n",
      "处理批次 4/6\n",
      "处理批次 5/6\n",
      "处理批次 6/6\n",
      "过滤后保留了 561 个关键词\n",
      "正在保存关键词...\n",
      "关键词已保存到 生成结果/social_media/keywords_raw.txt\n"
     ]
    }
   ],
   "source": [
    "# 步骤2: 加载和预处理数据\n",
    "df = load_and_preprocess_data()\n",
    "\n",
    "# 步骤3: 加载NLP模型\n",
    "nlp, classifier = load_nlp_models()\n",
    "\n",
    "# 步骤4: 提取初始关键词\n",
    "initial_keywords = extract_initial_keywords(df, nlp)\n",
    "\n",
    "# 步骤5: 提取TF-IDF关键词并生成产品标签\n",
    "tfidf_keywords, product_labels = extract_tfidf_keywords_and_labels(df)\n",
    "\n",
    "# 步骤6: 使用BART-MNLI模型过滤关键词\n",
    "combined_keywords = list(set(initial_keywords + tfidf_keywords))\n",
    "filtered_keywords = filter_keywords_with_bart(combined_keywords, product_labels, classifier)\n",
    "\n",
    "# 步骤7: 整合、清理和排序关键词\n",
    "final_keywords = process_final_keywords(filtered_keywords)\n",
    "\n",
    "# 步骤8: 保存关键词\n",
    "save_keywords(final_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4b1000-d935-4fbc-8692-ec83a54e35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(product_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27b9cbc-84a9-43a8-8b88-33c2c50deaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CUDA加速\n",
      "开始处理文件: 生成结果\\social_media\\keywords_raw.txt\n",
      "成功加载 549 个关键词\n",
      "第一阶段：格式清理\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "格式清理: 100%|██████████████████████████████████████████████████████████████████| 549/549 [00:00<00:00, 109692.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "格式清理后保留: 484/549 关键词\n",
      "\n",
      "第二阶段：语义过滤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "模式过滤: 100%|██████████████████████████████████████████████████████████████████| 484/484 [00:00<00:00, 193263.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模式过滤后保留: 335/484 关键词\n",
      "成功保存 335 个关键词到 生成结果\\social_media\\keywords_temp1.txt\n",
      "关键词清理完成!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 确定处理设备\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"使用MPS加速\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"使用CUDA加速\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"使用CPU\")\n",
    "\n",
    "# 从Excel文件名中获取基础文件夹名称\n",
    "excel_file = Asin_List_file\n",
    "#base_folder_name = os.path.splitext(excel_file)[0]\n",
    "\n",
    "base_folder_name = os.path.join(\"生成结果\", \"social_media\")\n",
    "\n",
    "# 配置文件路径\n",
    "input_file = os.path.join(base_folder_name, \"keywords_raw.txt\")\n",
    "# 创建一个临时文件\n",
    "temp_output_file = os.path.join(base_folder_name, \"keywords_temp1.txt\")\n",
    "\n",
    "def clean_keyword_format(keyword):\n",
    "    \"\"\"第一阶段：清理关键词的基本格式问题\"\"\"\n",
    "    if not isinstance(keyword, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 原始关键词备份\n",
    "    original = keyword\n",
    "    \n",
    "    # 基本清理\n",
    "    keyword = keyword.strip().lower()\n",
    "    keyword = re.sub(r'\\s+', ' ', keyword)\n",
    "    \n",
    "    # 删除非标准字符\n",
    "    keyword = re.sub(r'[^\\w\\s\\-]', '', keyword)\n",
    "    \n",
    "    # 空字符串检查\n",
    "    if not keyword:\n",
    "        return \"\"\n",
    "    \n",
    "    # 移除数字前缀和单位\n",
    "    if re.match(r'^\\d+', keyword):\n",
    "        # 移除纯数字+单位格式\n",
    "        if re.match(r'^\\d+\\s*(inch|inche|cm|mm|pcs|pc|piece|pieces)$', keyword):\n",
    "            return \"\"\n",
    "        \n",
    "        # 提取有实际意义的部分\n",
    "        cleaned = re.sub(r'^\\d+\\s*(inch|inche|cm|mm|pcs|pc|piece|pieces)\\s*', '', keyword)\n",
    "        if cleaned and len(cleaned) > 2:\n",
    "            keyword = cleaned\n",
    "    \n",
    "    # 移除尾部的pcs/pc等标记\n",
    "    if re.search(r'\\b(pcs|pc|piece|pieces)$', keyword):\n",
    "        keyword = re.sub(r'\\s*(pcs|pc|piece|pieces)$', '', keyword)\n",
    "    \n",
    "    # 移除商品数量标记（如2pcs, 3pcs等）\n",
    "    if re.search(r'^\\d+(pcs|pc)\\b', keyword):\n",
    "        keyword = re.sub(r'^\\d+(pcs|pc)\\s*', '', keyword)\n",
    "    \n",
    "    # 如果清理后过短，则放弃\n",
    "    if len(keyword) < 3:\n",
    "        return \"\"\n",
    "    \n",
    "    # 检查是否只剩下了代码/ID等无意义字符\n",
    "    if re.match(r'^[a-z0-9]+$', keyword) and len(keyword) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    return keyword\n",
    "\n",
    "def load_keywords(filepath):\n",
    "    \"\"\"从文件加载关键词\"\"\"\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                keyword = line.strip()\n",
    "                if keyword:\n",
    "                    keywords.append(keyword)\n",
    "        print(f\"成功加载 {len(keywords)} 个关键词\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {e}\")\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def create_custom_classifier():\n",
    "    \"\"\"创建自定义分类器，使用更适合的预训练模型\"\"\"\n",
    "    print(\"加载预训练模型...\")\n",
    "    \n",
    "    # 使用RoBERTa模型，它对细微语义差异更敏感\n",
    "    model_name = \"roberta-large\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "    # 将模型移至适当设备\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 创建分类器\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device if device != \"mps\" else -1  # MPS需要特殊处理\n",
    "    )\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def prepare_classifier_prompt(keyword):\n",
    "    \"\"\"准备分类器提示，更精确地指导模型判断\"\"\"\n",
    "    # 构建提示模板\n",
    "    prompts = [\n",
    "        f\"Is '{keyword}' a specific product feature, component, or attribute that can be used to search for products on social media? Answer: \",\n",
    "        f\"Would '{keyword}' help identify a specific product category or type when searching on social media? Answer: \",\n",
    "        f\"Is '{keyword}' a descriptive term about a product's physical characteristics rather than just its potential users? Answer: \"\n",
    "    ]\n",
    "    \n",
    "    # 随机选择一个提示以增加多样性\n",
    "    return random.choice(prompts)\n",
    "\n",
    "def filter_keywords_with_custom_logic(keywords):\n",
    "    \"\"\"使用自定义逻辑过滤关键词\"\"\"\n",
    "    # 第一阶段：格式清理\n",
    "    print(\"第一阶段：格式清理\")\n",
    "    format_cleaned = []\n",
    "    rejected_format = []\n",
    "    \n",
    "    for keyword in tqdm(keywords, desc=\"格式清理\"):\n",
    "        cleaned = clean_keyword_format(keyword)\n",
    "        if cleaned:\n",
    "            format_cleaned.append(cleaned)\n",
    "        else:\n",
    "            rejected_format.append(keyword)\n",
    "    \n",
    "    print(f\"格式清理后保留: {len(format_cleaned)}/{len(keywords)} 关键词\")\n",
    "    \n",
    "    # 第二阶段：语义过滤\n",
    "    print(\"\\n第二阶段：语义过滤\")\n",
    "    \n",
    "    # 定义明确的排除模式\n",
    "    exclude_patterns = [\n",
    "        r'^(boy|girl|adult|kids|children|teen|baby|women|men)s?$',  # 纯人群词\n",
    "        r'^[a-z]$',  # 单个字母\n",
    "        r'^\\d+$',  # 纯数字\n",
    "        r'^[0-9a-z]{5,10}$',  # 可能是产品代码\n",
    "        r'^(amazon|ebay|walmart|target)$',  # 商店名称\n",
    "        r'^(made in|shipping|delivery)$'  # 物流词\n",
    "    ]\n",
    "    \n",
    "    # 手动过滤明确的模式\n",
    "    filtered_keywords = []\n",
    "    rejected_patterns = []\n",
    "    \n",
    "    for keyword in tqdm(format_cleaned, desc=\"模式过滤\"):\n",
    "        # 检查是否匹配任何排除模式\n",
    "        if any(re.match(pattern, keyword) for pattern in exclude_patterns):\n",
    "            rejected_patterns.append(keyword)\n",
    "            continue\n",
    "        \n",
    "        filtered_keywords.append(keyword)\n",
    "    \n",
    "    print(f\"模式过滤后保留: {len(filtered_keywords)}/{len(format_cleaned)} 关键词\")\n",
    "    \n",
    "    # # 第三阶段：使用预训练模型进行高级过滤\n",
    "    # print(\"\\n第三阶段：高级语义过滤\")\n",
    "    \n",
    "    # # 加载自定义分类器\n",
    "    # classifier = create_custom_classifier()\n",
    "    \n",
    "    # semantic_filtered = []\n",
    "    # rejected_semantic = []\n",
    "    \n",
    "    # # 批量处理以提高效率\n",
    "    # batch_size = 16\n",
    "    \n",
    "    # # 对剩余关键词进行语义过滤\n",
    "    # for i in tqdm(range(0, len(filtered_keywords), batch_size), desc=\"语义过滤\"):\n",
    "    #     batch = filtered_keywords[i:i+batch_size]\n",
    "        \n",
    "    #     for keyword in batch:\n",
    "    #         # 构建提示\n",
    "    #         prompt = prepare_classifier_prompt(keyword)\n",
    "            \n",
    "    #         try:\n",
    "    #             # 使用模型检查关键词的相关性\n",
    "    #             time.sleep(0.05)  # 轻微延迟以避免设备过载\n",
    "    #             result = []\n",
    "                \n",
    "    #             # 判定关键词是否与产品相关\n",
    "    #             is_related = False\n",
    "                \n",
    "    #             # 使用简单启发式规则优先处理明显的产品关键词\n",
    "    #             if any(product_term in keyword for product_term in ['bag', 'backpack', 'case', 'holder', 'pack', 'product', 'accessory']):\n",
    "    #                 is_related = True\n",
    "    #             else:\n",
    "    #                 # 让模型判断更复杂的情况\n",
    "    #                 result = classifier(prompt)\n",
    "    #                 label = result[0]['label']\n",
    "    #                 score = result[0]['score']\n",
    "                    \n",
    "    #                 # RoBERTa的标签判断\n",
    "    #                 is_related = (label == \"LABEL_1\" and score > 0.85) or (label == \"LABEL_0\" and score < 0.25)\n",
    "                \n",
    "    #             if is_related:\n",
    "    #                 semantic_filtered.append(keyword)\n",
    "    #             else:\n",
    "    #                 rejected_semantic.append(keyword)\n",
    "                    \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"\\n处理关键词时出错 '{keyword}': {e}\")\n",
    "    #             # 出错时保守地保留关键词\n",
    "    #             semantic_filtered.append(keyword)\n",
    "    \n",
    "    # print(f\"语义过滤后保留: {len(semantic_filtered)}/{len(filtered_keywords)} 关键词\")\n",
    "    \n",
    "    # # 打印一些被过滤的示例\n",
    "    # if rejected_semantic:\n",
    "    #     sample_size = min(10, len(rejected_semantic))\n",
    "    #     print(f\"\\n被过滤掉的示例: {random.sample(rejected_semantic, sample_size)}\")\n",
    "    \n",
    "    # return semantic_filtered\n",
    "    return filtered_keywords\n",
    "\n",
    "def save_keywords(keywords, filepath):\n",
    "    \"\"\"保存过滤后的关键词到文件\"\"\"\n",
    "    # 按字母顺序排序\n",
    "    sorted_keywords = sorted(set(keywords))\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for keyword in sorted_keywords:\n",
    "                f.write(f\"{keyword}\\n\")\n",
    "        print(f\"成功保存 {len(sorted_keywords)} 个关键词到 {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件时出错: {e}\")\n",
    "\n",
    "# 直接执行流程\n",
    "print(f\"开始处理文件: {input_file}\")\n",
    "\n",
    "# 1. 加载关键词\n",
    "keywords = load_keywords(input_file)\n",
    "if keywords:\n",
    "    # 2. 使用增强过滤逻辑过滤关键词\n",
    "    filtered_keywords = filter_keywords_with_custom_logic(keywords)\n",
    "    \n",
    "    # 3. 保存结果\n",
    "    save_keywords(filtered_keywords, temp_output_file)\n",
    "    \n",
    "    print(\"关键词清理完成!\")\n",
    "else:\n",
    "    print(\"没有找到关键词，退出程序\")\n",
    "\n",
    "# # 清理临时文件\n",
    "# try:\n",
    "#     os.remove(os.path.join(base_folder_name, \"keywords_temp1.txt\"))\n",
    "#     os.remove(os.path.join(base_folder_name, \"keywords_raw.txt\"))\n",
    "#     print(\"已清理临时文件\")\n",
    "# except Exception as e:\n",
    "#     print(f\"临时文件清理失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8378bae7-9423-46a5-9148-1c188a258d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载NLP模型...\n",
      "使用的产品标签 (用于上下文和单字过滤): ['car', 'travel', 'seat', 'dog', 'dogs', 'booster', 'pet', 'car seat', 'seats', 'dog car']\n",
      "\n",
      "--- 开始处理 ---\n",
      "输入文件: 生成结果\\social_media\\keywords_temp1.txt\n",
      "输出文件: 生成结果\\social_media\\keywords.txt\n",
      "结果文件夹: 生成结果\\social_media\n",
      "成功加载并初步清洗 335 个关键词\n",
      "\n",
      "--- 步骤 1: 提取核心关键词 (含前缀移除、单字标签过滤、TF-IDF去重) ---\n",
      "\n",
      "将尝试过滤与以下单字标签匹配的单字关键词: ['dogs', 'seat', 'pet', 'booster', 'car', 'dog', 'travel', 'seats']\n",
      "分析关键词结构...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "解析关键词: 100%|███████████████████████████████████████████████████████████████████| 335/335 [00:01<00:00, 265.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别出 41 个核心产品术语\n",
      "识别出 12 种前缀模式\n",
      "\n",
      "产品标签中的单字词 (可能被过滤): ['dogs', 'seat', 'pet', 'booster', 'car', 'dog', 'travel', 'seats']\n",
      "\n",
      "常见前缀示例:\n",
      "  '25' (出现5次)\n",
      "  'non-slip' (出现2次)\n",
      "  '14lbs' (出现1次)\n",
      "  '20lbs' (出现1次)\n",
      "  '25lbs-elevated' (出现1次)\n",
      "  '35lbs' (出现1次)\n",
      "  '60lbs' (出现1次)\n",
      "  'anti-collapse' (出现1次)\n",
      "  'clip-on' (出现1次)\n",
      "  'medium-sized' (出现1次)\n",
      "  'pets-pattern' (出现1次)\n",
      "  'suvs-gray' (出现1次)\n",
      "\n",
      "核心产品术语示例 (来自关键词分析):\n",
      "  seat, car, dog, dogs, supplies, booster, travel, bed, seats, pet, puppy, console, safety, pets, carrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "前缀移除与单字标签过滤: 100%|████████████████████████████████████████████████████| 335/335 [00:00<00:00, 167572.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "标准化和去重 (TF-IDF)...\n",
      "前缀移除和过滤后剩余 327 个独立关键词。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "聚类相似关键词: 100%|████████████████████████████████████████████████████████████| 327/327 [00:00<00:00, 189465.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "移除/过滤统计 (Top 15):\n",
      "  移除/过滤 '固定前缀: 25' 5次\n",
      "  移除/过滤 '固定前缀: non-slip' 2次\n",
      "  移除/过滤 '固定前缀: pets-pattern' 1次\n",
      "  移除/过滤 '固定前缀: clip-on' 1次\n",
      "  移除/过滤 '固定前缀: anti-collapse' 1次\n",
      "  移除/过滤 '固定前缀: 25lbs-elevated' 1次\n",
      "  移除/过滤 '固定前缀: 60lbs' 1次\n",
      "  移除/过滤 '固定前缀: 35lbs' 1次\n",
      "  移除/过滤 '固定前缀: suvs-gray' 1次\n",
      "  移除/过滤 '固定前缀: 14lbs' 1次\n",
      "  移除/过滤 '固定前缀: 20lbs' 1次\n",
      "  移除/过滤 '固定前缀: medium-sized' 1次\n",
      "  移除/过滤 '清理后单字标签过滤: dogs' 1次\n",
      "TF-IDF 相似度去重后剩余 302 个关键词。\n",
      "\n",
      "--- 步骤 2: 使用 AI 精炼关键词 ---\n",
      "尝试使用QWEN大模型精炼关键词 (尝试 1/2)...\n",
      "正在使用 OpenAI 兼容模式 (方法1)...\n",
      "方法1成功获取精炼关键词，共 248 个\n",
      "关键词AI精炼完成: 输入302个，精炼后248个\n",
      "\n",
      "--- 步骤 3: 保存结果 ---\n",
      "成功保存 248 个关键词到 生成结果\\social_media\\keywords.txt\n",
      "\n",
      "--- 关键词处理完成! ---\n",
      "\n",
      "--- 程序执行结束 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import List, Optional, Set\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "# 忽略特定警告 (例如 InsecureRequestWarning)\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "\n",
    "# --- REMOVED: Static GENERIC_CATEGORY_BLOCKLIST ---\n",
    "# We will now derive potentially problematic single words from product_labels\n",
    "\n",
    "# 从Excel文件名中获取基础文件夹名称\n",
    "# excel_file = Asin_List_file\n",
    "# base_folder_name = os.path.splitext(excel_file)[0]\n",
    "base_folder_name = os.path.join(\"生成结果\", \"social_media\")\n",
    "os.makedirs(base_folder_name, exist_ok=True)\n",
    "\n",
    "# 配置文件路径\n",
    "input_file = os.path.join(base_folder_name, \"keywords_temp1.txt\")\n",
    "output_file = os.path.join(base_folder_name, \"keywords.txt\")\n",
    "\n",
    "# 加载spaCy模型\n",
    "print(\"加载NLP模型...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"未找到 en_core_web_lg 模型，尝试下载...\")\n",
    "    try:\n",
    "        spacy.cli.download(\"en_core_web_lg\")\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    except Exception as e:\n",
    "        print(f\"下载或加载 spaCy 模型失败: {e}\")\n",
    "        exit()\n",
    "\n",
    "# --- 新增：辅助函数，从产品标签提取单字标签 ---\n",
    "def get_single_word_labels(labels: Optional[List[str]]) -> Set[str]:\n",
    "    \"\"\"从产品标签列表中提取所有单字条目（小写）\"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    single_words = set()\n",
    "    for label in labels:\n",
    "        processed_label = label.strip().lower()\n",
    "        # 检查处理后的标签是否只包含一个单词（没有空格）\n",
    "        if processed_label and ' ' not in processed_label:\n",
    "            single_words.add(processed_label)\n",
    "    return single_words\n",
    "# --- 结束新增 ---\n",
    "\n",
    "# --- 修改：更新 AI Prompt (简化关于通用词的说明) ---\n",
    "def create_keyword_refinement_prompt(keywords, product_labels=None):\n",
    "    \"\"\"为QWEN模型创建关键词精炼提示词 (已更新，简化通用词说明)\"\"\"\n",
    "\n",
    "    product_context = \"\"\n",
    "    label_words_str = \"\"\n",
    "\n",
    "    if product_labels and len(product_labels) > 0:\n",
    "        product_context = f\"The product likely relates to these concepts: {', '.join(product_labels)}. \"\n",
    "        # 提取一些标签词用于上下文提示\n",
    "        temp_label_words = set()\n",
    "        for label in product_labels:\n",
    "             words = [word.lower() for word in re.findall(r'\\b[a-z]{3,}\\b', label.lower())]\n",
    "             temp_label_words.update(words)\n",
    "        label_words_str = ', '.join(list(temp_label_words)[:10])\n",
    "        product_context += f\"Keywords should be specific to this context (related to: {label_words_str}).\"\n",
    "\n",
    "    # 提示词稍微简化，因为部分单字过滤已在前面处理\n",
    "    prompt = f\"\"\"You are an e-commerce keyword expert. Refine the following keyword list to keep only terms describing specific product functions, usage scenarios, user experience, materials, features, target audience, or styles.\n",
    "\n",
    "{product_context}\n",
    "\n",
    "Perform these operations:\n",
    "\n",
    "1.  Remove keywords that are too generic or vague *for the given product context*. For example, if the context is 'travel pillow', remove standalone 'travel' or 'accessories' if present, but keep 'travel pillow'.\n",
    "2.  Remove pure numbers, standalone size specs (e.g., \"XL\"), unless they are a key model/feature.\n",
    "3.  Remove standalone packaging units (e.g., \"bag\", \"box\", \"set\", \"pcs\", \"pack\") ONLY if they are not the product's core identity (e.g., for furniture, remove 'box'; for a 'storage box', keep 'box'). Use context: {label_words_str}.\n",
    "4.  Remove dimensional/capacity specs (e.g., \"120l\", \"5kg\") unless they are a defining feature.\n",
    "5.  Keep phrases about usage/experience (e.g., \"easy to assemble\", \"comfortable grip\").\n",
    "6.  Keep terms for functions/scenarios (e.g., \"outdoor camping\", \"office work\").\n",
    "7.  Keep terms for features/materials (e.g., \"wireless charging\", \"stainless steel\").\n",
    "8.  Remove vague adjectives if standalone (e.g., remove \"good\", keep \"good quality\").\n",
    "9.  Make minor improvements for clarity/search habits.\n",
    "\n",
    "IMPORTANT: Focus on specificity relevant to the product context ({label_words_str}).\n",
    "\n",
    "Keyword list:\n",
    "{', '.join(keywords)}\n",
    "\n",
    "Return only the processed keyword list, one keyword per line, without explanations or numbering. If the list is short (under 200), be slightly more lenient.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "# --- 结束修改 ---\n",
    "\n",
    "\n",
    "def refine_keywords_with_ai(\n",
    "    keywords: List[str],\n",
    "    api_key: Optional[str] = None,\n",
    "    product_labels: Optional[List[str]] = None,\n",
    "    base_folder_name: str = None\n",
    ") -> List[str]:\n",
    "    \"\"\"使用QWEN大模型精炼关键词 (API调用逻辑保持不变)\"\"\"\n",
    "    if not keywords:\n",
    "        print(\"输入AI精炼的关键词列表为空，跳过AI步骤。\")\n",
    "        return []\n",
    "\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get(\"QWEN_API_KEY\")\n",
    "        if not api_key:\n",
    "            print(\"警告: 未提供QWEN API密钥，跳过AI精炼步骤\")\n",
    "            return keywords # 返回未经AI处理的列表\n",
    "\n",
    "    prompt = create_keyword_refinement_prompt(keywords, product_labels)\n",
    "\n",
    "    if base_folder_name:\n",
    "        try:\n",
    "            with open(os.path.join(base_folder_name, \"ai_prompt.txt\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"无法写入 ai_prompt.txt: {e}\")\n",
    "\n",
    "    refined_keywords = []\n",
    "    max_retries = 2\n",
    "    current_retry = 0\n",
    "\n",
    "    # --- API 调用逻辑 (保持和上一个版本一致，包含重试和错误处理) ---\n",
    "    while current_retry < max_retries and not refined_keywords:\n",
    "        current_retry += 1\n",
    "        print(f\"尝试使用QWEN大模型精炼关键词 (尝试 {current_retry}/{max_retries})...\")\n",
    "        # 尝试方法1: OpenAI 兼容模式\n",
    "        try:\n",
    "            print(\"正在使用 OpenAI 兼容模式 (方法1)...\")\n",
    "            from openai import OpenAI, APITimeoutError, APIConnectionError, RateLimitError\n",
    "            client = OpenAI(api_key=api_key, base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", timeout=60.0)\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"qwen-max\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an e-commerce and SEO keyword expert.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1, max_tokens=4090, top_p=0.9,\n",
    "            )\n",
    "            result = completion.choices[0].message.content.strip()\n",
    "            if result and isinstance(result, str):\n",
    "                refined_keywords = [kw.strip().lower() for kw in result.splitlines() if kw.strip()]\n",
    "                if refined_keywords:\n",
    "                    print(f\"方法1成功获取精炼关键词，共 {len(refined_keywords)} 个\")\n",
    "                    break\n",
    "                else: print(\"方法1返回了空或无效的结果。\")\n",
    "            else: print(\"方法1未能获取有效响应内容。\")\n",
    "        except (APITimeoutError, APIConnectionError) as e: print(f\"方法1连接或超时错误: {e}\")\n",
    "        except RateLimitError as e: print(f\"方法1触发速率限制: {e}\")\n",
    "        except Exception as e: print(f\"方法1精炼关键词时发生未知错误: {e}\")\n",
    "\n",
    "        # 如果方法1失败，尝试方法2: 原始 API 调用\n",
    "        if not refined_keywords and current_retry <= max_retries:\n",
    "            try:\n",
    "                print(\"方法1失败，尝试使用原始API调用 (方法2)...\")\n",
    "                headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "                payload = {\n",
    "                    \"model\": \"qwen-max\",\n",
    "                    \"input\": {\"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": \"You are an e-commerce and SEO keyword expert.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]},\n",
    "                    \"parameters\": {\"temperature\": 0.1, \"max_tokens\": 4090, \"top_p\": 0.9, \"result_format\": \"message\"}\n",
    "                }\n",
    "                response = requests.post(\n",
    "                    \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\",\n",
    "                    headers=headers, json=payload, timeout=60.0, verify=False\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    result_data = response.json()\n",
    "                    if \"output\" in result_data and \"choices\" in result_data[\"output\"] and len(result_data[\"output\"][\"choices\"]) > 0:\n",
    "                        content = result_data[\"output\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                        if content and isinstance(content, str):\n",
    "                            refined_keywords = [kw.strip().lower() for kw in content.splitlines() if kw.strip()]\n",
    "                            if refined_keywords:\n",
    "                                print(f\"方法2成功获取精炼关键词，共 {len(refined_keywords)} 个\")\n",
    "                                break\n",
    "                            else: print(f\"方法2返回了空或无效的结果内容。响应: {response.text[:200]}...\")\n",
    "                        else: print(f\"方法2未能获取有效响应内容。响应: {response.text[:200]}...\")\n",
    "                    else: print(f\"方法2 API响应结构不符合预期。响应: {response.text[:200]}...\")\n",
    "                else: print(f\"方法2 API调用失败，状态码: {response.status_code}, 响应: {response.text[:200]}...\")\n",
    "            except requests.exceptions.Timeout: print(\"方法2请求超时。\")\n",
    "            except requests.exceptions.RequestException as e: print(f\"方法2请求发生错误: {e}\")\n",
    "            except Exception as e: print(f\"方法2精炼关键词时发生未知错误: {e}\")\n",
    "\n",
    "    if not refined_keywords:\n",
    "        print(\"警告：AI大模型调用失败，将使用AI处理之前的关键词列表\")\n",
    "        if base_folder_name:\n",
    "            try:\n",
    "                with open(os.path.join(base_folder_name, \"ai_error_log.txt\"), 'a', encoding='utf-8') as f:\n",
    "                    f.write(f\"[{pd.Timestamp.now()}] AI refinement failed for prompt starting with: {prompt[:100]}...\\n\")\n",
    "            except Exception as e: print(f\"无法写入 ai_error_log.txt: {e}\")\n",
    "        return keywords # 返回未经AI处理的列表\n",
    "\n",
    "    if base_folder_name:\n",
    "        try:\n",
    "            with open(os.path.join(base_folder_name, \"ai_refined_keywords.txt\"), 'w', encoding='utf-8') as f:\n",
    "                for kw in refined_keywords: f.write(f\"{kw}\\n\")\n",
    "        except Exception as e: print(f\"无法写入 ai_refined_keywords.txt: {e}\")\n",
    "\n",
    "    print(f\"关键词AI精炼完成: 输入{len(keywords)}个，精炼后{len(refined_keywords)}个\")\n",
    "    return refined_keywords\n",
    "# --- 结束 AI 函数 ---\n",
    "\n",
    "\n",
    "# --- 修改：load_keywords 函数增加基础清洗 (保持不变) ---\n",
    "def load_keywords(filepath):\n",
    "    \"\"\"从文件加载关键词，并进行基础清洗（小写，去特殊字符）\"\"\"\n",
    "    keywords = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                keyword = line.strip().lower()\n",
    "                keyword = keyword.replace('|', '') # 移除 '|'\n",
    "                if keyword:\n",
    "                    keywords.append(keyword)\n",
    "        print(f\"成功加载并初步清洗 {len(keywords)} 个关键词\")\n",
    "    except FileNotFoundError:\n",
    "         print(f\"错误: 输入文件未找到 {filepath}\")\n",
    "         return []\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件 {filepath} 时出错: {e}\")\n",
    "        return []\n",
    "    return keywords\n",
    "# --- 结束修改 ---\n",
    "\n",
    "# --- REMOVED: filter_generic_keywords 函数 ---\n",
    "# 过滤逻辑将整合到 extract_core_keywords\n",
    "\n",
    "\n",
    "def analyze_keywords_structure(keywords):\n",
    "    \"\"\"分析关键词结构 (基本保持不变, 移除 blocklist 依赖)\"\"\"\n",
    "    print(\"分析关键词结构...\")\n",
    "    if not keywords: return [], []\n",
    "\n",
    "    parsed_keywords = []\n",
    "    if 'nlp' not in globals() or nlp is None:\n",
    "        print(\"错误: NLP模型未加载，无法分析关键词结构。\")\n",
    "        return [], []\n",
    "\n",
    "    for keyword in tqdm(keywords, desc=\"解析关键词\"):\n",
    "        if not keyword or not isinstance(keyword, str): continue\n",
    "        try:\n",
    "            doc = nlp(keyword.lower())\n",
    "            parsed_keywords.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"解析关键词 '{keyword}' 时出错: {e}\")\n",
    "            continue\n",
    "\n",
    "    product_terms = Counter()\n",
    "    for doc in parsed_keywords:\n",
    "        for token in doc:\n",
    "            if token.text and not token.text.isspace() and token.pos_ in [\"NOUN\", \"PROPN\"] and len(token.text) > 2:\n",
    "                product_terms[token.text.lower()] += 1\n",
    "\n",
    "    # 核心术语识别稍微调整，不再依赖外部 blocklist\n",
    "    # 可以考虑在这里过滤掉一些通用的单位词，如果需要的话\n",
    "    common_units_or_fillers = {'pcs', 'piece', 'pieces', 'pack', 'packs', 'set', 'sets', 'inch', 'cm', 'mm', 'kg', 'lb', 'oz', 'for', 'with', 'and'}\n",
    "    core_product_terms = [term for term, count in product_terms.most_common()\n",
    "                          if count >= 3 and term not in common_units_or_fillers and not term.isdigit()]\n",
    "\n",
    "    print(f\"识别出 {len(core_product_terms)} 个核心产品术语\")\n",
    "\n",
    "    # 前缀模式识别逻辑不变\n",
    "    prefix_patterns = []\n",
    "    for keyword in keywords:\n",
    "        if not keyword or not isinstance(keyword, str): continue\n",
    "        words = keyword.lower().split()\n",
    "        if len(words) >= 2:\n",
    "            potential_prefix = words[0]\n",
    "            if re.match(r'^\\d+', potential_prefix) or '-' in potential_prefix:\n",
    "                prefix_patterns.append(potential_prefix)\n",
    "            if len(words) >= 3 and re.match(r'^\\d+', potential_prefix) and words[1] in ['pcs', 'piece', 'pieces', 'pack', 'in', 'inch', 'inche']:\n",
    "                prefix_patterns.append(f\"{words[0]} {words[1]}\")\n",
    "\n",
    "    common_prefixes = Counter(prefix_patterns).most_common()\n",
    "    print(f\"识别出 {len(common_prefixes)} 种前缀模式\")\n",
    "\n",
    "    return core_product_terms, common_prefixes\n",
    "\n",
    "\n",
    "# --- 修改：extract_core_keywords 整合单字标签过滤 ---\n",
    "def extract_core_keywords(keywords, product_labels=None):\n",
    "    \"\"\"\n",
    "    提取核心关键词，移除多样化前缀，并过滤掉与产品标签中单字条目匹配的单字关键词。\n",
    "    \"\"\"\n",
    "    if not keywords: return []\n",
    "\n",
    "    # --- 新增：获取单字标签用于过滤 ---\n",
    "    single_word_labels_to_filter = get_single_word_labels(product_labels)\n",
    "    if single_word_labels_to_filter:\n",
    "        print(f\"\\n将尝试过滤与以下单字标签匹配的单字关键词: {list(single_word_labels_to_filter)}\")\n",
    "    # --- 结束新增 ---\n",
    "\n",
    "    # 1. 分析关键词结构\n",
    "    core_terms, common_prefixes = analyze_keywords_structure(keywords)\n",
    "\n",
    "    # 打印上下文信息 (保持不变)\n",
    "    # category_core_terms = process_product_labels(product_labels) if product_labels else set() # 这个函数现在没了，但可以用 single_word_labels 代替部分信息\n",
    "    print(f\"\\n产品标签中的单字词 (可能被过滤): {list(single_word_labels_to_filter)[:10]}\")\n",
    "    print(\"\\n常见前缀示例:\")\n",
    "    for prefix, count in common_prefixes[:15]: print(f\"  '{prefix}' (出现{count}次)\")\n",
    "    print(\"\\n核心产品术语示例 (来自关键词分析):\")\n",
    "    print(f\"  {', '.join(core_terms[:15])}\")\n",
    "\n",
    "    # 按长度排序关键词\n",
    "    # 先进行一次简单去重和基础过滤\n",
    "    initial_filtered = set()\n",
    "    for kw in keywords:\n",
    "        k = kw.strip()\n",
    "        if k and len(k) > 1: # 至少保留长度为2的词\n",
    "             initial_filtered.add(k)\n",
    "    sorted_keywords = sorted(list(initial_filtered), key=len, reverse=True)\n",
    "\n",
    "    processed_keywords = []\n",
    "    removal_stats = Counter() # 重命名统计计数器\n",
    "    prefix_patterns = [prefix for prefix, _ in common_prefixes]\n",
    "    unit_terms = {'pcs', 'piece', 'pieces', 'pack', 'in', 'inch', 'inche', 'in-1', 'set'} # 保留单位词用于前缀识别\n",
    "\n",
    "    for keyword in tqdm(sorted_keywords, desc=\"前缀移除与单字标签过滤\"):\n",
    "        original = keyword\n",
    "        words = keyword.lower().split() # 确保小写\n",
    "\n",
    "        # --- 修改：整合单字标签过滤逻辑 ---\n",
    "        is_single_word_keyword = len(words) == 1\n",
    "        if is_single_word_keyword:\n",
    "            # 如果当前关键词是单个词，并且这个词存在于从 product_labels 提取的单字标签集合中\n",
    "            if keyword in single_word_labels_to_filter:\n",
    "                removal_stats[f\"单字标签过滤: {keyword}\"] += 1\n",
    "                continue # 过滤掉这个词，进行下一轮循环\n",
    "            # 如果是单字，但不在过滤列表里，检查是否是纯数字或过短\n",
    "            elif keyword.isdigit():\n",
    "                 removal_stats[\"纯数字关键词\"] += 1\n",
    "                 continue\n",
    "            elif len(keyword) < 3: # 可选：过滤掉非常短的单字 (如 's', 'm')\n",
    "                 removal_stats[\"过短单字(<3)\"] += 1\n",
    "                 continue\n",
    "            else:\n",
    "                 # 保留不在过滤列表中的有效单字关键词\n",
    "                 processed_keywords.append(keyword)\n",
    "                 continue # 处理完单字情况\n",
    "        # --- 结束修改 ---\n",
    "\n",
    "        # --- 前缀移除逻辑 (基本保持不变, 增加有效性检查) ---\n",
    "        cleaned = False\n",
    "        # 策略1: 固定前缀模式移除\n",
    "        for prefix in prefix_patterns:\n",
    "            prefix_terms = prefix.split()\n",
    "            if len(words) > len(prefix_terms) and ' '.join(words[:len(prefix_terms)]) == prefix:\n",
    "                remaining_keyword = ' '.join(words[len(prefix_terms):]).strip()\n",
    "                if remaining_keyword and len(remaining_keyword) > 1 and not remaining_keyword.isdigit(): # 允许长度为2\n",
    "                    keyword = remaining_keyword\n",
    "                    removal_stats[f\"固定前缀: {prefix}\"] += 1\n",
    "                    cleaned = True\n",
    "                    break\n",
    "                else:\n",
    "                    cleaned = False; break # 移除后无效\n",
    "\n",
    "        if cleaned:\n",
    "            # 如果清理后变成单字，再次检查是否需要过滤\n",
    "            if ' ' not in keyword and keyword in single_word_labels_to_filter:\n",
    "                 removal_stats[f\"清理后单字标签过滤: {keyword}\"] += 1\n",
    "                 continue\n",
    "            elif len(keyword) > 1: # 确保清理后仍然有效\n",
    "                 processed_keywords.append(keyword)\n",
    "            continue\n",
    "\n",
    "        # 策略2: 数字+单位组合识别\n",
    "        if len(words) >= 3 and re.match(r'^\\d+', words[0]) and words[1] in unit_terms:\n",
    "            remaining_keyword = ' '.join(words[2:]).strip()\n",
    "            if remaining_keyword and len(remaining_keyword) > 1 and not remaining_keyword.isdigit():\n",
    "                keyword = remaining_keyword\n",
    "                # 再次检查清理后是否为需过滤的单字\n",
    "                if ' ' not in keyword and keyword in single_word_labels_to_filter:\n",
    "                     removal_stats[f\"清理后单字标签过滤: {keyword}\"] += 1\n",
    "                     continue\n",
    "                removal_stats[f\"数字单位前缀: {words[0]} {words[1]}\"] += 1\n",
    "                processed_keywords.append(keyword)\n",
    "                continue\n",
    "\n",
    "        # 策略3: 仅数字前缀识别\n",
    "        if re.match(r'^\\d+(-\\d+)*$', words[0]) and len(words) > 1:\n",
    "             remaining_keyword = ' '.join(words[1:]).strip()\n",
    "             if remaining_keyword and len(remaining_keyword) > 1 and not remaining_keyword.isdigit():\n",
    "                keyword = remaining_keyword\n",
    "                if ' ' not in keyword and keyword in single_word_labels_to_filter:\n",
    "                     removal_stats[f\"清理后单字标签过滤: {keyword}\"] += 1\n",
    "                     continue\n",
    "                removal_stats[f\"数字前缀: {words[0]}\"] += 1\n",
    "                processed_keywords.append(keyword)\n",
    "                continue\n",
    "\n",
    "        # 策略4: 复杂模式识别 (逻辑不变，增加后续检查)\n",
    "        if len(words) >= 2:\n",
    "            first_word = words[0]\n",
    "            if (re.match(r'^\\d+[a-z]+\\d*$', first_word) or\n",
    "                re.match(r'^\\d+-[a-z]+(-\\d+)?$', first_word)):\n",
    "                prefix_to_remove = first_word\n",
    "                idx_to_join = 1\n",
    "                if len(words) > 2 and words[1] in unit_terms.union(['casual', 'rolling', 'cartoon', 'tier']):\n",
    "                    prefix_to_remove = f\"{words[0]} {words[1]}\"\n",
    "                    idx_to_join = 2\n",
    "                remaining_keyword = ' '.join(words[idx_to_join:]).strip()\n",
    "                if remaining_keyword and len(remaining_keyword) > 1 and not remaining_keyword.isdigit():\n",
    "                    keyword = remaining_keyword\n",
    "                    if ' ' not in keyword and keyword in single_word_labels_to_filter:\n",
    "                         removal_stats[f\"清理后单字标签过滤: {keyword}\"] += 1\n",
    "                         continue\n",
    "                    removal_stats[f\"复杂模式前缀: {prefix_to_remove}\"] += 1\n",
    "                    processed_keywords.append(keyword)\n",
    "                    continue\n",
    "\n",
    "        # 如果没有应用任何清理规则，保留原始关键词 (但要确保它没在单字过滤列表里)\n",
    "        # 这一步理论上应该不会执行到单字情况，因为前面处理过了\n",
    "        # 但作为保险，检查一下\n",
    "        if keyword not in processed_keywords: # 避免重复添加\n",
    "             if ' ' not in keyword and keyword in single_word_labels_to_filter:\n",
    "                 # 不应该到这里，但以防万一\n",
    "                 removal_stats[f\"末尾单字标签过滤: {keyword}\"] += 1\n",
    "             elif len(keyword) > 1: # 确保有效\n",
    "                 processed_keywords.append(keyword)\n",
    "\n",
    "\n",
    "    # --- 标准化和去重 (TF-IDF 相似度部分保持不变) ---\n",
    "    print(\"\\n标准化和去重 (TF-IDF)...\")\n",
    "    if not processed_keywords:\n",
    "        print(\"没有可处理的关键词进行 TF-IDF 分析。\")\n",
    "        return []\n",
    "\n",
    "    unique_keywords = sorted(list(set(processed_keywords)), key=len, reverse=True)\n",
    "    print(f\"前缀移除和过滤后剩余 {len(unique_keywords)} 个独立关键词。\")\n",
    "    if len(unique_keywords) < 2: return unique_keywords\n",
    "\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(min_df=1, analyzer='char', ngram_range=(2, 5))\n",
    "        tfidf_matrix = vectorizer.fit_transform(unique_keywords)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    except Exception as e:\n",
    "        print(f\"TF-IDF 处理时出错: {e}. 返回简单去重结果。\")\n",
    "        return unique_keywords\n",
    "\n",
    "    clusters = {}\n",
    "    processed_indices = set()\n",
    "    similarity_threshold = 0.85\n",
    "\n",
    "    for i in tqdm(range(len(unique_keywords)), desc=\"聚类相似关键词\"):\n",
    "        if i in processed_indices: continue\n",
    "        similar_indices = np.where(similarity_matrix[i] >= similarity_threshold)[0]\n",
    "        if len(similar_indices) > 1:\n",
    "            cluster_keywords = [unique_keywords[j] for j in similar_indices if j < len(unique_keywords)]\n",
    "            representative = max(cluster_keywords, key=len)\n",
    "            clusters[representative] = cluster_keywords\n",
    "            processed_indices.update(similar_indices)\n",
    "        else:\n",
    "            representative = unique_keywords[i]\n",
    "            clusters[representative] = [representative]\n",
    "            processed_indices.add(i)\n",
    "\n",
    "    final_keywords = sorted(list(clusters.keys()))\n",
    "\n",
    "    print(\"\\n移除/过滤统计 (Top 15):\")\n",
    "    for item, count in removal_stats.most_common(15):\n",
    "        print(f\"  移除/过滤 '{item}' {count}次\")\n",
    "\n",
    "    print(f\"TF-IDF 相似度去重后剩余 {len(final_keywords)} 个关键词。\")\n",
    "    return final_keywords\n",
    "# --- 结束 extract_core_keywords 修改 ---\n",
    "\n",
    "\n",
    "def save_keywords(keywords, filepath):\n",
    "    \"\"\"保存最终关键词到文件 (保持不变)\"\"\"\n",
    "    try:\n",
    "        dir_path = os.path.dirname(filepath)\n",
    "        if dir_path: os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for keyword in keywords: f.write(f\"{keyword}\\n\")\n",
    "        print(f\"成功保存 {len(keywords)} 个关键词到 {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件 {filepath} 时出错: {e}\")\n",
    "\n",
    "# --- 主执行流程 ---\n",
    "\n",
    "# !! 重要：假设 product_labels 在这里被定义 !!\n",
    "# 示例，使用你提供的例子:\n",
    "#product_labels = ['travel', 'bag', 'luggage', 'shoes', 'gear', 'luggage travel', 'travel gear', 'duffels', 'clothing', 'jewelry']\n",
    "# product_labels = [\"Simple White T-Shirt\", \"Cotton Crew Neck Clothing\"] # 另一个例子\n",
    "# product_labels = None # 测试没有标签的情况\n",
    "\n",
    "print(f\"使用的产品标签 (用于上下文和单字过滤): {product_labels}\")\n",
    "\n",
    "os.environ[\"QWEN_API_KEY\"] = \"sk-2ea9416b45e04af6b6aa72d3c2ade52f\"\n",
    "# API Key (从环境变量读取)\n",
    "QWEN_API_KEY = os.environ.get(\"QWEN_API_KEY\")\n",
    "if not QWEN_API_KEY:\n",
    "    print(\"警告: 环境变量 QWEN_API_KEY 未设置。\")\n",
    "\n",
    "print(f\"\\n--- 开始处理 ---\")\n",
    "print(f\"输入文件: {input_file}\")\n",
    "print(f\"输出文件: {output_file}\")\n",
    "print(f\"结果文件夹: {base_folder_name}\")\n",
    "\n",
    "# 1. 加载并初步清洗关键词\n",
    "initial_keywords = load_keywords(input_file)\n",
    "\n",
    "if initial_keywords:\n",
    "    # --- 步骤合并：提取核心关键词，同时进行前缀移除和基于标签的单字过滤 ---\n",
    "    print(f\"\\n--- 步骤 1: 提取核心关键词 (含前缀移除、单字标签过滤、TF-IDF去重) ---\")\n",
    "    # 现在 extract_core_keywords 内部处理了单字标签的过滤\n",
    "    core_keywords = extract_core_keywords(initial_keywords, product_labels)\n",
    "\n",
    "    print(f\"\\n--- 步骤 2: 使用 AI 精炼关键词 ---\")\n",
    "    # 将上一步处理后的结果送入 AI\n",
    "    refined_keywords = refine_keywords_with_ai(\n",
    "        core_keywords,\n",
    "        api_key=QWEN_API_KEY,\n",
    "        product_labels=product_labels, # 仍然传递标签给 AI 用于整体上下文\n",
    "        base_folder_name=base_folder_name\n",
    "    )\n",
    "\n",
    "    # 确保最终结果是排序且唯一的列表\n",
    "    final_keywords_to_save = sorted(list(set(refined_keywords)))\n",
    "\n",
    "    print(f\"\\n--- 步骤 3: 保存结果 ---\")\n",
    "    save_keywords(final_keywords_to_save, output_file)\n",
    "\n",
    "    print(\"\\n--- 关键词处理完成! ---\")\n",
    "else:\n",
    "    print(f\"未能从 {input_file} 加载到任何关键词，程序结束。\")\n",
    "\n",
    "# 清理临时文件的部分保持注释状态\n",
    "# ...\n",
    "\n",
    "print(\"\\n--- 程序执行结束 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lynx",
   "language": "python",
   "name": "lynx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
