{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0180d1fb-2388-4eac-a1d6-075f2371a350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:26.253244Z",
     "iopub.status.busy": "2025-04-15T10:59:26.253244Z",
     "iopub.status.idle": "2025-04-15T10:59:28.156960Z",
     "shell.execute_reply": "2025-04-15T10:59:28.156960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_file_1: 生成结果/defect_analysis\\致欧-2025-01-10之后VOC数据_缺陷象限分析_完整列表.txt\n",
      "txt_file_2: 生成结果/needs_analysis\\致欧-2025-01-10之后VOC数据_用户需求象限分析_完整列表.txt\n",
      "txt_file_3: 生成结果/matches_analysis\\致欧-2025-01-10之后VOC数据_功能场景匹配象限分析_完整列表.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-2ea9416b45e04af6b6aa72d3c2ade52f\"\n",
    "\n",
    "# 设置环境变量和配置\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 避免警告\n",
    "OUTPUT_DIR = \"生成结果/last_quadrant\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_files = {\n",
    "    \"excel_file_1\": \"生成结果/last_quadrant/四象限数据-痛点.xlsx\",\n",
    "    \"excel_file_2\": \"生成结果/last_quadrant/四象限数据-需求.xlsx\",\n",
    "    \"excel_file_3\": \"生成结果/last_quadrant/四象限数据-场景匹配.xlsx\",\n",
    "    \"txt_file_4\": \"生成结果/social_media/Comment-归一化的模型结果.txt\",\n",
    "    \"txt_file_5\": \"生成结果/social_media/Like-归一化的模型结果.txt\",\n",
    "    \"txt_file_6\": \"生成结果/social_media/Repost-归一化的模型结果.txt\",\n",
    "    \"txt_file_7\": \"生成结果/social_media/Share-归一化的模型结果.txt\",\n",
    "    \"txt_file_8\": \"生成结果/social_media/View-归一化的模型结果.txt\",\n",
    "    \"md_file_1\": \"生成结果/social_media/理论假设.md\"\n",
    "}\n",
    "\n",
    "# 自动搜索三个文件夹中的txt文件\n",
    "defect_dir = \"生成结果/defect_analysis/\"\n",
    "needs_dir = \"生成结果/needs_analysis/\"\n",
    "matches_dir = \"生成结果/matches_analysis/\"\n",
    "\n",
    "# 在每个文件夹中查找第一个txt文件\n",
    "def find_first_txt(directory):\n",
    "    txt_files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    return txt_files[0] if txt_files else None\n",
    "\n",
    "# 更新data_files字典\n",
    "defect_txt = find_first_txt(defect_dir)\n",
    "if defect_txt:\n",
    "    data_files[\"txt_file_1\"] = defect_txt\n",
    "else:\n",
    "    print(f\"警告: 在 {defect_dir} 中未找到txt文件\")\n",
    "\n",
    "needs_txt = find_first_txt(needs_dir)\n",
    "if needs_txt:\n",
    "    data_files[\"txt_file_2\"] = needs_txt\n",
    "else:\n",
    "    print(f\"警告: 在 {needs_dir} 中未找到txt文件\")\n",
    "\n",
    "matches_txt = find_first_txt(matches_dir)\n",
    "if matches_txt:\n",
    "    data_files[\"txt_file_3\"] = matches_txt\n",
    "else:\n",
    "    print(f\"警告: 在 {matches_dir} 中未找到txt文件\")\n",
    "\n",
    "# 打印更新后的文件路径\n",
    "for key in [\"txt_file_1\", \"txt_file_2\", \"txt_file_3\"]:\n",
    "    if key in data_files:\n",
    "        print(f\"{key}: {data_files[key]}\")\n",
    "\n",
    "# Prompt文件路径列表\n",
    "prompt_files = {\n",
    "    \"prompt_1\": \"Data/建议表1-表达-流量关系表Prompt.txt\",\n",
    "    \"prompt_2\": \"Data/建议表2-消费者旅程不同阶段的产品表达关注表Prompt.txt\",\n",
    "    \"prompt_3\": \"Data/建议表3-品类功能利益点情感利益点分析表Prompt.txt\",\n",
    "    \"prompt_4\": \"Data/建议表4-产品改进建议Prompt.txt\",\n",
    "    \"prompt_5\": \"Data/建议表5-情感利益点改进建议Prompt.txt\",\n",
    "    \"prompt_6\": \"Data/建议表6-改进优先级排序Prompt.txt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0503a627-835b-4981-ba02-42950323b560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:28.157880Z",
     "iopub.status.busy": "2025-04-15T10:59:28.157880Z",
     "iopub.status.idle": "2025-04-15T10:59:37.281829Z",
     "shell.execute_reply": "2025-04-15T10:59:37.281829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载嵌入模型 BAAI/bge-large-zh-v1.5...\n",
      "将使用GPU: NVIDIA GeForce RTX 4090 D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功! 嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 为RTX 4090D选择高性能中文模型\n",
    "# 对于中文+数字的组合文本，我们选择性能更高的中文检索模型\n",
    "MODEL_NAME = \"BAAI/bge-large-zh-v1.5\"  # 高性能中文模型，约1.3GB\n",
    "\n",
    "def load_model(model_name=MODEL_NAME):\n",
    "    \"\"\"加载或下载嵌入模型\"\"\"\n",
    "    print(f\"正在加载嵌入模型 {model_name}...\")\n",
    "    try:\n",
    "        # 设置设备，使用GPU加速\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if device == \"cuda\":\n",
    "            print(f\"将使用GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"未检测到GPU，将使用CPU\")\n",
    "        \n",
    "        # 加载模型\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        # 验证模型\n",
    "        test_embedding = model.encode(\"测试句子\", convert_to_tensor=True)\n",
    "        print(f\"模型加载成功! 嵌入维度: {test_embedding.shape[0]}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"模型加载失败: {e}\")\n",
    "        print(\"请确保网络连接正常，或尝试使用其他模型\")\n",
    "        return None\n",
    "\n",
    "# 加载模型\n",
    "embedding_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f1ba92-d542-4173-a4b1-d18d1c69f141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.281829Z",
     "iopub.status.busy": "2025-04-15T10:59:37.281829Z",
     "iopub.status.idle": "2025-04-15T10:59:37.297307Z",
     "shell.execute_reply": "2025-04-15T10:59:37.297307Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用于估算token数量的简单函数\n",
    "def estimate_tokens(text):\n",
    "    # 一个简单的估算：按空格分词，每个词约1.3个token（中文需要调整）\n",
    "    chinese_char_count = sum(1 for c in text if '\\u4e00' <= c <= '\\u9fff')\n",
    "    english_word_count = len(text.split()) - chinese_char_count / 5  # 粗略调整\n",
    "    return int(chinese_char_count * 1.5 + english_word_count * 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42063bee-9a85-4f96-89bb-8c866bab272d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.297307Z",
     "iopub.status.busy": "2025-04-15T10:59:37.297307Z",
     "iopub.status.idle": "2025-04-15T10:59:37.327876Z",
     "shell.execute_reply": "2025-04-15T10:59:37.327876Z"
    }
   },
   "outputs": [],
   "source": [
    "# 文档处理类\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, data_files):\n",
    "        self.data_files = data_files\n",
    "        self.chunks = []\n",
    "        self.file_stats = {}\n",
    "        \n",
    "    def process_all_files(self):\n",
    "        \"\"\"处理所有文件，创建分块\"\"\"\n",
    "        print(\"开始处理数据文件...\")\n",
    "        \n",
    "        # 处理Excel文件\n",
    "        for key in [k for k in self.data_files.keys() if k.startswith(\"excel\")]:\n",
    "            self._process_excel_file(key, self.data_files[key])\n",
    "        \n",
    "        # 处理文本和MD文件\n",
    "        for key in [k for k in self.data_files.keys() if k.startswith(\"txt\") or k.startswith(\"md\")]:\n",
    "            self._process_text_file(key, self.data_files[key])\n",
    "            \n",
    "        print(f\"文件处理完成，共创建 {len(self.chunks)} 个文档块\")\n",
    "        return self.chunks, self.file_stats\n",
    "    \n",
    "    def _process_excel_file(self, file_key, file_path):\n",
    "        \"\"\"处理Excel文件，创建有意义的文档块\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # 存储文件统计信息\n",
    "            self.file_stats[file_key] = {\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns),\n",
    "                \"column_names\": df.columns.tolist()\n",
    "            }\n",
    "            \n",
    "            # 策略1：完整表格（如果不太大）\n",
    "            full_content = f\"文件: {file_key} (完整表格)\\n表格概览: {len(df)}行 x {len(df.columns)}列\\n\"\n",
    "            full_content += f\"列名: {', '.join(str(col) for col in df.columns)}\\n\\n\"\n",
    "            full_content += df.to_string(index=False)\n",
    "            \n",
    "            full_tokens = estimate_tokens(full_content)\n",
    "            \n",
    "            # 如果完整表格不太大，直接作为一个块\n",
    "            if full_tokens < 15000:\n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_full\",\n",
    "                    \"content\": full_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"excel_full\",\n",
    "                    \"tokens\": full_tokens\n",
    "                })\n",
    "                print(f\"添加完整表格: {file_key} ({full_tokens} tokens)\")\n",
    "                return\n",
    "            \n",
    "            # 策略2：表格元数据 + 列名说明\n",
    "            metadata = f\"文件: {file_key} (表格元数据)\\n\"\n",
    "            metadata += f\"表格概览: {len(df)}行 x {len(df.columns)}列\\n\"\n",
    "            metadata += f\"列名及说明:\\n\"\n",
    "            \n",
    "            # 尝试推断每列的数据类型和基本统计信息\n",
    "            for col in df.columns:\n",
    "                col_type = str(df[col].dtype)\n",
    "                metadata += f\"- {col} (类型: {col_type})\"\n",
    "                \n",
    "                # 对数值列添加统计信息\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    metadata += f\": 范围 {df[col].min()}-{df[col].max()}, 平均值 {df[col].mean():.2f}, 中位数 {df[col].median()}\"\n",
    "                # 对分类列添加唯一值信息\n",
    "                elif pd.api.types.is_categorical_dtype(df[col]) or df[col].nunique() < 10:\n",
    "                    unique_vals = \", \".join(str(v) for v in df[col].unique()[:5])\n",
    "                    metadata += f\": 值包括 {unique_vals}\"\n",
    "                    if df[col].nunique() > 5:\n",
    "                        metadata += f\" 等共{df[col].nunique()}个唯一值\"\n",
    "                # 对日期列添加时间范围\n",
    "                elif pd.api.types.is_datetime64_dtype(df[col]):\n",
    "                    metadata += f\": 时间范围 {df[col].min()} 到 {df[col].max()}\"\n",
    "                # 对文本列添加长度信息\n",
    "                elif pd.api.types.is_string_dtype(df[col]) or df[col].dtype == object:\n",
    "                    try:\n",
    "                        if df[col].notna().any():\n",
    "                            avg_len = df[col].astype(str).str.len().mean()\n",
    "                            metadata += f\": 平均长度 {avg_len:.1f}字符\"\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                metadata += \"\\n\"\n",
    "            \n",
    "            self.chunks.append({\n",
    "                \"id\": f\"{file_key}_metadata\",\n",
    "                \"content\": metadata,\n",
    "                \"source\": file_key,\n",
    "                \"chunk_type\": \"excel_metadata\",\n",
    "                \"tokens\": estimate_tokens(metadata)\n",
    "            })\n",
    "            \n",
    "            # 策略3：按列分组\n",
    "            # 简单方法：相邻的2-3列组合在一起\n",
    "            col_groups = []\n",
    "            current_group = []\n",
    "            for col in df.columns:\n",
    "                current_group.append(col)\n",
    "                if len(current_group) >= 3:  # 每组最多3列\n",
    "                    col_groups.append(current_group)\n",
    "                    current_group = []\n",
    "            \n",
    "            # 添加剩余的列\n",
    "            if current_group:\n",
    "                col_groups.append(current_group)\n",
    "            \n",
    "            # 为每个列组创建内容块\n",
    "            for i, group in enumerate(col_groups):\n",
    "                # 准备列组内容\n",
    "                group_content = f\"文件: {file_key} (列组 {i+1})\\n\"\n",
    "                group_content += f\"包含列: {', '.join(str(col) for col in group)}\\n\\n\"\n",
    "                \n",
    "                # 添加这些列的数据\n",
    "                group_df = df[group]\n",
    "                group_content += group_df.to_string(index=True)\n",
    "                \n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_colgroup_{i+1}\",\n",
    "                    \"content\": group_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"excel_column_group\",\n",
    "                    \"columns\": group,\n",
    "                    \"tokens\": estimate_tokens(group_content)\n",
    "                })\n",
    "            \n",
    "            # 策略4：按行分块，但保留列名\n",
    "            rows_per_chunk = 50  # 根据表格复杂度调整\n",
    "            \n",
    "            for i in range(0, len(df), rows_per_chunk):\n",
    "                end_idx = min(i + rows_per_chunk, len(df))\n",
    "                chunk_df = df.iloc[i:end_idx]\n",
    "                \n",
    "                rows_content = f\"文件: {file_key} (行 {i+1}-{end_idx}，共{len(df)}行)\\n\"\n",
    "                rows_content += f\"列名: {', '.join(str(col) for col in df.columns)}\\n\\n\"\n",
    "                rows_content += chunk_df.to_string(index=True)\n",
    "                \n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_rows_{i+1}_{end_idx}\",\n",
    "                    \"content\": rows_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"excel_rows\",\n",
    "                    \"row_range\": [i+1, end_idx],\n",
    "                    \"tokens\": estimate_tokens(rows_content)\n",
    "                })\n",
    "            \n",
    "            print(f\"处理完成: {file_key}, 创建了1个元数据块、{len(col_groups)}个列组块和{(len(df)-1)//rows_per_chunk+1}个行块\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理 {file_key} 时出错: {e}\")\n",
    "            # 添加错误信息\n",
    "            self.chunks.append({\n",
    "                \"id\": f\"{file_key}_error\",\n",
    "                \"content\": f\"文件: {file_key}\\n处理错误: {e}\",\n",
    "                \"source\": file_key,\n",
    "                \"chunk_type\": \"error\",\n",
    "                \"tokens\": 50\n",
    "            })\n",
    "    \n",
    "    def _process_text_file(self, file_key, file_path):\n",
    "        \"\"\"处理文本文件，创建有意义的文档块\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # 存储文件统计信息\n",
    "            line_count = content.count('\\n') + 1\n",
    "            self.file_stats[file_key] = {\n",
    "                \"characters\": len(content),\n",
    "                \"lines\": line_count\n",
    "            }\n",
    "            \n",
    "            # 文件较小时，保留为一个完整块\n",
    "            if len(content) < 12000:\n",
    "                full_content = f\"文件: {file_key} (完整内容)\\n\\n{content}\"\n",
    "                \n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_full\",\n",
    "                    \"content\": full_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"text_full\",\n",
    "                    \"tokens\": estimate_tokens(full_content)\n",
    "                })\n",
    "                \n",
    "                print(f\"添加完整文本: {file_key} ({len(content)} 字符)\")\n",
    "                return\n",
    "            \n",
    "            # 添加文件摘要/概览\n",
    "            summary = f\"文件: {file_key} (概览)\\n\"\n",
    "            # 避免在f-string中使用转义字符，先计算行数再使用变量\n",
    "            summary += f\"字符数: {len(content)}, 行数: {line_count}\\n\"\n",
    "            \n",
    "            # 提取文件前几行作为预览\n",
    "            first_lines = \"\\n\".join(content.split('\\n')[:10])\n",
    "            summary += f\"文件开头:\\n{first_lines}\\n...\"\n",
    "            \n",
    "            self.chunks.append({\n",
    "                \"id\": f\"{file_key}_summary\",\n",
    "                \"content\": summary,\n",
    "                \"source\": file_key,\n",
    "                \"chunk_type\": \"text_summary\",\n",
    "                \"tokens\": estimate_tokens(summary)\n",
    "            })\n",
    "            \n",
    "            # 检测文档结构\n",
    "            has_headers = bool(re.search(r'^#+\\s+', content, re.MULTILINE))\n",
    "            has_sections = bool(re.search(r'^[0-9]+\\.\\s+', content, re.MULTILINE))\n",
    "            \n",
    "            # 根据文档结构选择分块策略\n",
    "            if has_headers and file_key.startswith(\"md\"):\n",
    "                # 按Markdown标题分块\n",
    "                self._split_by_markdown_headers(file_key, content)\n",
    "            elif has_sections:\n",
    "                # 按编号章节分块\n",
    "                self._split_by_numbered_sections(file_key, content)\n",
    "            else:\n",
    "                # 按语义段落分块\n",
    "                self._split_by_semantic_paragraphs(file_key, content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理 {file_key} 时出错: {e}\")\n",
    "            self.chunks.append({\n",
    "                \"id\": f\"{file_key}_error\",\n",
    "                \"content\": f\"文件: {file_key}\\n处理错误: {e}\",\n",
    "                \"source\": file_key,\n",
    "                \"chunk_type\": \"error\",\n",
    "                \"tokens\": 50\n",
    "            })\n",
    "    \n",
    "    def _split_by_markdown_headers(self, file_key, content):\n",
    "        \"\"\"按Markdown标题分块，保留标题层次结构\"\"\"\n",
    "        # 识别所有标题行及其位置\n",
    "        header_pattern = re.compile(r'^(#+)\\s+(.*?)$', re.MULTILINE)\n",
    "        headers = [(m.group(1), m.group(2), m.start()) for m in header_pattern.finditer(content)]\n",
    "        \n",
    "        if not headers:\n",
    "            # 没有找到标题，回退到段落分块\n",
    "            self._split_by_semantic_paragraphs(file_key, content)\n",
    "            return\n",
    "        \n",
    "        # 添加文件结尾位置\n",
    "        headers.append((None, None, len(content)))\n",
    "        \n",
    "        # 按标题分块\n",
    "        for i in range(len(headers) - 1):\n",
    "            level, title, start = headers[i]\n",
    "            _, _, end = headers[i + 1]\n",
    "            \n",
    "            section_content = content[start:end]\n",
    "            \n",
    "            # 创建块内容，包含上下文信息\n",
    "            if i > 0:\n",
    "                # 添加父标题作为上下文\n",
    "                parent_idx = i - 1\n",
    "                while parent_idx >= 0:\n",
    "                    parent_level, parent_title, _ = headers[parent_idx]\n",
    "                    if parent_level and len(parent_level) < len(level):\n",
    "                        header_context = f\"文件: {file_key}\\n章节: {parent_title} > {title}\\n\\n\"\n",
    "                        break\n",
    "                    parent_idx -= 1\n",
    "                else:\n",
    "                    header_context = f\"文件: {file_key}\\n章节: {title}\\n\\n\"\n",
    "            else:\n",
    "                header_context = f\"文件: {file_key}\\n章节: {title}\\n\\n\"\n",
    "            \n",
    "            chunk_content = header_context + section_content\n",
    "            chunk_tokens = estimate_tokens(chunk_content)\n",
    "            \n",
    "            # 如果块太大，进一步分割\n",
    "            if chunk_tokens > 4000:\n",
    "                # 按段落分割大块\n",
    "                paragraphs = re.split(r'\\n\\s*\\n', section_content)\n",
    "                \n",
    "                # 创建包含标题的首块\n",
    "                first_para = header_context + paragraphs[0]\n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_h{len(level)}_{i}_0\",\n",
    "                    \"content\": first_para,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"markdown_section_start\",\n",
    "                    \"header\": title,\n",
    "                    \"level\": len(level),\n",
    "                    \"tokens\": estimate_tokens(first_para)\n",
    "                })\n",
    "                \n",
    "                # 分割剩余段落，保持上下文\n",
    "                current_chunk = \"\"\n",
    "                chunk_num = 1\n",
    "                \n",
    "                for j, para in enumerate(paragraphs[1:], 1):\n",
    "                    # 为每个新段添加上下文信息\n",
    "                    para_with_context = f\"文件: {file_key}\\n章节: {title} (续)\\n\\n{para}\"\n",
    "                    para_tokens = estimate_tokens(para_with_context)\n",
    "                    \n",
    "                    # 如果添加这个段落会使块太大，创建新块\n",
    "                    if estimate_tokens(current_chunk + para_with_context) > 3000:\n",
    "                        if current_chunk:\n",
    "                            self.chunks.append({\n",
    "                                \"id\": f\"{file_key}_h{len(level)}_{i}_{chunk_num}\",\n",
    "                                \"content\": current_chunk,\n",
    "                                \"source\": file_key,\n",
    "                                \"chunk_type\": \"markdown_section_continued\",\n",
    "                                \"header\": title,\n",
    "                                \"level\": len(level),\n",
    "                                \"tokens\": estimate_tokens(current_chunk)\n",
    "                            })\n",
    "                            chunk_num += 1\n",
    "                            current_chunk = para_with_context\n",
    "                    else:\n",
    "                        current_chunk += \"\\n\\n\" + para_with_context if current_chunk else para_with_context\n",
    "                \n",
    "                # 添加最后一个块\n",
    "                if current_chunk:\n",
    "                    self.chunks.append({\n",
    "                        \"id\": f\"{file_key}_h{len(level)}_{i}_{chunk_num}\",\n",
    "                        \"content\": current_chunk,\n",
    "                        \"source\": file_key,\n",
    "                        \"chunk_type\": \"markdown_section_end\",\n",
    "                        \"header\": title,\n",
    "                        \"level\": len(level),\n",
    "                        \"tokens\": estimate_tokens(current_chunk)\n",
    "                    })\n",
    "            else:\n",
    "                # 块大小合适，直接添加\n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_h{len(level)}_{i}\",\n",
    "                    \"content\": chunk_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"markdown_section\",\n",
    "                    \"header\": title,\n",
    "                    \"level\": len(level),\n",
    "                    \"tokens\": chunk_tokens\n",
    "                })\n",
    "        \n",
    "        print(f\"按Markdown标题分块: {file_key}, 创建了{len(self.chunks) - 1}个块\")\n",
    "    \n",
    "    def _split_by_numbered_sections(self, file_key, content):\n",
    "        \"\"\"按编号章节分块\"\"\"\n",
    "        # 识别所有编号章节标题\n",
    "        section_pattern = re.compile(r'^(\\d+\\.\\s*.*?)$', re.MULTILINE)\n",
    "        sections = [(m.group(1), m.start()) for m in section_pattern.finditer(content)]\n",
    "        \n",
    "        if not sections:\n",
    "            # 没有找到章节，回退到段落分块\n",
    "            self._split_by_semantic_paragraphs(file_key, content)\n",
    "            return\n",
    "        \n",
    "        # 添加文件结尾位置\n",
    "        sections.append((\"END\", len(content)))\n",
    "        \n",
    "        # 按章节分块\n",
    "        for i in range(len(sections) - 1):\n",
    "            title, start = sections[i]\n",
    "            _, end = sections[i + 1]\n",
    "            \n",
    "            section_content = content[start:end]\n",
    "            \n",
    "            # 创建块内容\n",
    "            chunk_content = f\"文件: {file_key}\\n章节: {title}\\n\\n{section_content}\"\n",
    "            chunk_tokens = estimate_tokens(chunk_content)\n",
    "            \n",
    "            # 如果块太大，按段落进一步拆分\n",
    "            if chunk_tokens > 4000:\n",
    "                paragraphs = re.split(r'\\n\\s*\\n', section_content)\n",
    "                \n",
    "                # 分批处理段落\n",
    "                current_chunk = f\"文件: {file_key}\\n章节: {title}\\n\\n{paragraphs[0]}\"\n",
    "                current_tokens = estimate_tokens(current_chunk)\n",
    "                chunk_num = 0\n",
    "                \n",
    "                for para in paragraphs[1:]:\n",
    "                    para_tokens = estimate_tokens(para)\n",
    "                    \n",
    "                    # 如果添加这个段落会使块太大，创建新块\n",
    "                    if current_tokens + para_tokens > 3500:\n",
    "                        self.chunks.append({\n",
    "                            \"id\": f\"{file_key}_section_{i}_{chunk_num}\",\n",
    "                            \"content\": current_chunk,\n",
    "                            \"source\": file_key,\n",
    "                            \"chunk_type\": \"numbered_section_part\",\n",
    "                            \"section\": title,\n",
    "                            \"part\": chunk_num,\n",
    "                            \"tokens\": current_tokens\n",
    "                        })\n",
    "                        chunk_num += 1\n",
    "                        current_chunk = f\"文件: {file_key}\\n章节: {title} (续)\\n\\n{para}\"\n",
    "                        current_tokens = estimate_tokens(current_chunk)\n",
    "                    else:\n",
    "                        current_chunk += f\"\\n\\n{para}\"\n",
    "                        current_tokens += para_tokens\n",
    "                \n",
    "                # 添加最后一个块\n",
    "                if current_chunk:\n",
    "                    self.chunks.append({\n",
    "                        \"id\": f\"{file_key}_section_{i}_{chunk_num}\",\n",
    "                        \"content\": current_chunk,\n",
    "                        \"source\": file_key,\n",
    "                        \"chunk_type\": \"numbered_section_part\",\n",
    "                        \"section\": title,\n",
    "                        \"part\": chunk_num,\n",
    "                        \"tokens\": current_tokens\n",
    "                    })\n",
    "            else:\n",
    "                # 块大小合适，直接添加\n",
    "                self.chunks.append({\n",
    "                    \"id\": f\"{file_key}_section_{i}\",\n",
    "                    \"content\": chunk_content,\n",
    "                    \"source\": file_key,\n",
    "                    \"chunk_type\": \"numbered_section\",\n",
    "                    \"section\": title,\n",
    "                    \"tokens\": chunk_tokens\n",
    "                })\n",
    "        \n",
    "        print(f\"按编号章节分块: {file_key}, 创建了{len(sections) - 1}个章节块\")\n",
    "    \n",
    "    def _split_by_semantic_paragraphs(self, file_key, content):\n",
    "        \"\"\"按语义段落分块，保留上下文连贯性\"\"\"\n",
    "        # 分割成自然段落\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', content)\n",
    "        \n",
    "        # 使用滑动窗口方法，保持上下文重叠\n",
    "        window_size = 5  # 每个窗口包含的段落数\n",
    "        stride = 3       # 窗口滑动步长\n",
    "        \n",
    "        for i in range(0, len(paragraphs), stride):\n",
    "            # 获取当前窗口的段落\n",
    "            window_paras = paragraphs[i:i + window_size]\n",
    "            \n",
    "            if not window_paras:\n",
    "                continue\n",
    "                \n",
    "            # 创建块内容\n",
    "            start_idx = i + 1\n",
    "            end_idx = min(i + len(window_paras), len(paragraphs))\n",
    "            \n",
    "            chunk_content = f\"文件: {file_key} (段落 {start_idx}-{end_idx}，共{len(paragraphs)}段)\\n\\n\"\n",
    "            chunk_content += \"\\n\\n\".join(window_paras)\n",
    "            \n",
    "            self.chunks.append({\n",
    "                \"id\": f\"{file_key}_para_{start_idx}_{end_idx}\",\n",
    "                \"content\": chunk_content,\n",
    "                \"source\": file_key,\n",
    "                \"chunk_type\": \"semantic_paragraphs\",\n",
    "                \"paragraph_range\": [start_idx, end_idx],\n",
    "                \"tokens\": estimate_tokens(chunk_content)\n",
    "            })\n",
    "        \n",
    "        print(f\"按语义段落分块: {file_key}, 创建了{(len(paragraphs) - 1) // stride + 1}个块\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2841dc70-c541-4cd9-9f5d-3cb327aaa4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.327876Z",
     "iopub.status.busy": "2025-04-15T10:59:37.327876Z",
     "iopub.status.idle": "2025-04-15T10:59:37.358523Z",
     "shell.execute_reply": "2025-04-15T10:59:37.358523Z"
    }
   },
   "outputs": [],
   "source": [
    "# 嵌入模型检索系统\n",
    "class EmbeddingRetriever:\n",
    "    def __init__(self, chunks, model):\n",
    "        self.chunks = chunks\n",
    "        self.model = model\n",
    "        \n",
    "        # 初始化文件索引\n",
    "        self.file_index = {}\n",
    "        for chunk in chunks:\n",
    "            source = chunk[\"source\"]\n",
    "            if source not in self.file_index:\n",
    "                self.file_index[source] = []\n",
    "            self.file_index[source].append(chunk)\n",
    "        \n",
    "        # 构建索引\n",
    "        self._build_index()\n",
    "    \n",
    "    # def _build_index(self):\n",
    "    #     \"\"\"构建嵌入索引\"\"\"\n",
    "    #     print(\"构建嵌入索引...\")\n",
    "        \n",
    "    #     # 准备文档内容\n",
    "    #     self.chunk_contents = [chunk[\"content\"] for chunk in self.chunks]\n",
    "        \n",
    "    #     # 计算嵌入 - 分批处理以避免内存溢出\n",
    "    #     batch_size = 32\n",
    "    #     all_embeddings = []\n",
    "        \n",
    "    #     # 使用tqdm展示进度\n",
    "    #     for i in tqdm(range(0, len(self.chunk_contents), batch_size), desc=\"生成嵌入向量\"):\n",
    "    #         batch = self.chunk_contents[i:i+batch_size]\n",
    "    #         batch_embeddings = self.model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "    #         all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "    #     # 合并所有批次的嵌入\n",
    "    #     self.embeddings = torch.cat(all_embeddings)\n",
    "        \n",
    "    #     print(f\"索引构建完成，文档数: {len(self.chunks)}, 嵌入维度: {self.embeddings.shape[1]}\")\n",
    "    def _build_index(self):\n",
    "        \"\"\"构建嵌入索引\"\"\"\n",
    "        print(\"构建嵌入索引...\")\n",
    "        \n",
    "        # 准备文档内容 - 这行没有问题，但应该在使用前定义\n",
    "        self.chunk_contents = [chunk[\"content\"] for chunk in self.chunks]\n",
    "        \n",
    "        # 为RTX 4090D优化的批处理设置\n",
    "        batch_size = 64  # 可以使用更大的批次\n",
    "        \n",
    "        # 如果使用CUDA，设置自动混合精度计算来加速\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"启用自动混合精度计算以提高性能\")\n",
    "            with torch.cuda.amp.autocast():\n",
    "                all_embeddings = []\n",
    "                for i in tqdm(range(0, len(self.chunk_contents), batch_size), desc=\"生成嵌入向量\"):\n",
    "                    batch = self.chunk_contents[i:i+batch_size]\n",
    "                    batch_embeddings = self.model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "                    all_embeddings.append(batch_embeddings)\n",
    "        else:\n",
    "            # CPU处理\n",
    "            all_embeddings = []\n",
    "            for i in tqdm(range(0, len(self.chunk_contents), batch_size), desc=\"生成嵌入向量\"):\n",
    "                batch = self.chunk_contents[i:i+batch_size]\n",
    "                batch_embeddings = self.model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        # 合并所有批次的嵌入\n",
    "        self.embeddings = torch.cat(all_embeddings)\n",
    "        \n",
    "        print(f\"索引构建完成，文档数: {len(self.chunks)}, 嵌入维度: {self.embeddings.shape[1]}\")\n",
    "    \n",
    "    def get_file_list(self):\n",
    "        \"\"\"获取所有文件名列表\"\"\"\n",
    "        return list(self.file_index.keys())\n",
    "    \n",
    "    def retrieve_files(self, query, top_k=5):\n",
    "        \"\"\"检索与查询最相关的文件\"\"\"\n",
    "        # 生成查询嵌入\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # 计算每个块的相似度\n",
    "        chunk_similarities = util.cos_sim(query_embedding, self.embeddings)[0].cpu().numpy()\n",
    "        \n",
    "        # 计算每个文件的加权分数\n",
    "        file_scores = {}\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            source = chunk[\"source\"]\n",
    "            if source not in file_scores:\n",
    "                file_scores[source] = {\"total\": 0, \"count\": 0, \"max\": 0}\n",
    "            \n",
    "            file_scores[source][\"total\"] += chunk_similarities[i]\n",
    "            file_scores[source][\"count\"] += 1\n",
    "            file_scores[source][\"max\"] = max(file_scores[source][\"max\"], chunk_similarities[i])\n",
    "        \n",
    "        # 使用加权组合：最大相似度有70%权重，平均相似度有30%权重\n",
    "        for source in file_scores:\n",
    "            avg_sim = file_scores[source][\"total\"] / file_scores[source][\"count\"]\n",
    "            max_sim = file_scores[source][\"max\"]\n",
    "            file_scores[source][\"score\"] = 0.7 * max_sim + 0.3 * avg_sim\n",
    "        \n",
    "        # 排序文件\n",
    "        sorted_files = sorted(\n",
    "            [{\"id\": source, \"relevance\": float(file_scores[source][\"score\"])} \n",
    "             for source in file_scores],\n",
    "            key=lambda x: x[\"relevance\"],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        return sorted_files[:top_k]\n",
    "    \n",
    "    # 在EmbeddingRetriever类中修改retrieve方法\n",
    "    def retrieve(self, query, top_k=25, max_tokens=45000, similarity_threshold=0.35):  # 增加参数\n",
    "        \"\"\"检索与查询最相关的文档块\"\"\"\n",
    "        # 生成查询嵌入\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # 计算相似度\n",
    "        similarities = util.cos_sim(query_embedding, self.embeddings)[0].cpu().numpy()\n",
    "        \n",
    "        # 应用相似度阈值过滤\n",
    "        qualified_indices = np.where(similarities > similarity_threshold)[0]\n",
    "        \n",
    "        # 如果结果太少，回退到top-k策略\n",
    "        if len(qualified_indices) < 5:\n",
    "            print(f\"注意: 只有 {len(qualified_indices)} 个文档块超过相似度阈值 {similarity_threshold}，采用前70个索引\")\n",
    "            top_indices = similarities.argsort()[-70:][::-1]  # 扩大候选池\n",
    "        else:\n",
    "            # 按相似度从高到低排序\n",
    "            top_indices = qualified_indices[np.argsort(-similarities[qualified_indices])]\n",
    "            if len(top_indices) > 70:\n",
    "                top_indices = top_indices[:70]  # 限制候选池大小\n",
    "        \n",
    "        # 按相关性组织结果\n",
    "        results = []\n",
    "        current_tokens = estimate_tokens(query)\n",
    "        sources_included = set()\n",
    "        \n",
    "        # 首先确保包含各文件中最相关的块\n",
    "        for idx in top_indices:\n",
    "            chunk = self.chunks[idx]\n",
    "            source = chunk[\"source\"]\n",
    "            \n",
    "            # 如果这个源文件还没有被包含，且token数量允许\n",
    "            if source not in sources_included and current_tokens + chunk[\"tokens\"] <= max_tokens:\n",
    "                results.append({\n",
    "                    \"chunk\": chunk,\n",
    "                    \"similarity\": float(similarities[idx])\n",
    "                })\n",
    "                current_tokens += chunk[\"tokens\"]\n",
    "                sources_included.add(source)\n",
    "                \n",
    "                # 如果已经包含了所有必要文件，且达到一定数量，可以提前结束\n",
    "                if len(sources_included) >= len(self.file_index) or len(results) >= 10:\n",
    "                    break\n",
    "        \n",
    "        # 然后添加其他高相关性块，直到达到token限制\n",
    "        for idx in top_indices:\n",
    "            chunk = self.chunks[idx]\n",
    "            \n",
    "            # 检查是否已经包含在结果中\n",
    "            if not any(r[\"chunk\"][\"id\"] == chunk[\"id\"] for r in results):\n",
    "                # 如果添加这个块不会超过token限制\n",
    "                if current_tokens + chunk[\"tokens\"] <= max_tokens * 0.9:  # 保留10%的余量\n",
    "                    results.append({\n",
    "                        \"chunk\": chunk,\n",
    "                        \"similarity\": float(similarities[idx])\n",
    "                    })\n",
    "                    current_tokens += chunk[\"tokens\"]\n",
    "            \n",
    "            # 如果已经达到top_k或接近token限制，就停止\n",
    "            if len(results) >= top_k or current_tokens > max_tokens * 0.85:\n",
    "                break\n",
    "        \n",
    "        # 估算提示token使用量\n",
    "        estimated_prompt_tokens = 1500  # 估计其他提示内容的token数\n",
    "        safe_token_limit = max_tokens - estimated_prompt_tokens\n",
    "        \n",
    "        # 如果仍然超出安全限制，进行裁剪\n",
    "        if current_tokens > safe_token_limit:\n",
    "            print(f\"警告: 检索结果超出安全token限制，进行裁剪 ({current_tokens} > {safe_token_limit})\")\n",
    "            \n",
    "            # 保留必要文件和高相关性块\n",
    "            sorted_results = sorted(results, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "            trimmed_results = []\n",
    "            trimmed_tokens = 0\n",
    "            sources_kept = set()\n",
    "            \n",
    "            # 首先保证每个源文件至少有一个块\n",
    "            for result in sorted_results:\n",
    "                source = result[\"chunk\"][\"source\"]\n",
    "                if source not in sources_kept:\n",
    "                    trimmed_results.append(result)\n",
    "                    trimmed_tokens += result[\"chunk\"][\"tokens\"]\n",
    "                    sources_kept.add(source)\n",
    "                    \n",
    "                    if trimmed_tokens >= safe_token_limit * 0.7:  # 使用70%的token预算保留每个文件\n",
    "                        break\n",
    "            \n",
    "            # 然后添加高相关性的块直到达到限制\n",
    "            for result in sorted_results:\n",
    "                if result not in trimmed_results and trimmed_tokens + result[\"chunk\"][\"tokens\"] <= safe_token_limit:\n",
    "                    trimmed_results.append(result)\n",
    "                    trimmed_tokens += result[\"chunk\"][\"tokens\"]\n",
    "            \n",
    "            results = trimmed_results\n",
    "            current_tokens = trimmed_tokens\n",
    "        \n",
    "        print(f\"检索到 {len(results)} 个文档块，总计约 {current_tokens} tokens (R1输入限制: {max_tokens})\")\n",
    "        \n",
    "        # 按相似度排序结果\n",
    "        results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        \n",
    "        return results, current_tokens\n",
    "    \n",
    "    def generate_file_metadata(self):\n",
    "        \"\"\"生成文件元数据，用于增强器\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        for source, chunks in self.file_index.items():\n",
    "            # 找出这个文件的元数据块\n",
    "            metadata_chunks = [c for c in chunks if c[\"chunk_type\"] in [\"excel_metadata\", \"text_summary\"]]\n",
    "            \n",
    "            if metadata_chunks:\n",
    "                # 使用元数据块的内容\n",
    "                chunk = metadata_chunks[0]\n",
    "                content = chunk[\"content\"]\n",
    "                \n",
    "                # 提取文件类型\n",
    "                file_type = \"Unknown\"\n",
    "                if source.startswith(\"excel\"):\n",
    "                    file_type = \"Excel表格\"\n",
    "                elif source.startswith(\"txt\"):\n",
    "                    file_type = \"文本文档\"\n",
    "                elif source.startswith(\"md\"):\n",
    "                    file_type = \"Markdown文档\"\n",
    "                \n",
    "                # 提取描述信息\n",
    "                description = source\n",
    "                if \"表格概览\" in content:\n",
    "                    description_match = re.search(r\"表格概览: .*行\", content)\n",
    "                    if description_match:\n",
    "                        description = description_match.group(0)\n",
    "                elif \"字符数\" in content:\n",
    "                    description_match = re.search(r\"字符数: .*行\", content)\n",
    "                    if description_match:\n",
    "                        description = description_match.group(0)\n",
    "                \n",
    "                # 提取关键列或章节\n",
    "                key_items = []\n",
    "                if file_type == \"Excel表格\" and \"列名:\" in content:\n",
    "                    columns_match = re.search(r\"列名: (.*)\", content)\n",
    "                    if columns_match:\n",
    "                        columns = columns_match.group(1).split(\", \")\n",
    "                        key_items = columns[:5]  # 取前5列作为关键列\n",
    "                else:\n",
    "                    # 提取文本文件中可能的章节\n",
    "                    sections = []\n",
    "                    for c in chunks:\n",
    "                        if \"章节:\" in c[\"content\"]:\n",
    "                            section_match = re.search(r\"章节: ([^>\\n]*)\", c[\"content\"])\n",
    "                            if section_match:\n",
    "                                sections.append(section_match.group(1).strip())\n",
    "                    key_items = list(set(sections))[:5]  # 去重并取前5个\n",
    "                \n",
    "                # 构建元数据\n",
    "                metadata[source] = {\n",
    "                    \"type\": file_type,\n",
    "                    \"description\": description,\n",
    "                    \"key_items\": key_items\n",
    "                }\n",
    "            else:\n",
    "                # 如果没有元数据块，使用基本信息\n",
    "                file_type = \"未知文件类型\"\n",
    "                if source.startswith(\"excel\"):\n",
    "                    file_type = \"Excel表格\"\n",
    "                elif source.startswith(\"txt\"):\n",
    "                    file_type = \"文本文档\"\n",
    "                elif source.startswith(\"md\"):\n",
    "                    file_type = \"Markdown文档\"\n",
    "                    \n",
    "                metadata[source] = {\n",
    "                    \"type\": file_type,\n",
    "                    \"description\": source,\n",
    "                    \"key_items\": []\n",
    "                }\n",
    "        \n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f055869-4203-41ca-9805-5ba65a2564d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.358523Z",
     "iopub.status.busy": "2025-04-15T10:59:37.358523Z",
     "iopub.status.idle": "2025-04-15T10:59:37.374621Z",
     "shell.execute_reply": "2025-04-15T10:59:37.373592Z"
    }
   },
   "outputs": [],
   "source": [
    "# 文本增强器 - 使用检索结果生成结构化上下文\n",
    "class TextRetrieverAugmenter:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "        self.file_metadata = retriever.generate_file_metadata()\n",
    "    \n",
    "    def augment_query(self, query, prompt_to_r1, max_files=5, max_context_tokens=45000):  # 增加默认token限制\n",
    "        \"\"\"增强查询，生成适合R1的详细上下文\"\"\"\n",
    "        # 1. 检索相关文件\n",
    "        relevant_files = self.retriever.retrieve_files(query, top_k=max_files)\n",
    "        \n",
    "        # 2. 生成增强上下文\n",
    "        augmented_context = self._generate_structured_context(query, relevant_files)\n",
    "        \n",
    "        # 3. 估算上下文token数量\n",
    "        context_tokens = estimate_tokens(augmented_context)\n",
    "        \n",
    "        # 4. 如果超出限制，进行缩减\n",
    "        if context_tokens > max_context_tokens:\n",
    "            # 简化策略: 保留较少的相关文件\n",
    "            reduced_files = max(2, int(max_files * max_context_tokens / context_tokens))\n",
    "            relevant_files = self.retriever.retrieve_files(query, top_k=reduced_files)\n",
    "            augmented_context = self._generate_structured_context(query, relevant_files, simplified=True)\n",
    "        \n",
    "        # 5. 融合到R1提示\n",
    "        enhanced_prompt = f\"\"\"\n",
    "    {prompt_to_r1}\n",
    "    \n",
    "    <RETRIEVAL_RESULTS>\n",
    "    {augmented_context}\n",
    "    </RETRIEVAL_RESULTS>\n",
    "    \n",
    "    请基于以上检索结果中提供的文件内容进行分析，按照任务要求生成响应。\n",
    "    请特别关注\"PRIMARY_FILES\"部分的关键信息，并参考\"RETRIEVAL_GUIDANCE\"提供的分析策略。\n",
    "    请根据Prompt生成结果后，进行检查，确保只输出最好、最完整的一个结果。\n",
    "    \"\"\"\n",
    "        return enhanced_prompt, relevant_files\n",
    "    \n",
    "    def _generate_structured_context(self, query, relevant_files, simplified=False):\n",
    "        \"\"\"生成结构化上下文信息\"\"\"\n",
    "        # 将相关文件分为主要和次要文件\n",
    "        primary_files = relevant_files[:min(3, len(relevant_files))]\n",
    "        secondary_files = relevant_files[3:min(5, len(relevant_files))]\n",
    "        \n",
    "        output = \"<FILE_SELECTION>\\n\"\n",
    "        output += \"以下是与查询相关的文件列表，按相关性从高到低排序：\\n\\n\"\n",
    "        \n",
    "        # 添加主要文件\n",
    "        output += \"<PRIMARY_FILES>\\n\"\n",
    "        for i, file in enumerate(primary_files):\n",
    "            file_id = file[\"id\"]\n",
    "            relevance = file[\"relevance\"]\n",
    "            metadata = self.file_metadata.get(file_id, {})\n",
    "            \n",
    "            output += f\"{i+1}. {file_id} [相关性:{relevance:.2f}]\\n\"\n",
    "            output += f\"   - 文件类型: {metadata.get('type', '未知类型')}\\n\"\n",
    "            output += f\"   - 主要内容: {metadata.get('description', '未知内容')}\\n\"\n",
    "            \n",
    "            # 添加关键项目\n",
    "            key_items = metadata.get('key_items', [])\n",
    "            if key_items:\n",
    "                item_type = \"关键列\" if metadata.get('type', '').startswith(\"Excel\") else \"关键章节\"\n",
    "                output += f\"   - {item_type}: {', '.join(key_items)}\\n\"\n",
    "            \n",
    "            output += \"\\n\"\n",
    "        \n",
    "        # 添加次要文件\n",
    "        if not simplified and secondary_files:\n",
    "            output += \"<SECONDARY_FILES>\\n\"\n",
    "            for i, file in enumerate(secondary_files):\n",
    "                file_id = file[\"id\"]\n",
    "                relevance = file[\"relevance\"]\n",
    "                metadata = self.file_metadata.get(file_id, {})\n",
    "                \n",
    "                output += f\"{i+len(primary_files)+1}. {file_id} [相关性:{relevance:.2f}]\\n\"\n",
    "                output += f\"   - 文件类型: {metadata.get('type', '未知类型')}\\n\"\n",
    "                output += f\"   - 主要内容: {metadata.get('description', '未知内容')}\\n\"\n",
    "                output += f\"   - 参考价值: 辅助分析信息\\n\\n\"\n",
    "        \n",
    "        # 添加文件间关系\n",
    "        output += \"<DATA_RELATIONSHIPS>\\n\"\n",
    "        \n",
    "        # 识别特定文件间的关系\n",
    "        excel_files = [f[\"id\"] for f in relevant_files if f[\"id\"].startswith(\"excel\")]\n",
    "        txt_files = [f[\"id\"] for f in relevant_files if f[\"id\"].startswith(\"txt\")]\n",
    "        md_files = [f[\"id\"] for f in relevant_files if f[\"id\"].startswith(\"md\")]\n",
    "        \n",
    "        # 分析文件关系\n",
    "        if len(primary_files) >= 2:\n",
    "            file1 = primary_files[0][\"id\"]\n",
    "            file2 = primary_files[1][\"id\"]\n",
    "            \n",
    "            # 检查是否是同一类型的文件\n",
    "            if file1.startswith(\"excel\") and file2.startswith(\"excel\"):\n",
    "                output += f\"- {file1}与{file2}都是Excel表格，可能包含相关但不同维度的数据，建议交叉分析\\n\"\n",
    "            elif file1.startswith(\"txt\") and file2.startswith(\"txt\"):\n",
    "                output += f\"- {file1}与{file2}都是文本文档，可能分别包含不同角度的分析，应结合阅读\\n\"\n",
    "            else:\n",
    "                output += f\"- {file1}与{file2}是不同类型的文件，可能一个包含数据而另一个包含分析结果\\n\"\n",
    "        \n",
    "        if excel_files and txt_files:\n",
    "            output += f\"- Excel文件({', '.join(excel_files[:2])})提供了结构化数据，而文本文件({', '.join(txt_files[:2])})可能包含对这些数据的分析和解释\\n\"\n",
    "        \n",
    "        if txt_files and md_files:\n",
    "            output += f\"- 文本文件({', '.join(txt_files[:2])})与Markdown文件({', '.join(md_files[:2])})可能包含相关内容，Markdown文件通常提供更结构化的总结\\n\"\n",
    "        \n",
    "        if not excel_files and not txt_files and not md_files:\n",
    "            output += \"- 未检测到明确的文件间关系，请根据内容自行判断\\n\"\n",
    "            \n",
    "        output += \"</DATA_RELATIONSHIPS>\\n\"\n",
    "        \n",
    "        # 如果不是简化版本，添加检索指导\n",
    "        if not simplified:\n",
    "            output += \"\\n<RETRIEVAL_GUIDANCE>\\n\"\n",
    "            output += \"为高效分析这些数据，建议采取以下策略:\\n\\n\"\n",
    "            \n",
    "            # 根据文件类型提供不同的建议\n",
    "            guidance_added = False\n",
    "            \n",
    "            if excel_files:\n",
    "                output += f\"1. 首先分析{excel_files[0]}获取基础数据结构和关键指标\\n\"\n",
    "                guidance_added = True\n",
    "                \n",
    "            if txt_files:\n",
    "                prefix = \"1\" if not guidance_added else \"2\"\n",
    "                output += f\"{prefix}. 结合{txt_files[0]}中的分析文本理解数据含义和背景\\n\"\n",
    "                guidance_added = True\n",
    "                \n",
    "            if md_files:\n",
    "                prefix = \"1\" if not guidance_added else (\"3\" if excel_files and txt_files else \"2\")\n",
    "                output += f\"{prefix}. 参考{md_files[0]}中的结论和建议验证您的分析方向\\n\"\n",
    "                \n",
    "            if not guidance_added:\n",
    "                output += \"1. 仔细阅读检索到的所有文件，寻找与查询相关的关键信息\\n\"\n",
    "                output += \"2. 对比不同文件中的内容，寻找共同点和差异点\\n\"\n",
    "            \n",
    "            output += \"\\n分析要点:\\n\"\n",
    "            output += \"- 注意文件间的数据一致性和互补关系\\n\"\n",
    "            output += \"- 优先关注相关性分数高的文件中的信息\\n\"\n",
    "            output += \"- 针对需求生成高质量的分析结果，确保表格格式规范\\n\"\n",
    "            output += \"</RETRIEVAL_GUIDANCE>\\n\"\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de4f36b-4729-45ac-a1a1-7a33e490045b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.376753Z",
     "iopub.status.busy": "2025-04-15T10:59:37.376250Z",
     "iopub.status.idle": "2025-04-15T10:59:37.389506Z",
     "shell.execute_reply": "2025-04-15T10:59:37.388900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 解析表格函数\n",
    "def parse_table_to_json(table_text):\n",
    "    \"\"\"尝试解析Markdown表格为JSON格式\"\"\"\n",
    "    try:\n",
    "        # 查找表格\n",
    "        lines = table_text.strip().split('\\n')\n",
    "        table_start = None\n",
    "        table_end = None\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if '|' in line:\n",
    "                if table_start is None:\n",
    "                    table_start = i\n",
    "                table_end = i\n",
    "        \n",
    "        if table_start is None or table_end is None:\n",
    "            return None\n",
    "        \n",
    "        # 提取表格行\n",
    "        table_lines = lines[table_start:table_end+1]\n",
    "        \n",
    "        # 解析表头\n",
    "        headers = [h.strip() for h in table_lines[0].split('|') if h.strip()]\n",
    "        \n",
    "        # 跳过分隔行\n",
    "        data_start = 2 if len(table_lines) > 1 and ('---' in table_lines[1] or ':--' in table_lines[1]) else 1\n",
    "        \n",
    "        # 解析数据行\n",
    "        results = []\n",
    "        for i in range(data_start, len(table_lines)):\n",
    "            line = table_lines[i]\n",
    "            cells = [c.strip() for c in line.split('|') if c.strip() or c == ' ']\n",
    "            \n",
    "            if len(cells) >= len(headers):\n",
    "                row_data = {}\n",
    "                for j, header in enumerate(headers):\n",
    "                    if j < len(cells):\n",
    "                        row_data[header] = cells[j]\n",
    "                results.append(row_data)\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"解析表格失败: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b85fff1-1322-4663-a5e4-fb4dd5a31de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.389506Z",
     "iopub.status.busy": "2025-04-15T10:59:37.389506Z",
     "iopub.status.idle": "2025-04-15T10:59:37.404058Z",
     "shell.execute_reply": "2025-04-15T10:59:37.404058Z"
    }
   },
   "outputs": [],
   "source": [
    "# 顺序处理多个Prompt的类\n",
    "class SequentialPromptProcessor:\n",
    "    def __init__(self, prompt_files, data_files, output_dir, retriever_augmenter):\n",
    "        self.prompt_files = prompt_files\n",
    "        self.data_files = data_files\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.retriever_augmenter = retriever_augmenter\n",
    "        \n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # 初始化结果存储\n",
    "        self.results = {}\n",
    "    \n",
    "    def process_all_prompts(self):\n",
    "        \"\"\"逐个处理所有Prompt文件\"\"\"\n",
    "        # 获取所有Prompt文件\n",
    "        prompt_items = sorted(self.prompt_files.items())\n",
    "        total_files = len(prompt_items)\n",
    "        \n",
    "        print(f\"找到{total_files}个Prompt文件，开始顺序处理...\")\n",
    "        \n",
    "        # 逐个处理\n",
    "        for i, (prompt_key, prompt_path) in enumerate(prompt_items, 1):\n",
    "            print(f\"\\n[{i}/{total_files}] 处理 {prompt_key}...\")\n",
    "            \n",
    "            # 处理单个Prompt\n",
    "            result = self.process_single_prompt(prompt_key, prompt_path)\n",
    "            \n",
    "            # 保存结果\n",
    "            self.results[prompt_key] = result\n",
    "            \n",
    "            # 单独保存当前表格结果\n",
    "            self._save_single_result(prompt_key, result)\n",
    "            \n",
    "            # 间隔一下，避免API限制\n",
    "            if i < total_files:\n",
    "                print(f\"等待3秒后处理下一个Prompt...\")\n",
    "                time.sleep(3)\n",
    "        \n",
    "        # 保存所有结果的汇总\n",
    "        self._save_all_results()\n",
    "        \n",
    "        print(f\"\\n所有Prompt处理完成！结果保存在 {self.output_dir}\")\n",
    "        return self.results\n",
    "    \n",
    "    def process_single_prompt(self, prompt_key, prompt_path):\n",
    "        \"\"\"处理单个Prompt文件\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 读取Prompt内容\n",
    "        with open(prompt_path, 'r', encoding='utf-8') as f:\n",
    "            prompt_text = f.read()\n",
    "        \n",
    "        print(f\"开始处理Prompt: {prompt_key} (长度: {len(prompt_text)}字符)\")\n",
    "        \n",
    "        # 提取查询核心 - 用于检索相关文件\n",
    "        # 使用Prompt的前300个字符作为检索查询\n",
    "        query = prompt_text[:min(300, len(prompt_text))]\n",
    "        \n",
    "        # 使用增强器生成增强上下文\n",
    "        try:\n",
    "            enhanced_prompt, relevant_files = self.retriever_augmenter.augment_query(query, prompt_text)\n",
    "            \n",
    "            # 输出检索到的相关文件\n",
    "            file_info = \", \".join([f\"{f['id']} ({f['relevance']:.2f})\" for f in relevant_files[:3]])\n",
    "            print(f\"检索到主要相关文件: {file_info}\")\n",
    "            \n",
    "            # 构建最终提示 - 添加额外指导\n",
    "            final_prompt = f\"\"\"\n",
    "{enhanced_prompt}\n",
    "\n",
    "请注意:\n",
    "1. <PRIMARY_FILES>标签内包含与任务最相关的核心文件信息，应作为主要分析依据\n",
    "2. <SECONDARY_FILES>标签内包含辅助信息，可用于补充分析\n",
    "3. <DATA_RELATIONSHIPS>标签解释了文件间的关系，有助于综合分析\n",
    "4. <RETRIEVAL_GUIDANCE>提供了高效分析这些数据的策略建议\n",
    "\n",
    "在分析过程中，请优先使用相关性更高的信息。\n",
    "请生成一个格式规范的Markdown表格，确保表格结构完整，列对齐，以满足任务要求。\n",
    "请确保只生成最完整的一个结果，输出前进行核查。\n",
    "\"\"\"\n",
    "            # 打印处理情况\n",
    "            print(f\"增强提示生成完成，长度约 {len(final_prompt)} 字符\")\n",
    "            \n",
    "            # 估算token数量\n",
    "            estimated_tokens = estimate_tokens(final_prompt)\n",
    "            print(f\"估计输入token数: {estimated_tokens}\")\n",
    "            \n",
    "            # 检查token数量是否在限制内\n",
    "            if estimated_tokens > 52000:  # R1模型限制\n",
    "                print(f\"警告: 估计token数超过限制，尝试缩减检索结果...\")\n",
    "                # 简化策略: 保留较少的相关文件\n",
    "                enhanced_prompt, relevant_files = self.retriever_augmenter.augment_query(\n",
    "                    query, prompt_text, max_files=5, max_context_tokens=35000)  # 减少文件数量和上下文长度\n",
    "                \n",
    "                final_prompt = f\"{enhanced_prompt}\\n\\n请注意优先使用相关性更高的信息，并生成一个格式规范的Markdown表格。\"\n",
    "                estimated_tokens = estimate_tokens(final_prompt)\n",
    "                print(f\"缩减后估计token数: {estimated_tokens}\")\n",
    "            \n",
    "            # 调用R1模型\n",
    "            print(f\"调用R1模型...\")\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"deepseek-r1\",\n",
    "                messages=[\n",
    "                    {'role': 'user', 'content': final_prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                \"reasoning\": completion.choices[0].message.reasoning_content,\n",
    "                \"result\": completion.choices[0].message.content,\n",
    "                \"retrieved_files\": [f[\"id\"] for f in relevant_files],\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            print(f\"R1响应完成，处理耗时: {result['processing_time']:.2f}秒\")\n",
    "            \n",
    "            # 尝试解析表格\n",
    "            table_json = parse_table_to_json(result[\"result\"])\n",
    "            if table_json:\n",
    "                print(f\"成功解析表格，包含 {len(table_json)} 行数据\")\n",
    "                result[\"table_data\"] = table_json\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"处理 {prompt_key} 时出错: {str(e)}\"\n",
    "            print(f\"错误: {error_msg}\")\n",
    "            return {\n",
    "                \"error\": error_msg,\n",
    "                \"processing_time\": time.time() - start_time\n",
    "            }\n",
    "    \n",
    "    def _save_single_result(self, prompt_key, result):\n",
    "        \"\"\"保存单个Prompt的结果\"\"\"\n",
    "        # 从prompt_key提取文件名前缀\n",
    "        # 例如从prompt_1提取出建议表1-表达-流量关系表\n",
    "        file_prefix = \"\"\n",
    "        for pkey, ppath in self.prompt_files.items():\n",
    "            if pkey == prompt_key:\n",
    "                # 提取Prompt.txt前面的文字\n",
    "                match = re.search(r'.*?(?=Prompt\\.txt)', os.path.basename(ppath))\n",
    "                if match:\n",
    "                    file_prefix = match.group(0).rstrip('-')\n",
    "                else:\n",
    "                    file_prefix = os.path.basename(ppath).split('Prompt.txt')[0].rstrip('-')\n",
    "                break\n",
    "        \n",
    "        if not file_prefix:\n",
    "            file_prefix = prompt_key\n",
    "        \n",
    "        # 保存原始结果（包含推理过程等）\n",
    "        result_path = self.output_dir / f\"{file_prefix}_full.json\"\n",
    "        with open(result_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 单独保存表格结果（Markdown格式）\n",
    "        if \"result\" in result:\n",
    "            table_path = self.output_dir / f\"{file_prefix}_table.md\"\n",
    "            with open(table_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result[\"result\"])\n",
    "        \n",
    "        # 如果成功解析了表格数据，保存Excel格式\n",
    "        if \"table_data\" in result and result[\"table_data\"]:\n",
    "            excel_path = self.output_dir / f\"{file_prefix}.xlsx\"\n",
    "            try:\n",
    "                df = pd.DataFrame(result[\"table_data\"])\n",
    "                df.to_excel(excel_path, index=False)\n",
    "                print(f\"已保存Excel表格到 {excel_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"保存Excel文件失败: {e}\")\n",
    "        \n",
    "        print(f\"已保存 {prompt_key} 的结果到 {self.output_dir}\")\n",
    "    \n",
    "    def _save_all_results(self):\n",
    "        \"\"\"保存所有结果的汇总\"\"\"\n",
    "        all_results_path = self.output_dir / \"all_tables_results.json\"\n",
    "        with open(all_results_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908fff0f-f045-4e5b-b244-e3a80859ecff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.404058Z",
     "iopub.status.busy": "2025-04-15T10:59:37.404058Z",
     "iopub.status.idle": "2025-04-15T10:59:37.586952Z",
     "shell.execute_reply": "2025-04-15T10:59:37.586952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理数据文件...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加完整表格: excel_file_1 (3592 tokens)\n",
      "添加完整表格: excel_file_2 (3673 tokens)\n",
      "添加完整表格: excel_file_3 (579 tokens)\n",
      "添加完整文本: txt_file_4 (3294 字符)\n",
      "添加完整文本: txt_file_5 (3418 字符)\n",
      "添加完整文本: txt_file_6 (3306 字符)\n",
      "添加完整文本: txt_file_7 (3286 字符)\n",
      "添加完整文本: txt_file_8 (658 字符)\n",
      "添加完整文本: md_file_1 (5923 字符)\n",
      "添加完整文本: txt_file_1 (9395 字符)\n",
      "添加完整文本: txt_file_2 (9582 字符)\n",
      "添加完整文本: txt_file_3 (1266 字符)\n",
      "文件处理完成，共创建 12 个文档块\n"
     ]
    }
   ],
   "source": [
    "# 执行文档处理\n",
    "processor = DocumentProcessor(data_files)\n",
    "chunks, file_stats = processor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c470657-15b5-4907-b7fd-b2f8e5665526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.586952Z",
     "iopub.status.busy": "2025-04-15T10:59:37.586952Z",
     "iopub.status.idle": "2025-04-15T10:59:37.754160Z",
     "shell.execute_reply": "2025-04-15T10:59:37.754160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建嵌入索引...\n",
      "启用自动混合精度计算以提高性能\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb04513709549caa8ad64bb7f99110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "生成嵌入向量:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引构建完成，文档数: 12, 嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "# 创建检索系统 - 使用预训练模型\n",
    "retriever = EmbeddingRetriever(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9230cb84-bacf-4f67-8eab-cebcddefa50c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.754160Z",
     "iopub.status.busy": "2025-04-15T10:59:37.754160Z",
     "iopub.status.idle": "2025-04-15T10:59:37.769449Z",
     "shell.execute_reply": "2025-04-15T10:59:37.769449Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建增强器\n",
    "augmenter = TextRetrieverAugmenter(retriever)\n",
    "\n",
    "# 创建处理器\n",
    "processor = SequentialPromptProcessor(prompt_files, data_files, OUTPUT_DIR, augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e967cc2a-c42e-40bb-8ded-abf759ec6c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:59:37.769449Z",
     "iopub.status.busy": "2025-04-15T10:59:37.769449Z",
     "iopub.status.idle": "2025-04-15T11:03:49.907375Z",
     "shell.execute_reply": "2025-04-15T11:03:49.907375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到6个Prompt文件，开始顺序处理...\n",
      "\n",
      "[1/6] 处理 prompt_1...\n",
      "开始处理Prompt: prompt_1 (长度: 2081字符)\n",
      "检索到主要相关文件: md_file_1 (0.62), excel_file_3 (0.59), excel_file_2 (0.59)\n",
      "增强提示生成完成，长度约 3517 字符\n",
      "估计输入token数: 2621\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 42.39秒\n",
      "成功解析表格，包含 3 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表1-表达-流量关系表.xlsx\n",
      "已保存 prompt_1 的结果到 生成结果\\last_quadrant\n",
      "等待3秒后处理下一个Prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] 处理 prompt_2...\n",
      "开始处理Prompt: prompt_2 (长度: 1791字符)\n",
      "检索到主要相关文件: md_file_1 (0.61), excel_file_3 (0.59), excel_file_2 (0.58)\n",
      "增强提示生成完成，长度约 3227 字符\n",
      "估计输入token数: 2286\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 42.86秒\n",
      "成功解析表格，包含 3 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表2-消费者旅程不同阶段的产品表达关注表.xlsx\n",
      "已保存 prompt_2 的结果到 生成结果\\last_quadrant\n",
      "等待3秒后处理下一个Prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] 处理 prompt_3...\n",
      "开始处理Prompt: prompt_3 (长度: 2073字符)\n",
      "检索到主要相关文件: excel_file_2 (0.59), excel_file_3 (0.56), excel_file_1 (0.56)\n",
      "增强提示生成完成，长度约 3499 字符\n",
      "估计输入token数: 2633\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 25.72秒\n",
      "成功解析表格，包含 3 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表3-品类功能利益点情感利益点分析表.xlsx\n",
      "已保存 prompt_3 的结果到 生成结果\\last_quadrant\n",
      "等待3秒后处理下一个Prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] 处理 prompt_4...\n",
      "开始处理Prompt: prompt_4 (长度: 2185字符)\n",
      "检索到主要相关文件: txt_file_2 (0.69), txt_file_3 (0.67), excel_file_3 (0.63)\n",
      "增强提示生成完成，长度约 3509 字符\n",
      "估计输入token数: 2683\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 42.24秒\n",
      "成功解析表格，包含 6 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表4-产品改进建议.xlsx\n",
      "已保存 prompt_4 的结果到 生成结果\\last_quadrant\n",
      "等待3秒后处理下一个Prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] 处理 prompt_5...\n",
      "开始处理Prompt: prompt_5 (长度: 2185字符)\n",
      "检索到主要相关文件: txt_file_2 (0.69), txt_file_3 (0.67), excel_file_3 (0.63)\n",
      "增强提示生成完成，长度约 3509 字符\n",
      "估计输入token数: 2683\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 40.06秒\n",
      "成功解析表格，包含 8 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表5-情感利益点改进建议.xlsx\n",
      "已保存 prompt_5 的结果到 生成结果\\last_quadrant\n",
      "等待3秒后处理下一个Prompt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] 处理 prompt_6...\n",
      "开始处理Prompt: prompt_6 (长度: 2308字符)\n",
      "检索到主要相关文件: excel_file_3 (0.62), txt_file_2 (0.62), excel_file_1 (0.62)\n",
      "增强提示生成完成，长度约 3635 字符\n",
      "估计输入token数: 2841\n",
      "调用R1模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1响应完成，处理耗时: 43.74秒\n",
      "成功解析表格，包含 6 行数据\n",
      "已保存Excel表格到 生成结果\\last_quadrant\\建议表6-改进优先级排序.xlsx\n",
      "已保存 prompt_6 的结果到 生成结果\\last_quadrant\n",
      "\n",
      "所有Prompt处理完成！结果保存在 生成结果\\last_quadrant\n",
      "\n",
      "处理总结:\n",
      "总共处理: 6个Prompt\n",
      "成功处理: 6个Prompt\n",
      "失败处理: 0个Prompt\n"
     ]
    }
   ],
   "source": [
    "# 处理所有Prompt\n",
    "results = processor.process_all_prompts()\n",
    "\n",
    "# 输出总结\n",
    "successful = sum(1 for r in results.values() if \"error\" not in r)\n",
    "print(f\"\\n处理总结:\")\n",
    "print(f\"总共处理: {len(results)}个Prompt\")\n",
    "print(f\"成功处理: {successful}个Prompt\")\n",
    "print(f\"失败处理: {len(results) - successful}个Prompt\")\n",
    "\n",
    "if len(results) - successful > 0:\n",
    "    print(\"\\n失败的Prompt:\")\n",
    "    for name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            print(f\"- {name}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcc836d-bce5-433d-ad61-65e1c575257d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T11:03:49.908423Z",
     "iopub.status.busy": "2025-04-15T11:03:49.908423Z",
     "iopub.status.idle": "2025-04-15T11:03:49.922416Z",
     "shell.execute_reply": "2025-04-15T11:03:49.922416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的Excel文件:\n",
      "- plot_quadrant_data_场景匹配.xlsx\n",
      "- plot_quadrant_data_痛点.xlsx\n",
      "- plot_quadrant_data_需求.xlsx\n",
      "- 四象限数据-场景匹配.xlsx\n",
      "- 四象限数据-痛点.xlsx\n",
      "- 四象限数据-需求.xlsx\n",
      "- 建议表1-表达-流量关系表.xlsx\n",
      "- 建议表2-消费者旅程不同阶段的产品表达关注表.xlsx\n",
      "- 建议表3-品类功能利益点情感利益点分析表.xlsx\n",
      "- 建议表4-产品改进建议.xlsx\n",
      "- 建议表5-情感利益点改进建议.xlsx\n",
      "- 建议表6-改进优先级排序.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 查看生成的文件\n",
    "import glob\n",
    "generated_files = glob.glob(f\"{OUTPUT_DIR}/*.xlsx\")\n",
    "print(f\"生成的Excel文件:\")\n",
    "for file in generated_files:\n",
    "    print(f\"- {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff8da0-b770-40a9-9aca-afc774538da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lynx",
   "language": "python",
   "name": "lynx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "148cc9b4ff234ae3923be51190301e25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c0a8b394bf049d5b9260097aadd4e35": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f524784a66e843989780732b04885ff1",
       "placeholder": "​",
       "style": "IPY_MODEL_c13119c987e445098807f846a0f00bb1",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  6.59it/s]"
      }
     },
     "340457330a5e4b7da5922c93ef6106ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "abb04513709549caa8ad64bb7f99110b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_be678441d7174fc28819351ca513e853",
        "IPY_MODEL_d1df7ba6913a491cbb28c10251262c58",
        "IPY_MODEL_2c0a8b394bf049d5b9260097aadd4e35"
       ],
       "layout": "IPY_MODEL_c22b9352e4c34a3c9330153cd85b73fe",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b314191393d64dcda9ed8c0b868f629f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "be678441d7174fc28819351ca513e853": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fec3bc65e0ce4453a12009b6de8b6126",
       "placeholder": "​",
       "style": "IPY_MODEL_b314191393d64dcda9ed8c0b868f629f",
       "tabbable": null,
       "tooltip": null,
       "value": "生成嵌入向量: 100%"
      }
     },
     "c13119c987e445098807f846a0f00bb1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c22b9352e4c34a3c9330153cd85b73fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1df7ba6913a491cbb28c10251262c58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_148cc9b4ff234ae3923be51190301e25",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_340457330a5e4b7da5922c93ef6106ae",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "f524784a66e843989780732b04885ff1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fec3bc65e0ce4453a12009b6de8b6126": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
