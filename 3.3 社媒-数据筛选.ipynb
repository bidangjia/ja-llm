{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:11.386822Z",
     "iopub.status.busy": "2025-04-15T10:45:11.384811Z",
     "iopub.status.idle": "2025-04-15T10:45:12.026127Z",
     "shell.execute_reply": "2025-04-15T10:45:12.026127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取文件: twitter_keywords_zo tech.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取数据，共 898 行和 37 列\n",
      "使用的文件夹名: 生成结果/social_media/\n",
      "使用的关键词文件: 生成结果/social_media/keywords.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# # 从Excel文件名中提取基础文件夹名称\n",
    "folder_name = '生成结果/social_media/'\n",
    "\n",
    "# 使用前面代码中生成的文件夹名和文件路径\n",
    "#filename_keywords = f'{folder_name}/keywords.txt'  # 指向新生成的关键词文件\n",
    "filename_keywords = os.path.join(folder_name, 'keywords.txt')\n",
    "# 设置数据文件夹路径\n",
    "origin_folder = './Data'\n",
    "\n",
    "# 查找所有以\"twitter_keywords\"开头的xlsx文件\n",
    "xlsx_pattern = os.path.join(origin_folder, 'twitter_keywords*.xlsx')\n",
    "xlsx_files = glob.glob(xlsx_pattern)\n",
    "\n",
    "if not xlsx_files:\n",
    "    print(f\"在 {origin_folder} 目录中未找到以'twitter_keywords'开头的Excel文件\")\n",
    "else:\n",
    "    # 使用找到的第一个文件（如果有多个文件，可以根据需要调整）\n",
    "    filename = xlsx_files[0]\n",
    "    print(f\"正在读取文件: {os.path.basename(filename)}\")\n",
    "    \n",
    "    # 读取Excel文件\n",
    "    data = pd.read_excel(filename)\n",
    "    \n",
    "    # 显示基本信息\n",
    "    print(f\"成功读取数据，共 {len(data)} 行和 {len(data.columns)} 列\")\n",
    "\n",
    "print(f\"使用的文件夹名: {folder_name}\")\n",
    "print(f\"使用的关键词文件: {filename_keywords}\")\n",
    "\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.026127Z",
     "iopub.status.busy": "2025-04-15T10:45:12.026127Z",
     "iopub.status.idle": "2025-04-15T10:45:12.041421Z",
     "shell.execute_reply": "2025-04-15T10:45:12.041421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 生成结果/social_media/hypo.md 不存在，请提供正确的文件路径\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def extract_markdown_table(markdown_file):\n",
    "    \"\"\"\n",
    "    从Markdown文件中提取表格内容并转换为DataFrame\n",
    "    \n",
    "    参数:\n",
    "    markdown_file - Markdown文件路径\n",
    "    \n",
    "    返回:\n",
    "    pandas DataFrame - 包含表格内容的数据框\n",
    "    \"\"\"\n",
    "    # 读取Markdown文件内容\n",
    "    with open(markdown_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 按行分割内容\n",
    "    lines = content.strip().split('\\n')\n",
    "    \n",
    "    # 找到表格行（以|开始和结束的行）\n",
    "    table_lines = []\n",
    "    in_table = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('|') and line.endswith('|'):\n",
    "            if not in_table:\n",
    "                in_table = True\n",
    "                table_lines = []\n",
    "            table_lines.append(line)\n",
    "        elif in_table and not line:\n",
    "            # 表格结束，找到了一个完整表格\n",
    "            break\n",
    "    \n",
    "    if not table_lines:\n",
    "        raise ValueError(\"未在Markdown文件中找到表格内容\")\n",
    "    \n",
    "    # 解析表格内容\n",
    "    rows = []\n",
    "    for line in table_lines:\n",
    "        # 分割行并移除前后的|\n",
    "        cells = line.strip().split('|')[1:-1]\n",
    "        # 清理每个单元格内容\n",
    "        cells = [cell.strip() for cell in cells]\n",
    "        rows.append(cells)\n",
    "    \n",
    "    # 第一行是表头\n",
    "    headers = rows[0]\n",
    "    \n",
    "    # 跳过表头和分隔行(通常是第二行，包含 ---|--- 类似内容)\n",
    "    data_rows = rows[2:] if len(rows) > 2 else []\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    \n",
    "    print(f\"已成功从Markdown提取表格，包含{len(df)}行和{len(df.columns)}列\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 使用示例\n",
    "# 假设Markdown文件路径\n",
    "markdown_file = os.path.join(folder_name, 'hypo.md')\n",
    "\n",
    "# 检查文件是否存在\n",
    "if os.path.exists(markdown_file):\n",
    "    # 从Markdown文件提取表格\n",
    "    variables_df = extract_markdown_table(markdown_file)\n",
    "    \n",
    "    # 显示提取的DataFrame\n",
    "    print(\"\\n提取的DataFrame头部:\")\n",
    "    display(variables_df.head())\n",
    "else:\n",
    "    print(f\"文件 {markdown_file} 不存在，请提供正确的文件路径\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.043756Z",
     "iopub.status.busy": "2025-04-15T10:45:12.041421Z",
     "iopub.status.idle": "2025-04-15T10:45:12.268350Z",
     "shell.execute_reply": "2025-04-15T10:45:12.268350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "处理后的数据样本:\n",
      "                    id  title  \\\n",
      "0  1911502146408952289    NaN   \n",
      "1  1910928938831847857    NaN   \n",
      "2  1910777809611694348    NaN   \n",
      "3  1909802064718578054    NaN   \n",
      "4  1909480340126581095    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "0  When choosing a shoe cabinet, consider space, ...  NaN   en   \n",
      "1  Jager-Smith Basic Duffle Polyester Bag/Gym Bag...  NaN   en   \n",
      "2  NEEWER DSLR Camera Clamp Adjustable Monitor Mo...  NaN   en   \n",
      "3  @Lukebraus @mrginden @nikicaga No it’s not, it...  NaN   en   \n",
      "4  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "0  Sun Apr 13 19:30:00 +0000 2025        2025-04-13          19:30:00   \n",
      "1  Sat Apr 12 05:32:17 +0000 2025        2025-04-12          05:32:17   \n",
      "2  Fri Apr 11 19:31:45 +0000 2025        2025-04-11          19:31:45   \n",
      "3  Wed Apr 09 02:54:29 +0000 2025        2025-04-09          02:54:29   \n",
      "4  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  author_video_cnt  \\\n",
      "0         False                     0  ...               NaN   \n",
      "1         False                     0  ...               NaN   \n",
      "2          True                     0  ...               NaN   \n",
      "3         False                     4  ...               NaN   \n",
      "4         False                     0  ...               NaN   \n",
      "\n",
      "   author_media_cnt  author_followers_cnt  author_org_attribute  \\\n",
      "0               666                   172                   NaN   \n",
      "1                34                     2                   NaN   \n",
      "2                16                    26                   NaN   \n",
      "3               203                   134                   NaN   \n",
      "4              1993                     6                   NaN   \n",
      "\n",
      "   author_verified_type  author_collection_cnt  \\\n",
      "0                 False                      0   \n",
      "1                 False                      0   \n",
      "2                 False                      0   \n",
      "3                 False                      0   \n",
      "4                 False                      0   \n",
      "\n",
      "                              author_profile_picture  \\\n",
      "0  https://pbs.twimg.com/profile_images/171799062...   \n",
      "1  https://pbs.twimg.com/profile_images/138328528...   \n",
      "2  https://pbs.twimg.com/profile_images/186457102...   \n",
      "3  https://pbs.twimg.com/profile_images/76942362/...   \n",
      "4  https://pbs.twimg.com/profile_images/186528787...   \n",
      "\n",
      "        author_registed_timestamp  author_registed_timestamp_date  \\\n",
      "0  Fri Oct 27 19:21:03 +0000 2023                      2023-10-27   \n",
      "1  Thu Sep 27 13:04:00 +0000 2018                      2018-09-27   \n",
      "2  Sat Jan 09 04:45:08 +0000 2021                      2021-01-09   \n",
      "3  Tue Aug 26 03:51:50 +0000 2008                      2008-08-26   \n",
      "4  Sun Jun 18 16:13:39 +0000 2023                      2023-06-18   \n",
      "\n",
      "   author_registed_timestamp_time  \n",
      "0                        19:21:03  \n",
      "1                        13:04:00  \n",
      "2                        04:45:08  \n",
      "3                        03:51:50  \n",
      "4                        16:13:39  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# 执行处理\n",
    "result = pd.read_excel(filename)\n",
    "\n",
    "# # 检查结果\n",
    "# print(\"处理后的数据框列:\")\n",
    "# print(result.columns.tolist())\n",
    "\n",
    "# # 定义感兴趣的交互字段\n",
    "# interaction_fields = ['rating', 'sumCnt', 'likeCnt', 'viewCnt', 'shareCnt', \n",
    "#                       'repostCnt', 'commentCnt', 'dislikeCnt', 'collectionCnt']\n",
    "\n",
    "# # 检查这些字段的空值情况\n",
    "# print(\"\\n处理后各交互字段的 NaN 值数量:\")\n",
    "# for field in interaction_fields:\n",
    "#     print(f\"{field} 的 null 值数量: {result[field].isna().sum()}\")\n",
    "\n",
    "# 验证前几行\n",
    "print(\"\\n处理后的数据样本:\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.268350Z",
     "iopub.status.busy": "2025-04-15T10:45:12.268350Z",
     "iopub.status.idle": "2025-04-15T10:45:12.283594Z",
     "shell.execute_reply": "2025-04-15T10:45:12.283594Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 定义可能来自 interaction 的所有字段\n",
    "# interaction_fields = ['rating', 'sumCnt', 'likeCnt', 'reactor', 'viewCnt', \n",
    "#                      'shareCnt', 'repostCnt', 'commentCnt', 'dislikeCnt', 'collectionCnt']\n",
    "\n",
    "# # 替换所有这些字段中的 NaN 值为 0\n",
    "# for field in interaction_fields:\n",
    "#     if field in result.columns:\n",
    "#         result[field] = result[field].fillna(0)\n",
    "\n",
    "# # 检查是否还有 NaN 值\n",
    "# print(\"处理后各交互字段的 NaN 值数量:\")\n",
    "# for field in interaction_fields:\n",
    "#     if field in result.columns:\n",
    "#         print(f\"{field}: {result[field].isna().sum()}\")\n",
    "\n",
    "# # 验证前几行\n",
    "# print(\"\\n处理后的数据样本:\")\n",
    "# print(result[interaction_fields].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先使用关键词筛选topic列，再筛选content列，防止错漏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.283594Z",
     "iopub.status.busy": "2025-04-15T10:45:12.283594Z",
     "iopub.status.idle": "2025-04-15T10:45:12.298625Z",
     "shell.execute_reply": "2025-04-15T10:45:12.298625Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "\n",
    "\n",
    "def load_keywords(file_path):\n",
    "    \"\"\"\n",
    "    加载关键词文件，每行一个关键词\n",
    "    返回一个包含所有关键词的集合（全部转为小写以确保大小写不敏感匹配）\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # 去除每行末尾的换行符，并过滤掉空行，全部转为小写\n",
    "        keywords = {line.strip().lower() for line in f if line.strip()}\n",
    "    \n",
    "    print(f\"已加载 {len(keywords)} 个关键词\")\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    对文本进行标准化处理，包括：\n",
    "    1. 移除表情符号和特殊字符\n",
    "    2. 转为小写\n",
    "    3. 处理标点符号\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 规范化为 NFKD 形式，这会将复合字符分解\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # 移除非ASCII字符（包括表情符号）\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # 转为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 用空格替换标点符号和特殊字符\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # 将多个空格替换为单个空格\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"\n",
    "    检查文本是否包含关键词列表中的任何一个关键词\n",
    "    大小写不敏感匹配\n",
    "    \n",
    "    参数:\n",
    "    text - 要检查的文本（应该已经标准化为小写）\n",
    "    keywords - 关键词集合（全部小写）\n",
    "    \n",
    "    返回:\n",
    "    True 如果文本包含任何关键词，否则 False\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # 将文本分割成单词，便于完整单词匹配\n",
    "    words = set(text.split())\n",
    "    \n",
    "    # 检查是否有关键词出现在文本中（作为完整单词）\n",
    "    if any(keyword in words for keyword in keywords):\n",
    "        return True\n",
    "    \n",
    "    # 检查是否有多词关键词是文本的子字符串\n",
    "    for keyword in keywords:\n",
    "        if ' ' in keyword and keyword in text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def filter_dataframe_by_keywords(df, keywords_file, verbose=True):\n",
    "    \"\"\"\n",
    "    通过关键词对DataFrame进行内容筛选\n",
    "    \n",
    "    参数:\n",
    "    df - 要筛选的DataFrame\n",
    "    keywords_file - 关键词文件路径\n",
    "    verbose - 是否显示详细进度\n",
    "    \n",
    "    返回:\n",
    "    筛选后的DataFrame\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 加载关键词（全部转为小写以确保大小写不敏感匹配）\n",
    "    keywords = load_keywords(keywords_file)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"原始数据: {len(df)} 行\")\n",
    "    \n",
    "    # 通过content筛选\n",
    "    if verbose:\n",
    "        print(\"正在通过content筛选...\")\n",
    "    \n",
    "    # 创建一个掩码数组来标记要保留的行\n",
    "    content_mask = pd.Series(False, index=df.index)\n",
    "    \n",
    "    if 'content' in df.columns:\n",
    "        # 对于每一行，检查其content是否包含任何关键词\n",
    "        for idx, content in tqdm(enumerate(df['content']), total=len(df), disable=not verbose):\n",
    "            if isinstance(content, str) and content.strip():\n",
    "                # 标准化content（转为小写等）\n",
    "                normalized_content = normalize_text(content)\n",
    "                if contains_keywords(normalized_content, keywords):\n",
    "                    content_mask.iloc[idx] = True\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"警告: DataFrame中没有'content'列，无法进行筛选\")\n",
    "            return df\n",
    "    \n",
    "    # 筛选结果\n",
    "    filtered_df = df[content_mask].copy()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    if verbose:\n",
    "        print(f\"筛选结果: {len(filtered_df)} 行 ({len(filtered_df)/len(df)*100:.2f}%)\")\n",
    "        print(f\"总耗时: {end_time - start_time:.2f} 秒\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# filtered_data = filter_dataframe_by_keywords(df, 'keywords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.298625Z",
     "iopub.status.busy": "2025-04-15T10:45:12.298625Z",
     "iopub.status.idle": "2025-04-15T10:45:12.359234Z",
     "shell.execute_reply": "2025-04-15T10:45:12.359234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 61 个关键词\n",
      "原始数据: 898 行\n",
      "正在通过content筛选...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/898 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 898/898 [00:00<00:00, 24239.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选结果: 605 行 (67.37%)\n",
      "总耗时: 0.04 秒\n",
      "(605, 37)\n",
      "                    id  title  \\\n",
      "4  1909480340126581095    NaN   \n",
      "6  1909020699747885481    NaN   \n",
      "7  1908203431765881038    NaN   \n",
      "8  1908194727209386420    NaN   \n",
      "9  1908053656731086945    NaN   \n",
      "\n",
      "                                             content  url lang  \\\n",
      "4  Metal Shoe Rack\\nUpto 24% off\\nLink for you: h...  NaN   en   \n",
      "6  @emob_ @DamnNearWhite @Tr3sMilagros @MyFirstKi...  NaN   en   \n",
      "7  J.T. Foote Adjustable Shoe Trees - Plastic \\n🛠...  NaN   en   \n",
      "8  EXVITO Metal Shoe Rack for Home – Adjustable &...  NaN   en   \n",
      "9  Closet Shoe Organizer for 24 Pairs with Adjust...  NaN   en   \n",
      "\n",
      "                     publish_time publish_time_date publish_time_time  \\\n",
      "4  Tue Apr 08 05:36:04 +0000 2025        2025-04-08          05:36:04   \n",
      "6  Sun Apr 06 23:09:37 +0000 2025        2025-04-06          23:09:37   \n",
      "7  Fri Apr 04 17:02:05 +0000 2025        2025-04-04          17:02:05   \n",
      "8  Fri Apr 04 16:27:30 +0000 2025        2025-04-04          16:27:30   \n",
      "9  Fri Apr 04 07:06:56 +0000 2025        2025-04-04          07:06:56   \n",
      "\n",
      "   is_truncated  interaction_like_cnt  ...  author_video_cnt  \\\n",
      "4         False                     0  ...               NaN   \n",
      "6         False                     1  ...               NaN   \n",
      "7         False                     0  ...               NaN   \n",
      "8         False                     0  ...               NaN   \n",
      "9         False                     0  ...               NaN   \n",
      "\n",
      "   author_media_cnt  author_followers_cnt  author_org_attribute  \\\n",
      "4              1993                     6                   NaN   \n",
      "6              5586                  4709                   NaN   \n",
      "7              1072                    24                   NaN   \n",
      "8              2179                  3974                   NaN   \n",
      "9                86                  3118                   NaN   \n",
      "\n",
      "   author_verified_type  author_collection_cnt  \\\n",
      "4                 False                      0   \n",
      "6                 False                      0   \n",
      "7                 False                      0   \n",
      "8                 False                      0   \n",
      "9                 False                      0   \n",
      "\n",
      "                              author_profile_picture  \\\n",
      "4  https://pbs.twimg.com/profile_images/186528787...   \n",
      "6  https://pbs.twimg.com/profile_images/191001923...   \n",
      "7  https://pbs.twimg.com/profile_images/941323346...   \n",
      "8  https://pbs.twimg.com/profile_images/182118449...   \n",
      "9  https://pbs.twimg.com/profile_images/189695809...   \n",
      "\n",
      "        author_registed_timestamp  author_registed_timestamp_date  \\\n",
      "4  Sun Jun 18 16:13:39 +0000 2023                      2023-06-18   \n",
      "6  Thu Feb 11 17:37:08 +0000 2016                      2016-02-11   \n",
      "7  Thu Dec 14 14:58:31 +0000 2017                      2017-12-14   \n",
      "8  Mon Jan 05 11:51:21 +0000 2015                      2015-01-05   \n",
      "9  Wed Oct 23 12:18:05 +0000 2024                      2024-10-23   \n",
      "\n",
      "   author_registed_timestamp_time  \n",
      "4                        16:13:39  \n",
      "6                        17:37:08  \n",
      "7                        14:58:31  \n",
      "8                        11:51:21  \n",
      "9                        12:18:05  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选数据\n",
    "filtered_data = filter_dataframe_by_keywords(result, filename_keywords)\n",
    "\n",
    "# 查看筛选结果\n",
    "print(filtered_data.shape)\n",
    "print(filtered_data.head())\n",
    "\n",
    "# 保存结果\n",
    "# filtered_data.to_csv('filtered_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T10:45:12.361101Z",
     "iopub.status.busy": "2025-04-15T10:45:12.359234Z",
     "iopub.status.idle": "2025-04-15T10:45:12.618189Z",
     "shell.execute_reply": "2025-04-15T10:45:12.618189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到: 生成结果/social_media/关键词筛选后的数据.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 创建文件夹（如果不存在）\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"已创建文件夹: {folder_name}\")\n",
    "\n",
    "# 修改保存路径，将输出文件保存到新文件夹\n",
    "output_path = os.path.join(folder_name, '关键词筛选后的数据.xlsx')\n",
    "filtered_data.to_excel(output_path, index=False)\n",
    "print(f\"数据已保存到: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lynx",
   "language": "python",
   "name": "lynx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
